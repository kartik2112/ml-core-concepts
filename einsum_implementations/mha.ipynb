{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f381dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb7b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_prod_attention(Q, K, V, mask=None):\n",
    "\t# softmax(Q@KT / sqrt(d_K))@V\n",
    "\td_K = K.size(-1)\n",
    "\tscores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_K)\n",
    "\tif mask is not None:\n",
    "\t\tscores = scores.masked_fill(mask==0, -1e9)\n",
    "\tattn_weights = F.softmax(scores)\n",
    "\treturn attn_weights @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d698d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "L = 5\n",
    "D = 3\n",
    "Q = torch.randn(B, L, D)\n",
    "K = torch.randn(B, L, D)\n",
    "V = torch.randn(B, L, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261b2216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wx/lqxvtyx538sf_yd2k1ncs65r0000gn/T/ipykernel_55844/2669259481.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attn_weights = F.softmax(scores)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1489, -2.9056,  2.1917],\n",
       "         [-0.5000, -1.5153,  1.3965],\n",
       "         [ 0.1936, -0.9806,  0.4869],\n",
       "         [-1.1397, -1.3985,  1.5173],\n",
       "         [-1.1454, -1.8547,  1.8464]],\n",
       "\n",
       "        [[ 0.8482,  0.1189,  0.4005],\n",
       "         [ 1.8838,  0.5028,  0.6647],\n",
       "         [ 2.1943, -0.0124,  0.3628],\n",
       "         [ 1.7696,  1.1031,  1.0317],\n",
       "         [ 1.5680,  1.2566,  0.9846]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_prod_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeaa77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self, d_model, num_heads, dropout=0.1):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tassert d_model % num_heads == 0\n",
    "\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\tself.d_k = d_model // num_heads\n",
    "\t\t\n",
    "\t\tself.W_q = nn.Linear(d_model, d_model)\n",
    "\t\tself.W_k = nn.Linear(d_model, d_model)\n",
    "\t\tself.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "\t\tself.W_o = nn.Linear(d_model, d_model)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\n",
    "\tdef split_heads(self, x):\n",
    "\t\tB, L, _ = x.size()\n",
    "\t\tx = x.view(B, L, self.num_heads, self.d_k)\n",
    "\t\treturn x.transpose(1, 2)\n",
    "\t\n",
    "\tdef combine_heads(self, x):\n",
    "\t\tB, _, L, _ = x.size()\n",
    "\t\tx = x.transpose(1, 2).contiguous() # B, L, num_heads, d_k\n",
    "\t\treturn x.view(B, L, self.d_model)\n",
    "\n",
    "\tdef forward(self, x, mask=None):\n",
    "\t\tQ = self.W_q(x)\n",
    "\t\tK = self.W_k(x)\n",
    "\t\tV = self.W_v(x)\n",
    "\n",
    "\t\tQ = self.split_heads(Q)\n",
    "\t\tK = self.split_heads(K)\n",
    "\t\tV = self.split_heads(V)\n",
    "\n",
    "\t\tscores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "\t\tif mask is not None:\n",
    "\t\t\tscores = scores.masked_fill(mask==0, -1e9)\n",
    "\t\t\n",
    "\t\tattn_weights = F.softmax(scores, dim=-1)\n",
    "\t\tattn_weights = self.dropout(attn_weights)\n",
    "\n",
    "\t\tattn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "\t\toutput = self.combine_heads(attn_output)\n",
    "\t\toutput = self.W_o(output)\n",
    "\n",
    "\t\treturn output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11c12ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 100\n",
    "num_heads = 10\n",
    "dropout = 0.1\n",
    "\n",
    "X = torch.randn(5, 15, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da638ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "op, _ = mha.forward(X)\n",
    "op.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f00f6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ein_MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self, D, H, dropout=0.1):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tassert D % H == 0\n",
    "\t\tself.D = D\n",
    "\t\tself.H = H\n",
    "\t\tself.d_K = D // H\n",
    "\n",
    "\t\tself.W_q = nn.Linear(self.D, self.D)\n",
    "\t\tself.W_k = nn.Linear(self.D, self.D)\n",
    "\t\tself.W_v = nn.Linear(self.D, self.D)\n",
    "\n",
    "\t\tself.W_o = nn.Linear(self.D, self.D)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\n",
    "\tdef split_heads(self, x):\n",
    "\t\tB, L, _ = x.size()\n",
    "\t\tx = x.view(B, L, self.H, self.d_K)\n",
    "\t\treturn torch.einsum('ijkl->ikjl', [x])\n",
    "\t\n",
    "\tdef combine_heads(self, x):\n",
    "\t\tB, _, L, _ = x.size()\n",
    "\t\tx = torch.einsum('ijkl->ikjl', [x])\n",
    "\t\treturn x.reshape(B, L, self.D)\n",
    "\t\n",
    "\tdef forward(self, x, mask = None):\n",
    "\t\tQ = self.W_q(x)\n",
    "\t\tK = self.W_k(x)\n",
    "\t\tV = self.W_v(x)\n",
    "\n",
    "\t\tQ = self.split_heads(Q)\n",
    "\t\tK = self.split_heads(K)\n",
    "\t\tV = self.split_heads(V)\n",
    "\n",
    "\t\tscores = torch.einsum('bhld,bhdk->bhlk', [Q, torch.einsum('bhld->bhdl', [K])]) / math.sqrt(self.d_K)\n",
    "\t\tif mask is not None:\n",
    "\t\t\tscores = scores.masked_fill(mask==0, -1e9)\n",
    "\t\tattn_weights = F.softmax(scores, dim=-1)\n",
    "\t\tattn_weights = self.dropout(attn_weights)\n",
    "\n",
    "\t\tattn_output = torch.einsum('bhlk,bhkd->bhld', [attn_weights, V])\n",
    "\n",
    "\t\tattn_output = self.combine_heads(attn_output)\n",
    "\n",
    "\t\toutput = self.W_o(attn_output)\n",
    "\t\treturn output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "715b83bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 15, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = ein_MultiHeadAttention(d_model, num_heads, dropout)\n",
    "op, _ = mha.forward(X)\n",
    "op.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcb6ebd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Num of parameters:  40,400'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Num of parameters: {sum([p.numel() for p in mha.parameters()]): ,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d2806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
