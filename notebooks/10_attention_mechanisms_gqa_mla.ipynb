{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Attention Mechanisms: GQA, MLA, and KV Cache\n",
    "\n",
    "This notebook explores modern attention mechanisms and optimization techniques for efficient LLM inference.\n",
    "\n",
    "We'll cover:\n",
    "1. **Multi-Head Attention (MHA)** - Baseline\n",
    "2. **Multi-Query Attention (MQA)** - Single K/V head\n",
    "3. **Grouped Query Attention (GQA)** - Middle ground (used in LLaMA 2)\n",
    "4. **Multi-Head Latent Attention (MLA)** - Low-rank compression\n",
    "5. **KV Cache** - Critical for autoregressive inference\n",
    "6. **Performance comparisons**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standard Multi-Head Attention (MHA)\n",
    "\n",
    "Traditional MHA has **separate Q, K, V heads for each attention head**:\n",
    "- H heads, each with separate Q, K, V projections\n",
    "- Memory: O(H × d)\n",
    "- Used in: Original Transformer, GPT-2, GPT-3\n",
    "\n",
    "$$\\text{MHA}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_H)W^O$$\n",
    "$$\\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Multi-Head Attention (MHA).\n",
    "    Each head has its own Q, K, V projections.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Separate projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None, kv_cache=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (batch_size, seq_len, d_model)\n",
    "            mask: Attention mask\n",
    "            kv_cache: Cached K, V from previous steps\n",
    "            use_cache: Whether to return cache for next step\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        # Reshape for multi-head: (batch, seq_len, num_heads, head_dim)\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Handle KV cache for autoregressive generation\n",
    "        if kv_cache is not None:\n",
    "            past_k, past_v = kv_cache\n",
    "            k = torch.cat([past_k, k], dim=1)\n",
    "            v = torch.cat([past_v, v], dim=1)\n",
    "        \n",
    "        # Transpose for attention: (batch, num_heads, seq_len, head_dim)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Compute attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        out = out.view(batch_size, seq_len, self.d_model)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        new_cache = (k.transpose(1, 2), v.transpose(1, 2)) if use_cache else None\n",
    "        return out, attn_weights, new_cache\n",
    "\n",
    "# Example\n",
    "mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "x = torch.randn(2, 10, 512)\n",
    "out, attn, cache = mha(x, use_cache=True)\n",
    "\n",
    "print(f\"MHA - Input: {x.shape}\")\n",
    "print(f\"MHA - Output: {out.shape}\")\n",
    "print(f\"MHA - Attention: {attn.shape}\")\n",
    "print(f\"MHA - Parameters: {sum(p.numel() for p in mha.parameters()):,}\")\n",
    "if cache:\n",
    "    print(f\"MHA - Cache K shape: {cache[0].shape}, Cache V shape: {cache[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Query Attention (MQA)\n",
    "\n",
    "MQA uses **single shared K and V** across all heads:\n",
    "- H query heads, but only 1 K head and 1 V head\n",
    "- Memory: O(d) for K, V (much smaller!)\n",
    "- Used in: PaLM, Falcon\n",
    "\n",
    "**Key benefit:** Dramatically reduces KV cache size during inference\n",
    "\n",
    "**Trade-off:** Slightly lower quality than MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Query Attention (MQA).\n",
    "    Multiple query heads, but single shared K and V.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Multiple query heads\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Single K and V (shared across all heads)\n",
    "        self.k_proj = nn.Linear(d_model, self.head_dim)\n",
    "        self.v_proj = nn.Linear(d_model, self.head_dim)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None, kv_cache=None, use_cache=False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project queries (multiple heads)\n",
    "        q = self.q_proj(x)\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Project K, V (single head, will be broadcast)\n",
    "        k = self.k_proj(x)  # (batch, seq_len, head_dim)\n",
    "        v = self.v_proj(x)  # (batch, seq_len, head_dim)\n",
    "        \n",
    "        # Handle KV cache\n",
    "        if kv_cache is not None:\n",
    "            past_k, past_v = kv_cache\n",
    "            k = torch.cat([past_k, k], dim=1)\n",
    "            v = torch.cat([past_v, v], dim=1)\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        # Q: (batch, num_heads, seq_len, head_dim)\n",
    "        # K, V: (batch, 1, kv_seq_len, head_dim) - will broadcast across heads\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.unsqueeze(1)  # Add head dimension for broadcasting\n",
    "        v = v.unsqueeze(1)\n",
    "        \n",
    "        # Compute attention (K, V broadcast across all query heads)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        out = out.view(batch_size, seq_len, self.d_model)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        # Cache stores only single K, V (not per-head)\n",
    "        new_cache = (k.squeeze(1), v.squeeze(1)) if use_cache else None\n",
    "        return out, attn_weights, new_cache\n",
    "\n",
    "# Example\n",
    "mqa = MultiQueryAttention(d_model=512, num_heads=8)\n",
    "x = torch.randn(2, 10, 512)\n",
    "out, attn, cache = mqa(x, use_cache=True)\n",
    "\n",
    "print(f\"\\nMQA - Input: {x.shape}\")\n",
    "print(f\"MQA - Output: {out.shape}\")\n",
    "print(f\"MQA - Attention: {attn.shape}\")\n",
    "print(f\"MQA - Parameters: {sum(p.numel() for p in mqa.parameters()):,}\")\n",
    "if cache:\n",
    "    print(f\"MQA - Cache K shape: {cache[0].shape}, Cache V shape: {cache[1].shape}\")\n",
    "    print(f\"MQA - Cache is {mha.num_heads}x smaller than MHA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grouped Query Attention (GQA)\n",
    "\n",
    "GQA is the **middle ground between MHA and MQA**:\n",
    "- H query heads divided into G groups\n",
    "- Each group shares one K and V head\n",
    "- Memory: O(G × d) where G < H\n",
    "- Used in: **LLaMA 2, Mistral**\n",
    "\n",
    "**Benefits:**\n",
    "- Better quality than MQA (more K/V diversity)\n",
    "- Smaller cache than MHA\n",
    "- Good balance for production LLMs\n",
    "\n",
    "Example: 32 query heads → 8 KV groups (4 queries per KV head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped Query Attention (GQA).\n",
    "    Query heads are divided into groups, each group shares K and V.\n",
    "    Used in LLaMA 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, num_kv_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        assert num_heads % num_kv_heads == 0, \"num_heads must be divisible by num_kv_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.num_queries_per_kv = num_heads // num_kv_heads\n",
    "        \n",
    "        # Query projection (all heads)\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # K, V projections (num_kv_heads)\n",
    "        self.k_proj = nn.Linear(d_model, num_kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(d_model, num_kv_heads * self.head_dim)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None, kv_cache=None, use_cache=False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project Q (all heads)\n",
    "        q = self.q_proj(x)\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Project K, V (num_kv_heads)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        k = k.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "        v = v.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
    "        \n",
    "        # Handle KV cache\n",
    "        if kv_cache is not None:\n",
    "            past_k, past_v = kv_cache\n",
    "            k = torch.cat([past_k, k], dim=1)\n",
    "            v = torch.cat([past_v, v], dim=1)\n",
    "        \n",
    "        # Transpose: (batch, num_heads, seq_len, head_dim)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Repeat K, V for each group\n",
    "        # Shape: (batch, num_heads, seq_len, head_dim)\n",
    "        k = k.repeat_interleave(self.num_queries_per_kv, dim=1)\n",
    "        v = v.repeat_interleave(self.num_queries_per_kv, dim=1)\n",
    "        \n",
    "        # Compute attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        out = out.view(batch_size, seq_len, self.d_model)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        # Cache stores only num_kv_heads (not all num_heads)\n",
    "        if use_cache:\n",
    "            k_cache = k[:, ::self.num_queries_per_kv, :, :].transpose(1, 2)\n",
    "            v_cache = v[:, ::self.num_queries_per_kv, :, :].transpose(1, 2)\n",
    "            new_cache = (k_cache, v_cache)\n",
    "        else:\n",
    "            new_cache = None\n",
    "        \n",
    "        return out, attn_weights, new_cache\n",
    "\n",
    "# Example: 8 heads with 2 KV heads (4 queries per KV)\n",
    "gqa = GroupedQueryAttention(d_model=512, num_heads=8, num_kv_heads=2)\n",
    "x = torch.randn(2, 10, 512)\n",
    "out, attn, cache = gqa(x, use_cache=True)\n",
    "\n",
    "print(f\"\\nGQA - Input: {x.shape}\")\n",
    "print(f\"GQA - Output: {out.shape}\")\n",
    "print(f\"GQA - Attention: {attn.shape}\")\n",
    "print(f\"GQA - Parameters: {sum(p.numel() for p in gqa.parameters()):,}\")\n",
    "print(f\"GQA - Queries per KV: {gqa.num_queries_per_kv}\")\n",
    "if cache:\n",
    "    print(f\"GQA - Cache K shape: {cache[0].shape}, Cache V shape: {cache[1].shape}\")\n",
    "    print(f\"GQA - Cache is {mha.num_heads // gqa.num_kv_heads}x smaller than MHA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Head Latent Attention (MLA)\n",
    "\n",
    "MLA uses **low-rank compression** of K and V:\n",
    "- Projects K, V to lower-dimensional latent space\n",
    "- Stores compressed representation\n",
    "- Decompresses for attention computation\n",
    "- Used in: DeepSeek-V2\n",
    "\n",
    "**Key innovation:**\n",
    "- KV cache size reduced by ~90%\n",
    "- Compression ratio configurable\n",
    "- Minimal quality loss with proper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Latent Attention (MLA).\n",
    "    Compresses K, V into low-rank latent representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, latent_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Latent dimension (compressed representation)\n",
    "        # Typically much smaller than d_model\n",
    "        self.latent_dim = latent_dim or (d_model // 4)\n",
    "        \n",
    "        # Query projection (full dimension)\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Compress to latent space\n",
    "        self.kv_compress = nn.Linear(d_model, self.latent_dim)\n",
    "        \n",
    "        # Decompress from latent to K, V\n",
    "        self.k_decompress = nn.Linear(self.latent_dim, d_model)\n",
    "        self.v_decompress = nn.Linear(self.latent_dim, d_model)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None, kv_cache=None, use_cache=False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Query projection (full dimension)\n",
    "        q = self.q_proj(x)\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Compress to latent space\n",
    "        kv_latent = self.kv_compress(x)  # (batch, seq_len, latent_dim)\n",
    "        \n",
    "        # Handle KV cache (cache stores compressed latent!)\n",
    "        if kv_cache is not None:\n",
    "            past_latent = kv_cache\n",
    "            kv_latent = torch.cat([past_latent, kv_latent], dim=1)\n",
    "        \n",
    "        # Decompress to K, V\n",
    "        k = self.k_decompress(kv_latent)\n",
    "        v = self.v_decompress(kv_latent)\n",
    "        \n",
    "        k = k.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        v = v.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose for attention\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Compute attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        out = out.view(batch_size, seq_len, self.d_model)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        # Cache stores compressed latent (much smaller!)\n",
    "        new_cache = kv_latent if use_cache else None\n",
    "        return out, attn_weights, new_cache\n",
    "\n",
    "# Example: latent_dim = d_model // 4 (4x compression)\n",
    "mla = MultiHeadLatentAttention(d_model=512, num_heads=8, latent_dim=128)\n",
    "x = torch.randn(2, 10, 512)\n",
    "out, attn, cache = mla(x, use_cache=True)\n",
    "\n",
    "print(f\"\\nMLA - Input: {x.shape}\")\n",
    "print(f\"MLA - Output: {out.shape}\")\n",
    "print(f\"MLA - Attention: {attn.shape}\")\n",
    "print(f\"MLA - Parameters: {sum(p.numel() for p in mla.parameters()):,}\")\n",
    "print(f\"MLA - Latent dimension: {mla.latent_dim} (compression: {mla.d_model / mla.latent_dim:.1f}x)\")\n",
    "if cache:\n",
    "    print(f\"MLA - Cache shape: {cache.shape}\")\n",
    "    print(f\"MLA - Cache is ~{mla.d_model / mla.latent_dim:.1f}x smaller than MHA per element!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. KV Cache Visualization and Analysis\n",
    "\n",
    "The KV cache is crucial for efficient autoregressive generation:\n",
    "- Store K, V from previous tokens\n",
    "- Reuse for each new token\n",
    "- Avoid recomputing attention for entire sequence\n",
    "\n",
    "**Without KV cache:** O(n²) for n tokens\n",
    "**With KV cache:** O(n) for n tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_kv_cache():\n",
    "    \"\"\"\n",
    "    Demonstrate KV cache usage in autoregressive generation.\n",
    "    \"\"\"\n",
    "    d_model = 256\n",
    "    num_heads = 8\n",
    "    batch_size = 1\n",
    "    \n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    print(\"=== Autoregressive Generation with KV Cache ===\")\n",
    "    print()\n",
    "    \n",
    "    # Initial prompt\n",
    "    prompt_len = 5\n",
    "    x_prompt = torch.randn(batch_size, prompt_len, d_model)\n",
    "    \n",
    "    print(f\"Step 1: Process prompt (length {prompt_len})\")\n",
    "    out, _, cache = mha(x_prompt, use_cache=True)\n",
    "    print(f\"  Cache K shape: {cache[0].shape}\")\n",
    "    print(f\"  Cache V shape: {cache[1].shape}\")\n",
    "    print(f\"  Total cache size: {cache[0].numel() + cache[1].numel()} elements\")\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    num_new_tokens = 3\n",
    "    for i in range(num_new_tokens):\n",
    "        print(f\"\\nStep {i+2}: Generate token {i+1}\")\n",
    "        \n",
    "        # Process single new token\n",
    "        x_new = torch.randn(batch_size, 1, d_model)\n",
    "        out, _, cache = mha(x_new, kv_cache=cache, use_cache=True)\n",
    "        \n",
    "        print(f\"  Input: {x_new.shape}\")\n",
    "        print(f\"  Cache K shape: {cache[0].shape} (grew by 1)\")\n",
    "        print(f\"  Cache V shape: {cache[1].shape} (grew by 1)\")\n",
    "    \n",
    "    print(f\"\\nFinal sequence length: {prompt_len + num_new_tokens}\")\n",
    "    print(f\"Final cache size: {cache[0].shape}\")\n",
    "\n",
    "demonstrate_kv_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Comparison\n",
    "\n",
    "Compare KV cache memory usage across different attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kv_cache_memory():\n",
    "    \"\"\"\n",
    "    Compare KV cache memory requirements across attention variants.\n",
    "    \"\"\"\n",
    "    # Model configuration\n",
    "    d_model = 4096\n",
    "    num_heads = 32\n",
    "    num_kv_heads = 8  # For GQA\n",
    "    latent_dim = 1024  # For MLA\n",
    "    \n",
    "    # Generation settings\n",
    "    batch_size = 1\n",
    "    seq_len = 2048\n",
    "    num_layers = 32\n",
    "    \n",
    "    head_dim = d_model // num_heads\n",
    "    bytes_per_param = 2  # FP16\n",
    "    \n",
    "    # Calculate cache size for each method\n",
    "    # Cache stores K and V for each layer\n",
    "    \n",
    "    # MHA: num_heads * head_dim = d_model per K or V\n",
    "    mha_cache_per_layer = 2 * batch_size * seq_len * d_model\n",
    "    mha_total = mha_cache_per_layer * num_layers * bytes_per_param\n",
    "    \n",
    "    # MQA: single head_dim per K or V\n",
    "    mqa_cache_per_layer = 2 * batch_size * seq_len * head_dim\n",
    "    mqa_total = mqa_cache_per_layer * num_layers * bytes_per_param\n",
    "    \n",
    "    # GQA: num_kv_heads * head_dim\n",
    "    gqa_cache_per_layer = 2 * batch_size * seq_len * num_kv_heads * head_dim\n",
    "    gqa_total = gqa_cache_per_layer * num_layers * bytes_per_param\n",
    "    \n",
    "    # MLA: latent_dim (compressed)\n",
    "    mla_cache_per_layer = batch_size * seq_len * latent_dim\n",
    "    mla_total = mla_cache_per_layer * num_layers * bytes_per_param\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=== KV Cache Memory Comparison ===\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Model: d_model={d_model}, num_heads={num_heads}, layers={num_layers}\")\n",
    "    print(f\"  Generation: batch={batch_size}, seq_len={seq_len}\")\n",
    "    print(f\"  Precision: FP16 ({bytes_per_param} bytes/param)\")\n",
    "    print()\n",
    "    \n",
    "    def format_size(bytes_val):\n",
    "        mb = bytes_val / (1024 ** 2)\n",
    "        gb = bytes_val / (1024 ** 3)\n",
    "        if gb >= 1:\n",
    "            return f\"{gb:.2f} GB\"\n",
    "        return f\"{mb:.2f} MB\"\n",
    "    \n",
    "    print(f\"MHA (Multi-Head Attention):\")\n",
    "    print(f\"  Total cache: {format_size(mha_total)}\")\n",
    "    print(f\"  Baseline (1.0x)\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"MQA (Multi-Query Attention):\")\n",
    "    print(f\"  Total cache: {format_size(mqa_total)}\")\n",
    "    print(f\"  Reduction: {mha_total / mqa_total:.1f}x smaller ({100 * (1 - mqa_total/mha_total):.1f}% less)\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"GQA (Grouped Query Attention):\")\n",
    "    print(f\"  Total cache: {format_size(gqa_total)}\")\n",
    "    print(f\"  Reduction: {mha_total / gqa_total:.1f}x smaller ({100 * (1 - gqa_total/mha_total):.1f}% less)\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"MLA (Multi-Head Latent Attention):\")\n",
    "    print(f\"  Total cache: {format_size(mla_total)}\")\n",
    "    print(f\"  Reduction: {mha_total / mla_total:.1f}x smaller ({100 * (1 - mla_total/mha_total):.1f}% less)\")\n",
    "    print()\n",
    "    \n",
    "    # Visualization\n",
    "    methods = ['MHA', 'MQA', 'GQA', 'MLA']\n",
    "    sizes_gb = [mha_total / (1024**3), mqa_total / (1024**3), \n",
    "                gqa_total / (1024**3), mla_total / (1024**3)]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(methods, sizes_gb, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    plt.ylabel('KV Cache Size (GB)', fontsize=12)\n",
    "    plt.title(f'KV Cache Memory Comparison\\n({num_layers} layers, seq_len={seq_len}, batch={batch_size})', \n",
    "              fontsize=14)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, size in zip(bars, sizes_gb):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{size:.2f} GB',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_kv_cache_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison\n",
    "\n",
    "Compare computational efficiency of different attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention_variants():\n",
    "    \"\"\"\n",
    "    Benchmark inference speed of different attention mechanisms.\n",
    "    \"\"\"\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    batch_size = 4\n",
    "    seq_len = 128\n",
    "    num_iterations = 100\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'MHA': MultiHeadAttention(d_model, num_heads),\n",
    "        'MQA': MultiQueryAttention(d_model, num_heads),\n",
    "        'GQA': GroupedQueryAttention(d_model, num_heads, num_kv_heads=2),\n",
    "        'MLA': MultiHeadLatentAttention(d_model, num_heads, latent_dim=128)\n",
    "    }\n",
    "    \n",
    "    # Set to eval mode\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    \n",
    "    # Benchmark\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== Attention Mechanism Benchmark ===\")\n",
    "    print(f\"Configuration: batch={batch_size}, seq_len={seq_len}, d_model={d_model}\")\n",
    "    print(f\"Iterations: {num_iterations}\\n\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for name, model in models.items():\n",
    "            x = torch.randn(batch_size, seq_len, d_model)\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(10):\n",
    "                _ = model(x)\n",
    "            \n",
    "            # Benchmark\n",
    "            start = time.time()\n",
    "            for _ in range(num_iterations):\n",
    "                _ = model(x)\n",
    "            end = time.time()\n",
    "            \n",
    "            avg_time = (end - start) / num_iterations * 1000  # ms\n",
    "            results[name] = avg_time\n",
    "            \n",
    "            print(f\"{name:8s}: {avg_time:.3f} ms/iteration\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    names = list(results.keys())\n",
    "    times = list(results.values())\n",
    "    \n",
    "    bars = plt.bar(names, times, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    plt.ylabel('Time (ms)', fontsize=12)\n",
    "    plt.title('Attention Mechanism Inference Speed', fontsize=14)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, time_val in zip(bars, times):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{time_val:.2f} ms',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "benchmark_attention_variants()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Trade-off Analysis\n",
    "\n",
    "Summarize the trade-offs between different attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comprehensive comparison table\n",
    "comparison = {\n",
    "    'Method': ['MHA', 'MQA', 'GQA', 'MLA'],\n",
    "    'KV Heads': ['H', '1', 'G (1 < G < H)', 'H'],\n",
    "    'KV Cache Size': ['H × d', 'd', 'G × d', 'latent_dim'],\n",
    "    'Quality': ['Best', 'Good', 'Very Good', 'Very Good'],\n",
    "    'Memory': ['High', 'Low', 'Medium', 'Very Low'],\n",
    "    'Speed': ['Baseline', 'Faster', 'Fast', 'Variable'],\n",
    "    'Used In': ['GPT-3', 'PaLM, Falcon', 'LLaMA 2, Mistral', 'DeepSeek-V2']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison)\n",
    "\n",
    "print(\"\\n=== Attention Mechanism Comparison ===\")\n",
    "print(df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "print(\"Key Insights:\")\n",
    "print()\n",
    "print(\"1. **MHA (Multi-Head Attention)**\")\n",
    "print(\"   - Best quality but highest memory\")\n",
    "print(\"   - Used in earlier large models (GPT-3)\")\n",
    "print(\"   - Each head has independent K, V\")\n",
    "print()\n",
    "print(\"2. **MQA (Multi-Query Attention)**\")\n",
    "print(\"   - Dramatic memory reduction (H× smaller)\")\n",
    "print(\"   - Single shared K, V across all heads\")\n",
    "print(\"   - Slight quality degradation\")\n",
    "print(\"   - Fast inference due to smaller cache\")\n",
    "print()\n",
    "print(\"3. **GQA (Grouped Query Attention)**\")\n",
    "print(\"   - Best balance: quality vs. memory\")\n",
    "print(\"   - Industry standard for modern LLMs\")\n",
    "print(\"   - LLaMA 2: 32 Q heads, 8 KV heads (4× reduction)\")\n",
    "print(\"   - Mistral: Similar configuration\")\n",
    "print()\n",
    "print(\"4. **MLA (Multi-Head Latent Attention)**\")\n",
    "print(\"   - Lowest memory via compression\")\n",
    "print(\"   - ~90% KV cache reduction\")\n",
    "print(\"   - Adds compression/decompression overhead\")\n",
    "print(\"   - Cutting-edge research (DeepSeek-V2)\")\n",
    "print()\n",
    "\n",
    "# Practical recommendations\n",
    "print(\"\\n=== Practical Recommendations ===\")\n",
    "print()\n",
    "print(\"Use **GQA** for:\")\n",
    "print(\"  ✓ Production LLM deployments\")\n",
    "print(\"  ✓ Long context windows (32K+ tokens)\")\n",
    "print(\"  ✓ Memory-constrained environments\")\n",
    "print(\"  ✓ High-quality generation requirements\")\n",
    "print()\n",
    "print(\"Use **MQA** for:\")\n",
    "print(\"  ✓ Maximum speed requirements\")\n",
    "print(\"  ✓ Extreme memory constraints\")\n",
    "print(\"  ✓ Applications where quality loss is acceptable\")\n",
    "print()\n",
    "print(\"Use **MLA** for:\")\n",
    "print(\"  ✓ Research and experimentation\")\n",
    "print(\"  ✓ Ultra-long contexts (100K+ tokens)\")\n",
    "print(\"  ✓ When willing to trade computation for memory\")\n",
    "print()\n",
    "print(\"Use **MHA** for:\")\n",
    "print(\"  ✓ Small models where memory isn't critical\")\n",
    "print(\"  ✓ Research baselines\")\n",
    "print(\"  ✓ Maximum quality requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Multi-Head Attention (MHA)**\n",
    "   - Traditional approach with separate K, V for each head\n",
    "   - High quality but memory-intensive\n",
    "\n",
    "2. **Multi-Query Attention (MQA)**\n",
    "   - Single shared K, V across all heads\n",
    "   - Massive memory savings\n",
    "   - Used in PaLM, Falcon\n",
    "\n",
    "3. **Grouped Query Attention (GQA)**\n",
    "   - Middle ground: groups of queries share K, V\n",
    "   - Industry standard for modern LLMs\n",
    "   - **LLaMA 2, Mistral use GQA**\n",
    "\n",
    "4. **Multi-Head Latent Attention (MLA)**\n",
    "   - Low-rank compression of K, V\n",
    "   - Cutting-edge memory optimization\n",
    "   - Used in DeepSeek-V2\n",
    "\n",
    "5. **KV Cache**\n",
    "   - Essential for efficient autoregressive generation\n",
    "   - Stores K, V from previous tokens\n",
    "   - Reduces O(n²) to O(n) complexity\n",
    "\n",
    "### Memory Savings Example (32-layer model, 2K context)\n",
    "- MHA: ~4 GB KV cache\n",
    "- GQA: ~1 GB KV cache (4× reduction)\n",
    "- MQA: ~128 MB KV cache (32× reduction)\n",
    "- MLA: ~512 MB KV cache (8× reduction)\n",
    "\n",
    "### Production Best Practices\n",
    "1. **Use GQA** for most production deployments\n",
    "2. **Implement KV cache** for all autoregressive models\n",
    "3. **Monitor cache size** as context grows\n",
    "4. **Consider MQA** for edge deployments\n",
    "5. **Explore MLA** for extreme context lengths\n",
    "\n",
    "Understanding these attention mechanisms is critical for:\n",
    "- Building efficient LLM inference systems\n",
    "- Optimizing memory usage\n",
    "- Scaling to longer contexts\n",
    "- Making informed architecture decisions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
