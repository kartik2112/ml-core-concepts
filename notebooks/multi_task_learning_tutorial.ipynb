{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task Learning (MTL) with PyTorch: A Comprehensive Tutorial\n",
    "\n",
    "**Author**: MTL Tutorial Series  \n",
    "**Purpose**: Interview Preparation & Practical Implementation Guide\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [Introduction to Multi-Task Learning](#introduction)\n",
    "2. [Interview Questions & Answers](#interview-prep)\n",
    "3. [Implementation: Setup & Imports](#setup)\n",
    "4. [Dataset Preparation](#dataset)\n",
    "5. [Model Architecture](#architecture)\n",
    "6. [Training Loop](#training)\n",
    "7. [Evaluation & Visualization](#evaluation)\n",
    "8. [Best Practices](#best-practices)\n",
    "9. [Production Considerations](#production)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Multi-Task Learning <a name=\"introduction\"></a>\n",
    "\n",
    "### What is Multi-Task Learning?\n",
    "\n",
    "**Multi-Task Learning (MTL)** is a machine learning paradigm where a model is trained to perform multiple related tasks simultaneously. Instead of training separate models for each task, MTL leverages shared representations to improve generalization and efficiency.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Shared Encoder**: Common feature extractor used by all tasks\n",
    "2. **Task-Specific Heads**: Separate output layers for each task\n",
    "3. **Joint Loss**: Combined loss from all tasks guides training\n",
    "4. **Transfer Learning**: Knowledge from one task helps others\n",
    "\n",
    "### Architecture Diagram\n",
    "\n",
    "```\n",
    "                    Input (28x28 MNIST Image)\n",
    "                             |\n",
    "                             v\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ Shared Encoder ‚îÇ\n",
    "                    ‚îÇ   (Conv Layers)‚îÇ\n",
    "                    ‚îÇ   + Flatten    ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                             ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ              ‚îÇ              ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄv‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄv‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄv‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄv‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ Task 1  ‚îÇ    ‚îÇ Task 2 ‚îÇ    ‚îÇ Task 3  ‚îÇ    ‚îÇ   Task 4    ‚îÇ\n",
    "         ‚îÇ Head    ‚îÇ    ‚îÇ Head   ‚îÇ    ‚îÇ Head    ‚îÇ    ‚îÇ   Head      ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ             ‚îÇ              ‚îÇ                 ‚îÇ\n",
    "         Digit(0-9)    Even/Odd      Greater>4        Norm Value\n",
    "        (10 classes)   (2 classes)   (2 classes)      (Regression)\n",
    "```\n",
    "\n",
    "### Benefits of MTL\n",
    "\n",
    "- **Better Generalization**: Shared representations act as regularization\n",
    "- **Data Efficiency**: Tasks with more data help tasks with less data\n",
    "- **Faster Training**: One model instead of multiple\n",
    "- **Reduced Overfitting**: Implicit regularization from multiple objectives\n",
    "- **Resource Efficient**: Single inference pass for all tasks\n",
    "\n",
    "### Our Example: MNIST Multi-Task\n",
    "\n",
    "We'll train a single model to perform **4 tasks** on MNIST digits:\n",
    "\n",
    "1. **Task 1**: Digit Classification (0-9) ‚Äî 10-class classification\n",
    "2. **Task 2**: Even/Odd Classification ‚Äî Binary classification\n",
    "3. **Task 3**: Greater than 4? ‚Äî Binary classification  \n",
    "4. **Task 4**: Normalized Value ‚Äî Regression (0.0-1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interview Questions & Answers <a name=\"interview-prep\"></a>\n",
    "\n",
    "### Q1: What is Multi-Task Learning and why use it?\n",
    "\n",
    "**Answer**: Multi-Task Learning is a training paradigm where a single model learns to solve multiple related tasks simultaneously by sharing representations. We use it because:\n",
    "- Shared representations improve generalization (acts as inductive bias)\n",
    "- Data-efficient: tasks with more data help tasks with sparse data\n",
    "- Computationally efficient: one model serves multiple purposes\n",
    "- Implicit regularization reduces overfitting\n",
    "\n",
    "### Q2: What's the difference between MTL and Transfer Learning?\n",
    "\n",
    "**Answer**: \n",
    "- **Transfer Learning**: Train on Task A, then fine-tune on Task B (sequential)\n",
    "- **Multi-Task Learning**: Train on Tasks A & B simultaneously (parallel)\n",
    "- MTL learns shared representations optimized for all tasks jointly\n",
    "- Transfer learning adapts a pre-trained model to a new domain\n",
    "\n",
    "### Q3: How do you handle different loss scales in MTL?\n",
    "\n",
    "**Answer**: Several approaches:\n",
    "1. **Loss Weighting**: Manually tune weights (Œ±‚ÇÅ, Œ±‚ÇÇ, ...) for each loss\n",
    "2. **Uncertainty Weighting**: Learn task-dependent uncertainty parameters\n",
    "3. **GradNorm**: Balance gradient magnitudes across tasks\n",
    "4. **Dynamic Task Prioritization**: Adjust weights based on task difficulty\n",
    "5. **Normalization**: Scale losses to similar ranges\n",
    "\n",
    "In practice, start with equal weights and tune based on validation performance.\n",
    "\n",
    "### Q4: What are common challenges in MTL?\n",
    "\n",
    "**Answer**:\n",
    "1. **Negative Transfer**: Tasks hurt each other (choose related tasks)\n",
    "2. **Task Balancing**: Some tasks dominate training\n",
    "3. **Catastrophic Interference**: New task learning hurts old tasks\n",
    "4. **Architecture Design**: Finding optimal sharing structure\n",
    "5. **Hyperparameter Tuning**: More complex than single-task\n",
    "\n",
    "### Q5: When should you NOT use MTL?\n",
    "\n",
    "**Answer**: Avoid MTL when:\n",
    "- Tasks are unrelated (no benefit from shared representations)\n",
    "- One task has significantly more data (may dominate)\n",
    "- Tasks require very different architectures\n",
    "- Latency is critical (single-task models can be more optimized)\n",
    "- Tasks have conflicting objectives\n",
    "\n",
    "### Q6: How do you design the architecture (hard vs soft sharing)?\n",
    "\n",
    "**Answer**:\n",
    "- **Hard Parameter Sharing**: Shared encoder + task-specific heads (most common)\n",
    "  - Pros: Simple, parameter-efficient, strong regularization\n",
    "  - Cons: Less flexible, potential negative transfer\n",
    "- **Soft Parameter Sharing**: Separate networks with regularized similarity\n",
    "  - Pros: More flexible, tasks can diverge\n",
    "  - Cons: More parameters, complex training\n",
    "\n",
    "### Q7: How do you evaluate MTL models?\n",
    "\n",
    "**Answer**:\n",
    "1. **Per-Task Metrics**: Accuracy, F1, MSE for each task\n",
    "2. **Average Performance**: Mean across all tasks\n",
    "3. **Relative Improvement**: Compare to single-task baselines\n",
    "4. **Task Correlation**: Analyze which tasks help/hurt each other\n",
    "5. **Efficiency Metrics**: Parameters, FLOPs, latency vs single-task\n",
    "\n",
    "### Q8: What are practical applications of MTL?\n",
    "\n",
    "**Answer**: MTL is used in:\n",
    "- **Computer Vision**: Object detection + segmentation + depth estimation\n",
    "- **NLP**: Named entity recognition + POS tagging + sentiment\n",
    "- **Autonomous Driving**: Lane detection + object detection + steering prediction\n",
    "- **Recommendation Systems**: CTR + conversion + engagement prediction\n",
    "- **Medical Imaging**: Disease classification + severity grading + localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation: Setup & Imports <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "‚úì Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Preparation <a name=\"dataset\"></a>\n",
    "\n",
    "### Multi-Task Dataset Wrapper\n",
    "\n",
    "We create a custom dataset class that wraps MNIST and generates labels for all 4 tasks:\n",
    "1. Original digit label (0-9)\n",
    "2. Even/Odd label (0=even, 1=odd)\n",
    "3. Greater than 4 label (0=‚â§4, 1=>4)\n",
    "4. Normalized value (digit/9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 60000\n",
      "Test dataset size: 10000\n",
      "Number of batches (train): 469\n",
      "‚úì Dataset ready!\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskMNIST(Dataset):\n",
    "    \"\"\"Wrapper for MNIST that provides labels for all 4 tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist = mnist_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, digit_label = self.mnist[idx]\n",
    "        \n",
    "        # Task 1: Digit classification (0-9)\n",
    "        task1_label = digit_label\n",
    "        \n",
    "        # Task 2: Even/Odd classification (0=even, 1=odd)\n",
    "        task2_label = digit_label % 2\n",
    "        \n",
    "        # Task 3: Greater than 4? (0=no, 1=yes)\n",
    "        task3_label = 1 if digit_label > 4 else 0\n",
    "        \n",
    "        # Task 4: Normalized digit value (regression: 0.0 to 1.0)\n",
    "        task4_label = digit_label / 9.0\n",
    "        \n",
    "        return image, {\n",
    "            'task1': task1_label,      # Classification (10 classes)\n",
    "            'task2': task2_label,      # Binary classification\n",
    "            'task3': task3_label,      # Binary classification\n",
    "            'task4': task4_label       # Regression\n",
    "        }\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "# Load MNIST datasets\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Wrap with multi-task dataset\n",
    "train_dataset = MultiTaskMNIST(mnist_train)\n",
    "test_dataset = MultiTaskMNIST(mnist_test)\n",
    "\n",
    "# Create data loaders\n",
    "# Note: Set num_workers=0 if you encounter multiprocessing errors on Windows\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Number of batches (train): {len(train_loader)}\")\n",
    "print(\"‚úì Dataset ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Visualization complete!\n"
     ]
    }
   ],
   "source": [
    "# Visualize sample data with all task labels\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    image, labels = train_dataset[i]\n",
    "    axes[i].imshow(image.squeeze(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    title = f\"Digit: {labels['task1']}\\n\"\n",
    "    title += f\"Even/Odd: {'Odd' if labels['task2'] else 'Even'}\\n\"\n",
    "    title += f\">4: {'Yes' if labels['task3'] else 'No'}\\n\"\n",
    "    title += f\"Norm: {labels['task4']:.2f}\"\n",
    "    axes[i].set_title(title, fontsize=9)\n",
    "\n",
    "plt.suptitle('Sample Images with Multi-Task Labels', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture <a name=\"architecture\"></a>\n",
    "\n",
    "### Multi-Task Neural Network\n",
    "\n",
    "Our architecture consists of:\n",
    "1. **Shared Encoder**: Convolutional layers + pooling (feature extraction)\n",
    "2. **Task-Specific Heads**: Separate fully connected layers for each task\n",
    "\n",
    "#### Design Decisions:\n",
    "- **Shared layers**: Learn general digit representations\n",
    "- **Task heads**: Specialize for each task's output\n",
    "- **Batch Normalization**: Improves training stability\n",
    "- **Dropout**: Prevents overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "MultiTaskCNN(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Linear(in_features=3136, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (task1_head): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      "  (task2_head): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      "  (task3_head): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      "  (task4_head): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (4): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 441,999\n",
      "Trainable parameters: 441,999\n",
      "‚úì Model created!\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskCNN(nn.Module):\n",
    "    \"\"\"Multi-Task Learning model with shared encoder and task-specific heads.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super(MultiTaskCNN, self).__init__()\n",
    "        \n",
    "        # Shared Encoder: Convolutional layers for feature extraction\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Conv Block 1: 1 -> 32 channels\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 28x28 -> 14x14\n",
    "            \n",
    "            # Conv Block 2: 32 -> 64 channels\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 14x14 -> 7x7\n",
    "            \n",
    "            # Flatten: 64 * 7 * 7 = 3136\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # Shared FC layer\n",
    "            nn.Linear(64 * 7 * 7, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Task 1 Head: Digit Classification (10 classes)\n",
    "        self.task1_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "        # Task 2 Head: Even/Odd Classification (2 classes)\n",
    "        self.task2_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "        # Task 3 Head: Greater than 4 (2 classes)\n",
    "        self.task3_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "        # Task 4 Head: Normalized Value Regression (1 output)\n",
    "        self.task4_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Output in [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Shared feature extraction\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # Task-specific predictions\n",
    "        task1_out = self.task1_head(features)  # Logits for digit classification\n",
    "        task2_out = self.task2_head(features)  # Logits for even/odd\n",
    "        task3_out = self.task3_head(features)  # Logits for >4\n",
    "        task4_out = self.task4_head(features)  # Regression value\n",
    "        \n",
    "        return {\n",
    "            'task1': task1_out,\n",
    "            'task2': task2_out,\n",
    "            'task3': task3_out,\n",
    "            'task4': task4_out\n",
    "        }\n",
    "\n",
    "# Instantiate model\n",
    "model = MultiTaskCNN(hidden_dim=HIDDEN_DIM).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model Architecture:\\n{model}\")\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"‚úì Model created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop <a name=\"training\"></a>\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "We use appropriate loss functions for each task:\n",
    "- **Tasks 1, 2, 3**: CrossEntropyLoss (classification)\n",
    "- **Task 4**: MSELoss (regression)\n",
    "\n",
    "### Combined Loss\n",
    "\n",
    "Total Loss = Œ±‚ÇÅ¬∑L‚ÇÅ + Œ±‚ÇÇ¬∑L‚ÇÇ + Œ±‚ÇÉ¬∑L‚ÇÉ + Œ±‚ÇÑ¬∑L‚ÇÑ\n",
    "\n",
    "where Œ± values are task weights (we'll use equal weights for simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loss functions and optimizer configured!\n",
      "Task weights: {'task1': 1.0, 'task2': 1.0, 'task3': 1.0, 'task4': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Define loss functions\n",
    "criterion_task1 = nn.CrossEntropyLoss()  # Digit classification\n",
    "criterion_task2 = nn.CrossEntropyLoss()  # Even/odd\n",
    "criterion_task3 = nn.CrossEntropyLoss()  # >4\n",
    "criterion_task4 = nn.MSELoss()           # Normalized value\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler (optional but recommended)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Task weights (can be tuned)\n",
    "task_weights = {\n",
    "    'task1': 1.0,  # Digit classification\n",
    "    'task2': 1.0,  # Even/odd\n",
    "    'task3': 1.0,  # >4\n",
    "    'task4': 10.0  # Regression (scaled up since MSE is typically smaller)\n",
    "}\n",
    "\n",
    "print(\"‚úì Loss functions and optimizer configured!\")\n",
    "print(f\"Task weights: {task_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    epoch_losses = {'task1': 0, 'task2': 0, 'task3': 0, 'task4': 0, 'total': 0}\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc='Training', leave=False):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Move labels to device\n",
    "        task1_labels = labels['task1'].to(device)\n",
    "        task2_labels = labels['task2'].to(device)\n",
    "        task3_labels = labels['task3'].to(device)\n",
    "        task4_labels = labels['task4'].float().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Compute individual losses\n",
    "        loss1 = criterion_task1(outputs['task1'], task1_labels)\n",
    "        loss2 = criterion_task2(outputs['task2'], task2_labels)\n",
    "        loss3 = criterion_task3(outputs['task3'], task3_labels)\n",
    "        loss4 = criterion_task4(outputs['task4'].squeeze(), task4_labels)\n",
    "        \n",
    "        # Combined weighted loss\n",
    "        total_loss = (task_weights['task1'] * loss1 + \n",
    "                      task_weights['task2'] * loss2 + \n",
    "                      task_weights['task3'] * loss3 + \n",
    "                      task_weights['task4'] * loss4)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        epoch_losses['task1'] += loss1.item()\n",
    "        epoch_losses['task2'] += loss2.item()\n",
    "        epoch_losses['task3'] += loss3.item()\n",
    "        epoch_losses['task4'] += loss4.item()\n",
    "        epoch_losses['total'] += total_loss.item()\n",
    "    \n",
    "    # Average losses\n",
    "    for key in epoch_losses:\n",
    "        epoch_losses[key] /= len(train_loader)\n",
    "    \n",
    "    return epoch_losses\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_losses = {'task1': 0, 'task2': 0, 'task3': 0, 'task4': 0, 'total': 0}\n",
    "    \n",
    "    # Accuracy counters\n",
    "    correct = {'task1': 0, 'task2': 0, 'task3': 0}\n",
    "    total = 0\n",
    "    \n",
    "    # For storing predictions and labels (for confusion matrix)\n",
    "    all_preds = {'task1': [], 'task2': [], 'task3': []}\n",
    "    all_labels = {'task1': [], 'task2': [], 'task3': []}\n",
    "    task4_preds = []\n",
    "    task4_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Evaluating', leave=False):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            task1_labels = labels['task1'].to(device)\n",
    "            task2_labels = labels['task2'].to(device)\n",
    "            task3_labels = labels['task3'].to(device)\n",
    "            task4_labels_batch = labels['task4'].float().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute losses\n",
    "            loss1 = criterion_task1(outputs['task1'], task1_labels)\n",
    "            loss2 = criterion_task2(outputs['task2'], task2_labels)\n",
    "            loss3 = criterion_task3(outputs['task3'], task3_labels)\n",
    "            loss4 = criterion_task4(outputs['task4'].squeeze(), task4_labels_batch)\n",
    "            \n",
    "            total_loss = (task_weights['task1'] * loss1 + \n",
    "                          task_weights['task2'] * loss2 + \n",
    "                          task_weights['task3'] * loss3 + \n",
    "                          task_weights['task4'] * loss4)\n",
    "            \n",
    "            epoch_losses['task1'] += loss1.item()\n",
    "            epoch_losses['task2'] += loss2.item()\n",
    "            epoch_losses['task3'] += loss3.item()\n",
    "            epoch_losses['task4'] += loss4.item()\n",
    "            epoch_losses['total'] += total_loss.item()\n",
    "            \n",
    "            # Calculate accuracies\n",
    "            _, pred1 = torch.max(outputs['task1'], 1)\n",
    "            _, pred2 = torch.max(outputs['task2'], 1)\n",
    "            _, pred3 = torch.max(outputs['task3'], 1)\n",
    "            \n",
    "            correct['task1'] += (pred1 == task1_labels).sum().item()\n",
    "            correct['task2'] += (pred2 == task2_labels).sum().item()\n",
    "            correct['task3'] += (pred3 == task3_labels).sum().item()\n",
    "            total += task1_labels.size(0)\n",
    "            \n",
    "            # Store predictions for confusion matrix\n",
    "            all_preds['task1'].extend(pred1.cpu().numpy())\n",
    "            all_preds['task2'].extend(pred2.cpu().numpy())\n",
    "            all_preds['task3'].extend(pred3.cpu().numpy())\n",
    "            \n",
    "            all_labels['task1'].extend(task1_labels.cpu().numpy())\n",
    "            all_labels['task2'].extend(task2_labels.cpu().numpy())\n",
    "            all_labels['task3'].extend(task3_labels.cpu().numpy())\n",
    "            \n",
    "            # Store regression predictions\n",
    "            task4_preds.extend(outputs['task4'].squeeze().cpu().numpy())\n",
    "            task4_labels.extend(task4_labels_batch.cpu().numpy())\n",
    "    \n",
    "    # Average losses\n",
    "    for key in epoch_losses:\n",
    "        epoch_losses[key] /= len(test_loader)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    accuracies = {\n",
    "        'task1': 100 * correct['task1'] / total,\n",
    "        'task2': 100 * correct['task2'] / total,\n",
    "        'task3': 100 * correct['task3'] / total\n",
    "    }\n",
    "    \n",
    "    return epoch_losses, accuracies, all_preds, all_labels, task4_preds, task4_labels\n",
    "\n",
    "\n",
    "print(\"‚úì Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/469 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/kartikshenoy/anaconda3/envs/main/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kartikshenoy/anaconda3/envs/main/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MultiTaskMNIST' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/kartikshenoy/anaconda3/envs/main/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kartikshenoy/anaconda3/envs/main/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MultiTaskMNIST' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "                                                 \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 70649) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1133\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    112\u001b[39m timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/multiprocessing/connection.py:256\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/multiprocessing/connection.py:423\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/multiprocessing/connection.py:1118\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1117\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1118\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[39m, in \u001b[36m_set_SIGCHLD_handler.<locals>.handler\u001b[39m\u001b[34m(signum, frame)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandler\u001b[39m(signum, frame):\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid 70649) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m epoch_start = time.time()\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m train_losses = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     22\u001b[39m test_losses, accuracies, _, _, _, _ = evaluate(model, test_loader, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, device)\u001b[39m\n\u001b[32m      3\u001b[39m model.train()\n\u001b[32m      5\u001b[39m epoch_losses = {\u001b[33m'\u001b[39m\u001b[33mtask1\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtask2\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtask3\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtask4\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move labels to device\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1329\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1295\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1292\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1293\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1294\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1297\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1146\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1145\u001b[39m     pids_str = \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 70649) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'test_loss': [],\n",
    "    'task1_acc': [],\n",
    "    'task2_acc': [],\n",
    "    'task3_acc': []\n",
    "}\n",
    "\n",
    "best_test_acc = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_losses = train_epoch(model, train_loader, optimizer, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_losses, accuracies, _, _, _, _ = evaluate(model, test_loader, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_losses['total'])\n",
    "    history['test_loss'].append(test_losses['total'])\n",
    "    history['task1_acc'].append(accuracies['task1'])\n",
    "    history['task2_acc'].append(accuracies['task2'])\n",
    "    history['task3_acc'].append(accuracies['task3'])\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_time:.1f}s)\")\n",
    "    print(f\"  Train Loss: {train_losses['total']:.4f} | Test Loss: {test_losses['total']:.4f}\")\n",
    "    print(f\"  Task 1 (Digit) Acc: {accuracies['task1']:.2f}%\")\n",
    "    print(f\"  Task 2 (Even/Odd) Acc: {accuracies['task2']:.2f}%\")\n",
    "    print(f\"  Task 3 (>4) Acc: {accuracies['task3']:.2f}%\")\n",
    "    print()\n",
    "    \n",
    "    # Save best model\n",
    "    avg_acc = (accuracies['task1'] + accuracies['task2'] + accuracies['task3']) / 3\n",
    "    if avg_acc > best_test_acc:\n",
    "        best_test_acc = avg_acc\n",
    "        try:\n",
    "            torch.save(model.state_dict(), 'best_mtl_model.pth')\n",
    "            print(f\"  ‚Üí Saved checkpoint (avg acc: {best_test_acc:.2f}%)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not save checkpoint: {e}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n‚úì Training complete in {total_time:.1f}s!\")\n",
    "print(f\"Best average test accuracy: {best_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Visualization <a name=\"evaluation\"></a>\n",
    "\n",
    "Let's visualize the training curves and evaluate the model comprehensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['test_loss'], label='Test Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Test Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history['task1_acc'], label='Task 1 (Digit)', marker='o')\n",
    "axes[1].plot(history['task2_acc'], label='Task 2 (Even/Odd)', marker='s')\n",
    "axes[1].plot(history['task3_acc'], label='Task 3 (>4)', marker='^')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Task Accuracies')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Training curves plotted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "import os\n",
    "if os.path.exists('best_mtl_model.pth'):\n",
    "    model.load_state_dict(torch.load('best_mtl_model.pth'))\n",
    "    print(\"Loaded best model checkpoint\\n\")\n",
    "else:\n",
    "    print(\"Warning: No checkpoint found. Using current model state.\\n\")\n",
    "    print(\"Make sure you've run the training cell above first!\\n\")\n",
    "\n",
    "test_losses, accuracies, all_preds, all_labels, task4_preds, task4_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTask 1 (Digit Classification):     {accuracies['task1']:.2f}%\")\n",
    "print(f\"Task 2 (Even/Odd Classification):  {accuracies['task2']:.2f}%\")\n",
    "print(f\"Task 3 (>4 Classification):        {accuracies['task3']:.2f}%\")\n",
    "\n",
    "# Regression metrics for Task 4\n",
    "task4_mae = np.mean(np.abs(np.array(task4_preds) - np.array(task4_labels)))\n",
    "task4_rmse = np.sqrt(np.mean((np.array(task4_preds) - np.array(task4_labels))**2))\n",
    "print(f\"\\nTask 4 (Regression):\")\n",
    "print(f\"  MAE:  {task4_mae:.4f}\")\n",
    "print(f\"  RMSE: {task4_rmse:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for classification tasks\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Task 1: Digit Classification\n",
    "cm1 = confusion_matrix(all_labels['task1'], all_preds['task1'])\n",
    "sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', ax=axes[0], cbar=False)\n",
    "axes[0].set_title('Task 1: Digit Classification')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# Task 2: Even/Odd\n",
    "cm2 = confusion_matrix(all_labels['task2'], all_preds['task2'])\n",
    "sns.heatmap(cm2, annot=True, fmt='d', cmap='Greens', ax=axes[1], \n",
    "            xticklabels=['Even', 'Odd'], yticklabels=['Even', 'Odd'], cbar=False)\n",
    "axes[1].set_title('Task 2: Even/Odd Classification')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "# Task 3: Greater than 4\n",
    "cm3 = confusion_matrix(all_labels['task3'], all_preds['task3'])\n",
    "sns.heatmap(cm3, annot=True, fmt='d', cmap='Oranges', ax=axes[2], \n",
    "            xticklabels=['‚â§4', '>4'], yticklabels=['‚â§4', '>4'], cbar=False)\n",
    "axes[2].set_title('Task 3: Greater than 4')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Confusion matrices plotted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Regression visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot: Predicted vs True\n",
    "axes[0].scatter(task4_labels, task4_preds, alpha=0.3, s=10)\n",
    "axes[0].plot([0, 1], [0, 1], 'r--', label='Perfect Prediction')\n",
    "axes[0].set_xlabel('True Normalized Value')\n",
    "axes[0].set_ylabel('Predicted Normalized Value')\n",
    "axes[0].set_title('Task 4: Regression Performance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "errors = np.array(task4_preds) - np.array(task4_labels)\n",
    "axes[1].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[1].set_xlabel('Prediction Error')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Task 4: Error Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Regression analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions\n",
    "model.eval()\n",
    "fig, axes = plt.subplots(2, 5, figsize=(16, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        image, labels = test_dataset[i]\n",
    "        image_tensor = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(image_tensor)\n",
    "        \n",
    "        pred1 = torch.argmax(outputs['task1'], dim=1).item()\n",
    "        pred2 = torch.argmax(outputs['task2'], dim=1).item()\n",
    "        pred3 = torch.argmax(outputs['task3'], dim=1).item()\n",
    "        pred4 = outputs['task4'].squeeze().item()\n",
    "        \n",
    "        # Display\n",
    "        axes[i].imshow(image.squeeze(), cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        title = f\"True: {labels['task1']} | Pred: {pred1}\\n\"\n",
    "        title += f\"E/O: {labels['task2']}/{pred2}\\n\"\n",
    "        title += f\">4: {labels['task3']}/{pred3}\\n\"\n",
    "        title += f\"Norm: {labels['task4']:.2f}/{pred4:.2f}\"\n",
    "        \n",
    "        # Color based on correctness\n",
    "        color = 'green' if pred1 == labels['task1'] else 'red'\n",
    "        axes[i].set_title(title, fontsize=9, color=color)\n",
    "\n",
    "plt.suptitle('Sample Predictions (Green=Correct, Red=Wrong)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Sample predictions visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices <a name=\"best-practices\"></a>\n",
    "\n",
    "### 1. Task Selection\n",
    "- Choose **related tasks** that can benefit from shared representations\n",
    "- Ensure tasks have some **common structure** or **domain**\n",
    "- Avoid tasks with conflicting objectives\n",
    "\n",
    "### 2. Architecture Design\n",
    "- Start with **hard parameter sharing** (shared encoder + task heads)\n",
    "- Use **deeper shared layers** for low-level features\n",
    "- Keep task-specific heads **relatively shallow**\n",
    "- Consider **task similarity** when deciding sharing depth\n",
    "\n",
    "### 3. Loss Balancing\n",
    "- Start with **equal weights**, then tune based on validation\n",
    "- **Normalize losses** to similar scales (especially regression vs classification)\n",
    "- Consider **uncertainty weighting** for automatic balancing\n",
    "- Monitor **per-task losses** during training\n",
    "\n",
    "### 4. Training Strategy\n",
    "- Use **batch normalization** for stable training\n",
    "- Apply **dropout** to prevent overfitting\n",
    "- Consider **gradient clipping** if training is unstable\n",
    "- Use **learning rate scheduling**\n",
    "\n",
    "### 5. Evaluation\n",
    "- Evaluate **each task separately**\n",
    "- Compare to **single-task baselines**\n",
    "- Check for **negative transfer** (MTL worse than single-task)\n",
    "- Analyze **task correlations**\n",
    "\n",
    "### 6. Debugging\n",
    "- If one task dominates: adjust loss weights\n",
    "- If tasks interfere: reduce shared capacity or use soft sharing\n",
    "- If training unstable: check loss scales, add normalization\n",
    "- Visualize **gradient magnitudes** across tasks\n",
    "\n",
    "### 7. Advanced Techniques\n",
    "- **GradNorm**: Balance gradient magnitudes\n",
    "- **Uncertainty Weighting**: Learn task weights automatically\n",
    "- **Task-Specific Batch Normalization**: Separate BN for each task\n",
    "- **Cross-Stitch Networks**: Learn task sharing patterns\n",
    "- **Progressive Neural Networks**: Add capacity per task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production Considerations <a name=\"production\"></a>\n",
    "\n",
    "### Deployment Strategies\n",
    "\n",
    "#### 1. Model Serving\n",
    "```python\n",
    "# Example: TorchScript for production\n",
    "model.eval()\n",
    "example_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "traced_model.save('mtl_model_traced.pt')\n",
    "```\n",
    "\n",
    "#### 2. Task Selection at Inference\n",
    "```python\n",
    "# Allow clients to request specific tasks\n",
    "def predict(image, tasks=['task1', 'task2', 'task3', 'task4']):\n",
    "    outputs = model(image)\n",
    "    return {task: outputs[task] for task in tasks}\n",
    "```\n",
    "\n",
    "#### 3. Monitoring\n",
    "- Track **per-task accuracy** in production\n",
    "- Monitor **inference latency**\n",
    "- Log **task usage patterns**\n",
    "- Watch for **distribution shift** per task\n",
    "\n",
    "### Optimization\n",
    "\n",
    "#### Model Compression\n",
    "- **Quantization**: Reduce precision (FP32 ‚Üí INT8)\n",
    "- **Pruning**: Remove unnecessary connections\n",
    "- **Knowledge Distillation**: Train smaller student model\n",
    "\n",
    "#### Inference Optimization\n",
    "- **Batch inference**: Process multiple images together\n",
    "- **ONNX Runtime**: Cross-platform deployment\n",
    "- **TensorRT**: GPU optimization\n",
    "- **Edge deployment**: Mobile/IoT optimization\n",
    "\n",
    "### A/B Testing\n",
    "- Compare MTL vs single-task models\n",
    "- Test different task weight configurations\n",
    "- Measure business metrics, not just accuracy\n",
    "\n",
    "### Maintenance\n",
    "- Regularly **retrain** with new data\n",
    "- **Version control** for models and task definitions\n",
    "- Maintain **rollback capability**\n",
    "- Document **task dependencies**\n",
    "\n",
    "### When to Use MTL in Production\n",
    "\n",
    "‚úÖ **Use MTL when:**\n",
    "- Multiple related predictions are needed\n",
    "- Latency budget allows single forward pass\n",
    "- Tasks share common features\n",
    "- Resource efficiency is important\n",
    "- Tasks benefit from joint training\n",
    "\n",
    "‚ùå **Avoid MTL when:**\n",
    "- Tasks are completely unrelated\n",
    "- Different update frequencies per task\n",
    "- Critical tasks need maximum accuracy\n",
    "- Tasks have very different input requirements\n",
    "- Debugging complexity is unacceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've covered:\n",
    "\n",
    "1. ‚úÖ **Multi-Task Learning Fundamentals**\n",
    "   - Concepts and benefits\n",
    "   - Architecture design\n",
    "   - Interview questions\n",
    "\n",
    "2. ‚úÖ **Complete Implementation**\n",
    "   - Custom multi-task dataset\n",
    "   - Shared encoder with task-specific heads\n",
    "   - Combined loss training\n",
    "   - Comprehensive evaluation\n",
    "\n",
    "3. ‚úÖ **4 Different Tasks**\n",
    "   - Digit classification (10 classes)\n",
    "   - Even/Odd classification (binary)\n",
    "   - Greater than 4 classification (binary)\n",
    "   - Normalized value regression (continuous)\n",
    "\n",
    "4. ‚úÖ **Best Practices**\n",
    "   - Loss balancing strategies\n",
    "   - Training tips\n",
    "   - Debugging techniques\n",
    "   - Production considerations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To deepen your understanding:\n",
    "1. Experiment with different **task weight combinations**\n",
    "2. Try **different architectures** (ResNet, deeper networks)\n",
    "3. Implement **uncertainty weighting** or **GradNorm**\n",
    "4. Apply MTL to your own **domain-specific tasks**\n",
    "5. Compare **MTL vs single-task** baselines quantitatively\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Paper**: \"An Overview of Multi-Task Learning in Deep Neural Networks\" (Ruder, 2017)\n",
    "- **Paper**: \"Multi-Task Learning Using Uncertainty to Weigh Losses\" (Kendall et al., 2018)\n",
    "- **PyTorch Docs**: https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your interviews! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
