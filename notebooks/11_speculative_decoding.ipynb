{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Decoding: Fast LLM Inference\n",
    "\n",
    "This notebook explores speculative decoding, a technique for accelerating autoregressive LLM inference.\n",
    "\n",
    "We'll cover:\n",
    "1. **Problem**: Why is LLM generation slow?\n",
    "2. **Solution**: Speculative decoding algorithm\n",
    "3. **Implementation**: Draft model + target model\n",
    "4. **Performance analysis**: Speedup calculations\n",
    "5. **Variants**: Self-speculative, multi-token prediction\n",
    "\n",
    "## The Problem: Memory-Bound Generation\n",
    "\n",
    "LLM generation is slow because:\n",
    "- Each token requires full model forward pass\n",
    "- Memory bandwidth limited (loading weights)\n",
    "- GPU underutilized (low arithmetic intensity)\n",
    "- Can't parallelize across sequence (autoregressive)\n",
    "\n",
    "**Key insight**: Generate multiple tokens in parallel, accept if correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standard Autoregressive Decoding (Baseline)\n",
    "\n",
    "Traditional approach: Generate one token at a time\n",
    "- Run model forward pass\n",
    "- Sample next token\n",
    "- Append to sequence\n",
    "- Repeat\n",
    "\n",
    "**Complexity**: O(n) forward passes for n tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple language model for demonstration.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead=8, dim_feedforward=d_model*4, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Token indices (batch_size, seq_len)\n",
    "        Returns:\n",
    "            Logits (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)\n",
    "        x = self.transformer(x)\n",
    "        return self.output(x)\n",
    "\n",
    "def standard_decoding(model, prompt_tokens, max_new_tokens, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Standard autoregressive decoding (one token at a time).\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        prompt_tokens: Initial tokens (batch_size, prompt_len)\n",
    "        max_new_tokens: Number of tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Generated tokens, number of forward passes\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = prompt_tokens.clone()\n",
    "    num_forward_passes = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Forward pass through entire sequence\n",
    "            logits = model(generated)\n",
    "            num_forward_passes += 1\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    return generated, num_forward_passes\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 1000\n",
    "d_model = 256\n",
    "model = SimpleLanguageModel(vocab_size, d_model, num_layers=4)\n",
    "\n",
    "prompt = torch.randint(0, vocab_size, (1, 5))  # Batch=1, prompt_len=5\n",
    "generated, num_passes = standard_decoding(model, prompt, max_new_tokens=10)\n",
    "\n",
    "print(f\"Standard Decoding:\")\n",
    "print(f\"  Prompt length: {prompt.shape[1]}\")\n",
    "print(f\"  Generated length: {generated.shape[1]}\")\n",
    "print(f\"  New tokens: {generated.shape[1] - prompt.shape[1]}\")\n",
    "print(f\"  Forward passes: {num_passes}\")\n",
    "print(f\"  Tokens per pass: {(generated.shape[1] - prompt.shape[1]) / num_passes:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Speculative Decoding Algorithm\n",
    "\n",
    "**Key Idea:**\n",
    "1. Use small **draft model** to quickly generate K candidate tokens\n",
    "2. Verify all K tokens in parallel with **target model** (one forward pass)\n",
    "3. Accept tokens where draft and target agree\n",
    "4. Reject first mismatch and resample from target\n",
    "\n",
    "**Benefits:**\n",
    "- Generate multiple tokens per target model forward pass\n",
    "- Mathematically equivalent to standard decoding (same distribution)\n",
    "- Speedup: 2-3× typical, up to 5× possible\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "1. Generate K draft tokens: x₁, x₂, ..., xₖ (fast, draft model)\n",
    "2. Get target model probabilities: p_target(x₁), p_target(x₂|x₁), ...\n",
    "3. Get draft model probabilities: p_draft(x₁), p_draft(x₂|x₁), ...\n",
    "4. For each position i:\n",
    "   - Accept xᵢ with probability min(1, p_target(xᵢ) / p_draft(xᵢ))\n",
    "   - If rejected, resample from adjusted distribution and stop\n",
    "5. If all accepted, sample one bonus token from target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_decoding(target_model, draft_model, prompt_tokens, \n",
    "                         max_new_tokens, k=4, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Speculative decoding with draft model.\n",
    "    \n",
    "    Args:\n",
    "        target_model: Large, high-quality model\n",
    "        draft_model: Small, fast model\n",
    "        prompt_tokens: Initial tokens\n",
    "        max_new_tokens: Number of tokens to generate\n",
    "        k: Number of speculative tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Generated tokens, stats dict\n",
    "    \"\"\"\n",
    "    target_model.eval()\n",
    "    draft_model.eval()\n",
    "    \n",
    "    generated = prompt_tokens.clone()\n",
    "    num_target_forward = 0\n",
    "    num_draft_forward = 0\n",
    "    num_accepted = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while (generated.shape[1] - prompt_tokens.shape[1]) < max_new_tokens:\n",
    "            # Step 1: Draft model generates K speculative tokens\n",
    "            draft_tokens = generated.clone()\n",
    "            draft_probs_list = []\n",
    "            \n",
    "            for _ in range(k):\n",
    "                logits = draft_model(draft_tokens)\n",
    "                num_draft_forward += 1\n",
    "                \n",
    "                next_token_logits = logits[:, -1, :] / temperature\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                draft_probs_list.append(probs)\n",
    "                draft_tokens = torch.cat([draft_tokens, next_token], dim=1)\n",
    "            \n",
    "            # Step 2: Target model verifies all K tokens in parallel\n",
    "            target_logits = target_model(draft_tokens)\n",
    "            num_target_forward += 1\n",
    "            \n",
    "            # Step 3: Verification - check which tokens to accept\n",
    "            accepted = 0\n",
    "            for i in range(k):\n",
    "                draft_token = draft_tokens[:, generated.shape[1] + i]\n",
    "                \n",
    "                # Get target probability for this token\n",
    "                target_logits_at_pos = target_logits[:, generated.shape[1] + i - 1, :]\n",
    "                target_probs = F.softmax(target_logits_at_pos / temperature, dim=-1)\n",
    "                \n",
    "                # Get draft probability\n",
    "                draft_probs = draft_probs_list[i]\n",
    "                \n",
    "                # Acceptance criterion: min(1, p_target / p_draft)\n",
    "                target_prob = target_probs[0, draft_token.item()]\n",
    "                draft_prob = draft_probs[0, draft_token.item()]\n",
    "                acceptance_prob = min(1.0, (target_prob / (draft_prob + 1e-10)).item())\n",
    "                \n",
    "                # Accept or reject\n",
    "                if torch.rand(1).item() < acceptance_prob:\n",
    "                    generated = torch.cat([generated, draft_token.unsqueeze(0)], dim=1)\n",
    "                    accepted += 1\n",
    "                else:\n",
    "                    # Rejection: resample from adjusted distribution\n",
    "                    adjusted_probs = torch.clamp(target_probs - draft_probs, min=0)\n",
    "                    adjusted_probs = adjusted_probs / (adjusted_probs.sum() + 1e-10)\n",
    "                    resampled_token = torch.multinomial(adjusted_probs, num_samples=1)\n",
    "                    generated = torch.cat([generated, resampled_token], dim=1)\n",
    "                    break\n",
    "            \n",
    "            # If all tokens accepted, sample bonus token from target\n",
    "            if accepted == k:\n",
    "                bonus_logits = target_logits[:, -1, :] / temperature\n",
    "                bonus_probs = F.softmax(bonus_logits, dim=-1)\n",
    "                bonus_token = torch.multinomial(bonus_probs, num_samples=1)\n",
    "                generated = torch.cat([generated, bonus_token], dim=1)\n",
    "                accepted += 1\n",
    "            \n",
    "            num_accepted.append(accepted)\n",
    "            \n",
    "            # Check if we've generated enough\n",
    "            if (generated.shape[1] - prompt_tokens.shape[1]) >= max_new_tokens:\n",
    "                break\n",
    "    \n",
    "    stats = {\n",
    "        'num_target_forward': num_target_forward,\n",
    "        'num_draft_forward': num_draft_forward,\n",
    "        'num_accepted_per_round': num_accepted,\n",
    "        'avg_accepted': np.mean(num_accepted) if num_accepted else 0,\n",
    "        'tokens_per_target_forward': (generated.shape[1] - prompt_tokens.shape[1]) / num_target_forward\n",
    "    }\n",
    "    \n",
    "    return generated[:, :prompt_tokens.shape[1] + max_new_tokens], stats\n",
    "\n",
    "# Create draft model (smaller, faster)\n",
    "draft_model = SimpleLanguageModel(vocab_size, d_model=128, num_layers=2)\n",
    "\n",
    "# Run speculative decoding\n",
    "prompt = torch.randint(0, vocab_size, (1, 5))\n",
    "generated_spec, stats = speculative_decoding(\n",
    "    target_model=model,\n",
    "    draft_model=draft_model,\n",
    "    prompt_tokens=prompt,\n",
    "    max_new_tokens=10,\n",
    "    k=4\n",
    ")\n",
    "\n",
    "print(f\"\\nSpeculative Decoding:\")\n",
    "print(f\"  Prompt length: {prompt.shape[1]}\")\n",
    "print(f\"  Generated length: {generated_spec.shape[1]}\")\n",
    "print(f\"  New tokens: {generated_spec.shape[1] - prompt.shape[1]}\")\n",
    "print(f\"  Target forward passes: {stats['num_target_forward']}\")\n",
    "print(f\"  Draft forward passes: {stats['num_draft_forward']}\")\n",
    "print(f\"  Avg tokens accepted per round: {stats['avg_accepted']:.2f}\")\n",
    "print(f\"  Tokens per target forward: {stats['tokens_per_target_forward']:.2f}\")\n",
    "print(f\"  Speedup vs standard: {10 / stats['num_target_forward']:.2f}×\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Acceptance Rate Analysis\n",
    "\n",
    "The speedup depends on the **acceptance rate**: how often draft model matches target model.\n",
    "\n",
    "**Factors affecting acceptance:**\n",
    "- Draft model quality (larger draft → higher acceptance)\n",
    "- Task difficulty (easier tasks → higher acceptance)\n",
    "- Temperature (higher temperature → easier to match)\n",
    "- K value (more speculation → lower per-token acceptance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_acceptance_rate(target_model, draft_models, prompt, k_values=[2, 4, 8]):\n",
    "    \"\"\"\n",
    "    Analyze how acceptance rate varies with different configurations.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Test different K values\n",
    "    print(\"=== Acceptance Rate Analysis ===\")\n",
    "    print()\n",
    "    \n",
    "    for k in k_values:\n",
    "        for model_name, draft_model in draft_models.items():\n",
    "            _, stats = speculative_decoding(\n",
    "                target_model=target_model,\n",
    "                draft_model=draft_model,\n",
    "                prompt_tokens=prompt,\n",
    "                max_new_tokens=20,\n",
    "                k=k\n",
    "            )\n",
    "            \n",
    "            speedup = 20 / stats['num_target_forward']\n",
    "            results.append({\n",
    "                'k': k,\n",
    "                'model': model_name,\n",
    "                'avg_accepted': stats['avg_accepted'],\n",
    "                'speedup': speedup\n",
    "            })\n",
    "            \n",
    "            print(f\"K={k}, Model={model_name}:\")\n",
    "            print(f\"  Avg accepted: {stats['avg_accepted']:.2f} / {k}\")\n",
    "            print(f\"  Acceptance rate: {stats['avg_accepted']/k*100:.1f}%\")\n",
    "            print(f\"  Speedup: {speedup:.2f}×\")\n",
    "            print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create draft models of different sizes\n",
    "draft_models = {\n",
    "    'Tiny (d=64, L=1)': SimpleLanguageModel(vocab_size, d_model=64, num_layers=1),\n",
    "    'Small (d=128, L=2)': SimpleLanguageModel(vocab_size, d_model=128, num_layers=2),\n",
    "    'Medium (d=192, L=3)': SimpleLanguageModel(vocab_size, d_model=192, num_layers=3)\n",
    "}\n",
    "\n",
    "prompt = torch.randint(0, vocab_size, (1, 10))\n",
    "results = analyze_acceptance_rate(model, draft_models, prompt, k_values=[2, 4, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Visualization\n",
    "\n",
    "Visualize the trade-offs between K, acceptance rate, and speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize acceptance rate and speedup\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Average accepted tokens\n",
    "ax = axes[0]\n",
    "for model_name in df['model'].unique():\n",
    "    model_data = df[df['model'] == model_name]\n",
    "    ax.plot(model_data['k'], model_data['avg_accepted'], marker='o', label=model_name, linewidth=2)\n",
    "\n",
    "# Add diagonal line showing k value\n",
    "k_vals = df['k'].unique()\n",
    "ax.plot(k_vals, k_vals, 'k--', alpha=0.3, label='Perfect acceptance (K tokens)')\n",
    "\n",
    "ax.set_xlabel('K (number of speculative tokens)', fontsize=11)\n",
    "ax.set_ylabel('Average tokens accepted per round', fontsize=11)\n",
    "ax.set_title('Acceptance Rate vs K', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Speedup\n",
    "ax = axes[1]\n",
    "for model_name in df['model'].unique():\n",
    "    model_data = df[df['model'] == model_name]\n",
    "    ax.plot(model_data['k'], model_data['speedup'], marker='o', label=model_name, linewidth=2)\n",
    "\n",
    "ax.axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='No speedup')\n",
    "ax.set_xlabel('K (number of speculative tokens)', fontsize=11)\n",
    "ax.set_ylabel('Speedup factor', fontsize=11)\n",
    "ax.set_title('Speedup vs K', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"1. Larger draft models → higher acceptance rate\")\n",
    "print(\"2. Higher K → more speculation but lower per-token acceptance\")\n",
    "print(\"3. Optimal K depends on draft model quality\")\n",
    "print(\"4. Typical speedup: 2-3× for well-matched draft model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Speedup Calculation\n",
    "\n",
    "Theoretical speedup analysis:\n",
    "\n",
    "**Let:**\n",
    "- α = acceptance rate (probability draft token accepted)\n",
    "- K = number of speculative tokens\n",
    "- T_draft = time for draft model forward pass\n",
    "- T_target = time for target model forward pass\n",
    "\n",
    "**Expected tokens per round:**\n",
    "$$E[\\text{tokens}] = \\sum_{i=0}^{K-1} (i+1) \\cdot \\alpha^i (1-\\alpha) + (K+1) \\cdot \\alpha^K$$\n",
    "\n",
    "**Speedup (ignoring draft cost):**\n",
    "$$\\text{Speedup} \\approx E[\\text{tokens}] = \\frac{1 - \\alpha^{K+1}}{1 - \\alpha}$$\n",
    "\n",
    "**With draft cost:**\n",
    "$$\\text{Speedup} = \\frac{E[\\text{tokens}]}{1 + K \\cdot (T_{\\text{draft}} / T_{\\text{target}})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_speedup(alpha, k, draft_cost_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Calculate theoretical speedup for speculative decoding.\n",
    "    \n",
    "    Args:\n",
    "        alpha: Acceptance rate (0 to 1)\n",
    "        k: Number of speculative tokens\n",
    "        draft_cost_ratio: T_draft / T_target\n",
    "    \n",
    "    Returns:\n",
    "        Expected speedup\n",
    "    \"\"\"\n",
    "    # Expected tokens accepted per round\n",
    "    if alpha == 1.0:\n",
    "        expected_tokens = k + 1\n",
    "    else:\n",
    "        expected_tokens = (1 - alpha**(k+1)) / (1 - alpha)\n",
    "    \n",
    "    # Time cost per round (normalized to T_target = 1)\n",
    "    time_per_round = 1 + k * draft_cost_ratio\n",
    "    \n",
    "    # Speedup = tokens per unit time\n",
    "    speedup = expected_tokens / time_per_round\n",
    "    \n",
    "    return speedup, expected_tokens\n",
    "\n",
    "# Visualize speedup surface\n",
    "alphas = np.linspace(0.1, 0.95, 50)\n",
    "ks = [2, 4, 6, 8, 10]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Speedup vs acceptance rate\n",
    "plt.subplot(1, 2, 1)\n",
    "for k in ks:\n",
    "    speedups = [theoretical_speedup(alpha, k)[0] for alpha in alphas]\n",
    "    plt.plot(alphas, speedups, label=f'K={k}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Acceptance Rate (α)', fontsize=11)\n",
    "plt.ylabel('Speedup Factor', fontsize=11)\n",
    "plt.title('Theoretical Speedup vs Acceptance Rate', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Plot 2: Expected tokens vs acceptance rate\n",
    "plt.subplot(1, 2, 2)\n",
    "for k in ks:\n",
    "    expected = [theoretical_speedup(alpha, k)[1] for alpha in alphas]\n",
    "    plt.plot(alphas, expected, label=f'K={k}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Acceptance Rate (α)', fontsize=11)\n",
    "plt.ylabel('Expected Tokens Per Round', fontsize=11)\n",
    "plt.title('Expected Tokens Accepted Per Round', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some example speedups\n",
    "print(\"\\n=== Example Speedup Calculations ===\")\n",
    "print()\n",
    "for alpha in [0.5, 0.7, 0.9]:\n",
    "    print(f\"Acceptance rate: {alpha*100:.0f}%\")\n",
    "    for k in [4, 8]:\n",
    "        speedup, expected = theoretical_speedup(alpha, k, draft_cost_ratio=0.1)\n",
    "        print(f\"  K={k}: {speedup:.2f}× speedup ({expected:.2f} tokens/round)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Variants of Speculative Decoding\n",
    "\n",
    "### 7.1 Self-Speculative Decoding\n",
    "- Use **same model** for draft and target\n",
    "- Draft uses fewer layers (early exit)\n",
    "- No need for separate draft model\n",
    "\n",
    "### 7.2 Multi-Token Prediction\n",
    "- Train model to predict multiple future tokens\n",
    "- Use predictions as draft candidates\n",
    "- No separate draft model needed\n",
    "\n",
    "### 7.3 Staged Speculative Decoding\n",
    "- Multiple draft models of increasing size\n",
    "- Filter candidates through stages\n",
    "- Better quality-speed trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfSpeculativeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model with early exit for self-speculative decoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_layers=6, draft_layers=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.draft_layers = draft_layers\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model, nhead=8, dim_feedforward=d_model*4, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output heads\n",
    "        self.draft_output = nn.Linear(d_model, vocab_size)\n",
    "        self.final_output = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x, early_exit=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Token indices\n",
    "            early_exit: If True, exit after draft_layers\n",
    "        \"\"\"\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)\n",
    "        \n",
    "        # Run through layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            \n",
    "            # Early exit for draft\n",
    "            if early_exit and i == self.draft_layers - 1:\n",
    "                return self.draft_output(x)\n",
    "        \n",
    "        return self.final_output(x)\n",
    "\n",
    "# Example\n",
    "self_spec_model = SelfSpeculativeModel(vocab_size, d_model=256, num_layers=6, draft_layers=3)\n",
    "\n",
    "x = torch.randint(0, vocab_size, (1, 10))\n",
    "draft_output = self_spec_model(x, early_exit=True)\n",
    "full_output = self_spec_model(x, early_exit=False)\n",
    "\n",
    "print(\"Self-Speculative Model:\")\n",
    "print(f\"  Total layers: {self_spec_model.num_layers}\")\n",
    "print(f\"  Draft layers: {self_spec_model.draft_layers}\")\n",
    "print(f\"  Draft output shape: {draft_output.shape}\")\n",
    "print(f\"  Full output shape: {full_output.shape}\")\n",
    "print(f\"\\nBenefit: ~{self_spec_model.draft_layers / self_spec_model.num_layers * 100:.0f}% computation for draft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Considerations\n",
    "\n",
    "### When to Use Speculative Decoding\n",
    "\n",
    "**Best for:**\n",
    "- Long-form generation (stories, articles, code)\n",
    "- High latency requirements\n",
    "- Memory-bound inference\n",
    "- When draft model available\n",
    "\n",
    "**Not ideal for:**\n",
    "- Very short generations (< 20 tokens)\n",
    "- Compute-bound scenarios\n",
    "- When draft model too slow or unavailable\n",
    "\n",
    "### Practical Implementation Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Speculative Decoding: Production Guide ===\")\n",
    "print()\n",
    "\n",
    "print(\"1. Draft Model Selection:\")\n",
    "print(\"   - Size: 5-20% of target model parameters\")\n",
    "print(\"   - Examples:\")\n",
    "print(\"     • Target: LLaMA 70B → Draft: LLaMA 7B (10%)\")\n",
    "print(\"     • Target: GPT-3.5 175B → Draft: GPT-3.5 6.7B (4%)\")\n",
    "print(\"   - Same architecture helps (shared tokenizer, similar outputs)\")\n",
    "print()\n",
    "\n",
    "print(\"2. Optimal K Selection:\")\n",
    "print(\"   - Typical range: K = 4 to 8\")\n",
    "print(\"   - Higher K if:\")\n",
    "print(\"     • Draft model very good\")\n",
    "print(\"     • Draft model very fast\")\n",
    "print(\"     • Easy task (high acceptance)\")\n",
    "print(\"   - Lower K if:\")\n",
    "print(\"     • Draft model slower\")\n",
    "print(\"     • Low acceptance rate\")\n",
    "print(\"     • Need low latency\")\n",
    "print()\n",
    "\n",
    "print(\"3. Memory Considerations:\")\n",
    "print(\"   - Need both models in memory\")\n",
    "print(\"   - Can share KV cache structure\")\n",
    "print(\"   - Consider model quantization (draft in INT8)\")\n",
    "print()\n",
    "\n",
    "print(\"4. Performance Tuning:\")\n",
    "print(\"   - Measure acceptance rate in production\")\n",
    "print(\"   - Adjust K based on observed acceptance\")\n",
    "print(\"   - A/B test different draft models\")\n",
    "print(\"   - Monitor latency vs. throughput\")\n",
    "print()\n",
    "\n",
    "print(\"5. Expected Real-World Speedups:\")\n",
    "print(\"   - Code generation: 2.5-3.5×\")\n",
    "print(\"   - Story writing: 2.0-3.0×\")\n",
    "print(\"   - Chat responses: 1.5-2.5×\")\n",
    "print(\"   - Technical Q&A: 1.8-2.8×\")\n",
    "print()\n",
    "\n",
    "print(\"6. Implementation Checklist:\")\n",
    "print(\"   ✓ Verify output distribution matches standard decoding\")\n",
    "print(\"   ✓ Handle edge cases (end of sequence, special tokens)\")\n",
    "print(\"   ✓ Implement efficient batch verification\")\n",
    "print(\"   ✓ Monitor GPU utilization\")\n",
    "print(\"   ✓ Add fallback to standard decoding if needed\")\n",
    "print(\"   ✓ Log acceptance rates and speedups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison with Other Speedup Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Technique': [\n",
    "        'Speculative Decoding',\n",
    "        'Continuous Batching',\n",
    "        'FlashAttention',\n",
    "        'Quantization (INT8)',\n",
    "        'KV Cache Optimization',\n",
    "    ],\n",
    "    'Speedup': [\n",
    "        '2-3×',\n",
    "        '5-10× (throughput)',\n",
    "        '1.5-2×',\n",
    "        '1.5-2×',\n",
    "        '2-3×'\n",
    "    ],\n",
    "    'Memory Impact': [\n",
    "        '+Draft model',\n",
    "        'Higher utilization',\n",
    "        '-20-30%',\n",
    "        '-50-75%',\n",
    "        'Enables longer context'\n",
    "    ],\n",
    "    'Quality Impact': [\n",
    "        'None (same dist)',\n",
    "        'None',\n",
    "        'None',\n",
    "        'Minimal',\n",
    "        'None'\n",
    "    ],\n",
    "    'Best Use Case': [\n",
    "        'Long generation',\n",
    "        'High throughput',\n",
    "        'All inference',\n",
    "        'Deployment',\n",
    "        'Long context'\n",
    "    ],\n",
    "    'Complexity': [\n",
    "        'Medium',\n",
    "        'High',\n",
    "        'Medium',\n",
    "        'Low',\n",
    "        'Medium'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n=== LLM Inference Optimization Techniques ===\")\n",
    "print()\n",
    "print(df.to_string(index=False))\n",
    "print()\n",
    "print(\"Note: These techniques are complementary and can be combined!\")\n",
    "print(\"Example: Speculative + Continuous Batching + FlashAttention = 10-20× total speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **The Problem**\n",
    "   - LLM generation is memory-bound\n",
    "   - One token at a time is slow\n",
    "   - GPU underutilized\n",
    "\n",
    "2. **The Solution**\n",
    "   - Generate K draft tokens (fast)\n",
    "   - Verify all in parallel (one target pass)\n",
    "   - Accept correct predictions\n",
    "   - Mathematically equivalent to standard decoding\n",
    "\n",
    "3. **Key Parameters**\n",
    "   - **K**: Number of speculative tokens (typically 4-8)\n",
    "   - **Draft model**: 5-20% size of target model\n",
    "   - **Acceptance rate**: Determines speedup (aim for >60%)\n",
    "\n",
    "4. **Expected Speedup**\n",
    "   - **Typical**: 2-3× faster\n",
    "   - **Best case**: 4-5× faster (high acceptance)\n",
    "   - **Worst case**: ~1× (if draft model poor)\n",
    "\n",
    "5. **Production Considerations**\n",
    "   - Need both models in memory\n",
    "   - Monitor acceptance rates\n",
    "   - Tune K based on task\n",
    "   - Combine with other optimizations\n",
    "\n",
    "6. **When to Use**\n",
    "   - ✅ Long-form generation\n",
    "   - ✅ Memory-bound inference\n",
    "   - ✅ When draft model available\n",
    "   - ❌ Very short outputs\n",
    "   - ❌ Extreme latency requirements\n",
    "\n",
    "### Real-World Applications\n",
    "- **Code generation**: GitHub Copilot, CodeLlama\n",
    "- **Chat systems**: Faster response generation\n",
    "- **Content creation**: Articles, stories, documentation\n",
    "- **Translation**: Long document translation\n",
    "\n",
    "### Further Reading\n",
    "- Original paper: \"Fast Inference from Transformers via Speculative Decoding\" (Chen et al., 2023)\n",
    "- Self-speculative: \"Speculative Decoding with Big Little Decoder\" (Zhou et al., 2023)\n",
    "- Multi-token prediction: \"Better & Faster Large Language Models via Multi-token Prediction\" (Gloeckle et al., 2024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
