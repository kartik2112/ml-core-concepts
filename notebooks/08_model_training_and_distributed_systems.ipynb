{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training and Distributed Systems\n",
        "\n",
        "This comprehensive notebook covers advanced topics in model training, distributed systems, and resource estimation:\n",
        "\n",
        "1. **Training Token Optimization** - Chinchilla optimal scaling laws\n",
        "2. **Mixture of Experts (MoE)** - Parameter calculations and token requirements\n",
        "3. **GPU Requirements & Training Time** - FLOPs calculations and resource estimation\n",
        "4. **FLOPs of Transformer Block** - Detailed forward/backward pass calculations\n",
        "5. **Model Sharding Strategies** - TP, FSDP, PP, and more\n",
        "6. **Communication Patterns** - Collective operations and bandwidth analysis\n",
        "7. **Parallelism Trade-offs** - When to use each strategy\n",
        "8. **Inference** - Latency, throughput, and KV cache considerations\n",
        "9. **RL Training** - Async vs sync training comparison\n",
        "\n",
        "This notebook is designed for interview preparation and practical understanding of large-scale ML systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from typing import Tuple, Dict, List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"NumPy version:\", np.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Training Token Optimization: Chinchilla Scaling Laws\n",
        "\n",
        "The Chinchilla paper (Hoffmann et al., 2022) found that for compute-optimal training:\n",
        "\n",
        "### Key Finding:\n",
        "**For every parameter in the model, you should train on approximately 20 tokens.**\n",
        "\n",
        "### Formula:\n",
        "```\n",
        "Optimal Tokens = 20 \u00d7 Model Parameters\n",
        "```\n",
        "\n",
        "### Why This Matters:\n",
        "- Most models before Chinchilla were **undertrained**\n",
        "- GPT-3 (175B params) was trained on 300B tokens (~1.7x)\n",
        "- Chinchilla (70B params) trained on 1.4T tokens (20x) outperformed GPT-3\n",
        "- Modern models (LLaMA, Gemini) follow or exceed this ratio\n",
        "\n",
        "### Trade-offs:\n",
        "- **Inference Cost**: Larger models cost more to run\n",
        "- **Training Cost**: Training on 20x tokens is expensive\n",
        "- **Real-world**: Many train beyond 20x for better performance (e.g., LLaMA trained on 1-2T tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chinchilla_optimal_tokens(model_params: float) -> float:\n",
        "    \"\"\"\n",
        "    Calculate optimal number of training tokens according to Chinchilla.\n",
        "    \n",
        "    Args:\n",
        "        model_params: Number of model parameters (in billions)\n",
        "    \n",
        "    Returns:\n",
        "        Optimal training tokens (in billions)\n",
        "    \"\"\"\n",
        "    return 20 * model_params\n",
        "\n",
        "def compute_budget(model_params: float, tokens: float) -> float:\n",
        "    \"\"\"\n",
        "    Estimate compute budget in FLOPs.\n",
        "    \n",
        "    Args:\n",
        "        model_params: Model parameters in billions\n",
        "        tokens: Training tokens in billions\n",
        "    \n",
        "    Returns:\n",
        "        Total FLOPs (in 10^21 = ZettaFLOPs)\n",
        "    \"\"\"\n",
        "    # Rule of thumb: 6 FLOPs per parameter per token (FWD + BWD)\n",
        "    flops_per_token = 6 * model_params * 1e9\n",
        "    total_flops = flops_per_token * tokens * 1e9\n",
        "    return total_flops / 1e21  # Return in ZettaFLOPs\n",
        "\n",
        "# Example calculations\n",
        "models = {\n",
        "    \"GPT-3\": 175,\n",
        "    \"Chinchilla\": 70,\n",
        "    \"LLaMA-2-7B\": 7,\n",
        "    \"LLaMA-2-13B\": 13,\n",
        "    \"LLaMA-2-70B\": 70,\n",
        "}\n",
        "\n",
        "print(\"Model Training Token Requirements (Chinchilla Optimal):\")\n",
        "print(\"=\" * 70)\n",
        "for model_name, params in models.items():\n",
        "    optimal_tokens = chinchilla_optimal_tokens(params)\n",
        "    compute = compute_budget(params, optimal_tokens)\n",
        "    print(f\"{model_name:20s}: {params:6.0f}B params -> {optimal_tokens:7.0f}B tokens ({compute:6.2f} ZettaFLOPs)\")\n",
        "\n",
        "# GPT-3 actual training\n",
        "gpt3_actual_tokens = 300\n",
        "gpt3_ratio = gpt3_actual_tokens / models[\"GPT-3\"]\n",
        "print(f\"\\nGPT-3 was trained on {gpt3_actual_tokens}B tokens = {gpt3_ratio:.1f}x ratio (undertrained!)\")\n",
        "\n",
        "# LLaMA-2 actual training\n",
        "llama2_actual_tokens = 2000\n",
        "llama2_70b_ratio = llama2_actual_tokens / models[\"LLaMA-2-70B\"]\n",
        "print(f\"LLaMA-2-70B trained on {llama2_actual_tokens}B tokens = {llama2_70b_ratio:.1f}x ratio (overtrained for quality)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Chinchilla Scaling Law\n",
        "param_sizes = np.logspace(0, 3, 50)  # 1B to 1000B parameters\n",
        "optimal_tokens = chinchilla_optimal_tokens(param_sizes)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Tokens vs Parameters\n",
        "ax1.loglog(param_sizes, optimal_tokens, 'b-', linewidth=2, label='Chinchilla Optimal (20x)')\n",
        "ax1.loglog(param_sizes, param_sizes, 'r--', linewidth=2, alpha=0.5, label='1x ratio (GPT-3 style)')\n",
        "\n",
        "# Mark specific models\n",
        "for model_name, params in models.items():\n",
        "    tokens = chinchilla_optimal_tokens(params)\n",
        "    ax1.scatter(params, tokens, s=100, zorder=5)\n",
        "    ax1.annotate(model_name, (params, tokens), xytext=(5, 5), \n",
        "                textcoords='offset points', fontsize=8)\n",
        "\n",
        "ax1.set_xlabel('Model Parameters (Billions)', fontsize=12)\n",
        "ax1.set_ylabel('Training Tokens (Billions)', fontsize=12)\n",
        "ax1.set_title('Chinchilla Optimal: 20 Tokens per Parameter', fontsize=14)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Compute Budget\n",
        "compute_budgets = [compute_budget(p, chinchilla_optimal_tokens(p)) for p in param_sizes]\n",
        "ax2.loglog(param_sizes, compute_budgets, 'g-', linewidth=2)\n",
        "ax2.set_xlabel('Model Parameters (Billions)', fontsize=12)\n",
        "ax2.set_ylabel('Compute Budget (ZettaFLOPs)', fontsize=12)\n",
        "ax2.set_title('Total Compute Required for Chinchilla Optimal', fontsize=14)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mixture of Experts (MoE) Parameters\n",
        "\n",
        "MoE models have different parameter counting:\n",
        "\n",
        "### Total vs Active Parameters:\n",
        "- **Total Parameters**: All parameters in the model (including all experts)\n",
        "- **Active Parameters**: Parameters used for a single forward pass\n",
        "\n",
        "### Formula for MoE Layer:\n",
        "```\n",
        "Total Parameters = Shared_Params + (Num_Experts \u00d7 Expert_Size)\n",
        "Active Parameters = Shared_Params + (Top_K \u00d7 Expert_Size)\n",
        "```\n",
        "\n",
        "### Example: GPT-4 (rumored):\n",
        "- Total: ~1.7T parameters (8 experts \u00d7 220B each)\n",
        "- Active: ~220B per forward pass (top-1 routing)\n",
        "- Training tokens: Should be based on **active parameters**\n",
        "\n",
        "### Optimal Tokens for MoE:\n",
        "```\n",
        "Optimal Tokens = 20 \u00d7 Active_Parameters\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class MoEConfig:\n",
        "    \"\"\"Configuration for a Mixture of Experts model.\"\"\"\n",
        "    shared_params: float  # Billions\n",
        "    num_experts: int\n",
        "    expert_size: float  # Billions\n",
        "    top_k: int\n",
        "    \n",
        "    @property\n",
        "    def total_params(self) -> float:\n",
        "        \"\"\"Total parameters including all experts.\"\"\"\n",
        "        return self.shared_params + (self.num_experts * self.expert_size)\n",
        "    \n",
        "    @property\n",
        "    def active_params(self) -> float:\n",
        "        \"\"\"Active parameters per forward pass.\"\"\"\n",
        "        return self.shared_params + (self.top_k * self.expert_size)\n",
        "    \n",
        "    def optimal_tokens(self) -> float:\n",
        "        \"\"\"Optimal training tokens based on active parameters.\"\"\"\n",
        "        return chinchilla_optimal_tokens(self.active_params)\n",
        "    \n",
        "    def compute_efficiency(self) -> float:\n",
        "        \"\"\"Ratio of active to total parameters.\"\"\"\n",
        "        return self.active_params / self.total_params\n",
        "\n",
        "# Example MoE configurations\n",
        "moe_models = {\n",
        "    \"GPT-4 (rumored)\": MoEConfig(shared_params=100, num_experts=8, expert_size=220, top_k=1),\n",
        "    \"Mixtral-8x7B\": MoEConfig(shared_params=5, num_experts=8, expert_size=7, top_k=2),\n",
        "    \"Switch-C (Google)\": MoEConfig(shared_params=100, num_experts=128, expert_size=10, top_k=1),\n",
        "}\n",
        "\n",
        "print(\"MoE Model Analysis:\")\n",
        "print(\"=\" * 90)\n",
        "print(f\"{'Model':<25} {'Total':<12} {'Active':<12} {'Efficiency':<12} {'Optimal Tokens'}\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "for name, config in moe_models.items():\n",
        "    print(f\"{name:<25} {config.total_params:>8.0f}B     {config.active_params:>8.0f}B     \"\n",
        "          f\"{config.compute_efficiency()*100:>8.1f}%     {config.optimal_tokens():>8.0f}B tokens\")\n",
        "\n",
        "print(\"\\nKey Insight: MoE models have high total params but low active params,\")\n",
        "print(\"making them compute-efficient but memory-intensive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GPU Requirements & Training Time\n",
        "\n",
        "### Formula for Training Time:\n",
        "\n",
        "```\n",
        "Total FLOPs = 6 \u00d7 N \u00d7 D\n",
        "```\n",
        "Where:\n",
        "- N = Number of parameters\n",
        "- D = Number of tokens\n",
        "- 6 = FLOPs per param per token (\u22482 for FWD, \u22484 for BWD)\n",
        "\n",
        "```\n",
        "GPU Days = Total_FLOPs / (Num_GPUs \u00d7 GPU_FLOPS \u00d7 Utilization \u00d7 86400)\n",
        "```\n",
        "\n",
        "### GPU Specs (BF16/FP16):\n",
        "- **A100**: ~312 TFLOPS\n",
        "- **H100**: ~990 TFLOPS (3.2x faster than A100)\n",
        "- **H200**: ~990 TFLOPS (same compute as H100, more memory)\n",
        "\n",
        "### Typical Utilization:\n",
        "- **Good**: 40-50% MFU (Model FLOPs Utilization)\n",
        "- **Great**: 50-60% MFU\n",
        "- **Excellent**: 60%+ MFU (very hard to achieve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPUSpecs:\n",
        "    \"\"\"GPU specifications.\"\"\"\n",
        "    A100_TFLOPS = 312\n",
        "    H100_TFLOPS = 990\n",
        "    H200_TFLOPS = 990\n",
        "    \n",
        "    A100_MEMORY_GB = 80\n",
        "    H100_MEMORY_GB = 80\n",
        "    H200_MEMORY_GB = 141\n",
        "\n",
        "def estimate_training_time(\n",
        "    model_params_b: float,\n",
        "    training_tokens_b: float,\n",
        "    num_gpus: int,\n",
        "    gpu_tflops: float,\n",
        "    mfu: float = 0.5\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Estimate training time and cost.\n",
        "    \n",
        "    Args:\n",
        "        model_params_b: Model parameters in billions\n",
        "        training_tokens_b: Training tokens in billions\n",
        "        num_gpus: Number of GPUs\n",
        "        gpu_tflops: GPU TFLOPS (theoretical)\n",
        "        mfu: Model FLOPs Utilization (0.0 to 1.0)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with training time estimates\n",
        "    \"\"\"\n",
        "    # Calculate total FLOPs\n",
        "    total_flops = 6 * model_params_b * 1e9 * training_tokens_b * 1e9\n",
        "    \n",
        "    # Calculate effective FLOPS per GPU\n",
        "    effective_tflops_per_gpu = gpu_tflops * mfu\n",
        "    total_tflops = num_gpus * effective_tflops_per_gpu\n",
        "    \n",
        "    # Calculate time\n",
        "    seconds = total_flops / (total_tflops * 1e12)\n",
        "    hours = seconds / 3600\n",
        "    days = hours / 24\n",
        "    \n",
        "    # GPU hours\n",
        "    gpu_hours = hours * num_gpus\n",
        "    \n",
        "    return {\n",
        "        'total_flops': total_flops,\n",
        "        'seconds': seconds,\n",
        "        'hours': hours,\n",
        "        'days': days,\n",
        "        'gpu_hours': gpu_hours,\n",
        "        'effective_tflops': total_tflops,\n",
        "    }\n",
        "\n",
        "# Example: Train LLaMA-2-70B\n",
        "print(\"Training Time Estimation for LLaMA-2-70B:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "model_size = 70  # billions\n",
        "tokens = 2000  # billions (2T)\n",
        "\n",
        "for gpu_type, tflops in [('A100', GPUSpecs.A100_TFLOPS), ('H100', GPUSpecs.H100_TFLOPS)]:\n",
        "    for num_gpus in [512, 1024, 2048]:\n",
        "        result = estimate_training_time(model_size, tokens, num_gpus, tflops, mfu=0.5)\n",
        "        print(f\"{gpu_type} x {num_gpus:4d}: {result['days']:6.1f} days \"\n",
        "              f\"({result['gpu_hours']:8,.0f} GPU-hours, {result['effective_tflops']:7,.0f} TFLOPS)\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Training Time vs GPU Count\n",
        "model_size = 70  # LLaMA-2-70B\n",
        "tokens = 2000\n",
        "gpu_counts = np.array([128, 256, 512, 1024, 2048, 4096])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Training days vs GPU count\n",
        "for gpu_type, tflops, color in [('A100', GPUSpecs.A100_TFLOPS, 'blue'), \n",
        "                                 ('H100', GPUSpecs.H100_TFLOPS, 'red')]:\n",
        "    days = [estimate_training_time(model_size, tokens, n, tflops)['days'] for n in gpu_counts]\n",
        "    ax1.plot(gpu_counts, days, marker='o', linewidth=2, label=gpu_type, color=color)\n",
        "\n",
        "ax1.set_xlabel('Number of GPUs', fontsize=12)\n",
        "ax1.set_ylabel('Training Days', fontsize=12)\n",
        "ax1.set_title(f'Training Time: {model_size}B params, {tokens}B tokens (50% MFU)', fontsize=14)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xscale('log')\n",
        "ax1.set_yscale('log')\n",
        "\n",
        "# Plot 2: Effect of MFU\n",
        "mfu_values = np.linspace(0.3, 0.7, 5)\n",
        "num_gpus = 1024\n",
        "days_by_mfu = [estimate_training_time(model_size, tokens, num_gpus, \n",
        "                                       GPUSpecs.H100_TFLOPS, mfu)['days'] \n",
        "               for mfu in mfu_values]\n",
        "\n",
        "ax2.plot(mfu_values * 100, days_by_mfu, marker='s', linewidth=2, color='green')\n",
        "ax2.set_xlabel('Model FLOPs Utilization (%)', fontsize=12)\n",
        "ax2.set_ylabel('Training Days', fontsize=12)\n",
        "ax2.set_title(f'Impact of MFU (H100 x {num_gpus})', fontsize=14)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key Takeaway: Doubling GPUs halves training time (perfect scaling).\")\n",
        "print(\"In practice, communication overhead limits scaling efficiency.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. FLOPs of Transformer Block\n",
        "\n",
        "### Transformer Block Components:\n",
        "1. **Self-Attention**: Q, K, V projections + attention computation + output projection\n",
        "2. **Feed-Forward Network (FFN)**: Two linear layers\n",
        "3. **Layer Norms**: Negligible FLOPs\n",
        "\n",
        "### FLOPs Calculation:\n",
        "\n",
        "#### For a single token (sequence length = 1):\n",
        "\n",
        "**Attention FLOPs**:\n",
        "```\n",
        "Q, K, V projections: 3 \u00d7 (2 \u00d7 d_model \u00d7 d_model) = 6 \u00d7 d_model\u00b2\n",
        "Attention computation: 2 \u00d7 seq_len \u00d7 d_model \u00d7 seq_len \u2248 2 \u00d7 d_model \u00d7 seq_len\u00b2\n",
        "Output projection: 2 \u00d7 d_model \u00d7 d_model = 2 \u00d7 d_model\u00b2\n",
        "Total Attention: 8 \u00d7 d_model\u00b2 + 2 \u00d7 d_model \u00d7 seq_len\u00b2\n",
        "```\n",
        "\n",
        "**FFN FLOPs**:\n",
        "```\n",
        "First layer: 2 \u00d7 d_model \u00d7 d_ff = 2 \u00d7 d_model \u00d7 (4 \u00d7 d_model) = 8 \u00d7 d_model\u00b2\n",
        "Second layer: 2 \u00d7 d_ff \u00d7 d_model = 2 \u00d7 (4 \u00d7 d_model) \u00d7 d_model = 8 \u00d7 d_model\u00b2\n",
        "Total FFN: 16 \u00d7 d_model\u00b2\n",
        "```\n",
        "\n",
        "**Total per block (ignoring attention computation for long sequences)**:\n",
        "```\n",
        "FWD FLOPs \u2248 24 \u00d7 d_model\u00b2 per token\n",
        "BWD FLOPs \u2248 2 \u00d7 FWD = 48 \u00d7 d_model\u00b2\n",
        "Total (FWD + BWD) = 72 \u00d7 d_model\u00b2 per token\n",
        "```\n",
        "\n",
        "### Full Model:\n",
        "```\n",
        "Total FLOPs per token = 72 \u00d7 d_model\u00b2 \u00d7 num_layers\n",
        "```\n",
        "\n",
        "### Relationship:\n",
        "- **BWD = 2 \u00d7 FWD** (computing gradients requires 2x compute)\n",
        "- **Full step = FWD + BWD = 3 \u00d7 FWD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_transformer_flops(\n",
        "    d_model: int,\n",
        "    num_layers: int,\n",
        "    seq_len: int,\n",
        "    vocab_size: int = 50000,\n",
        "    include_attention_compute: bool = False\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate FLOPs for a transformer model.\n",
        "    \n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        num_layers: Number of transformer blocks\n",
        "        seq_len: Sequence length\n",
        "        vocab_size: Vocabulary size\n",
        "        include_attention_compute: Include attention matrix computation\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with FLOPs breakdown\n",
        "    \"\"\"\n",
        "    # Per-layer FLOPs\n",
        "    # Attention projections (Q, K, V, O)\n",
        "    attn_proj_flops = 8 * d_model * d_model\n",
        "    \n",
        "    # Attention computation (if included)\n",
        "    attn_compute_flops = 0\n",
        "    if include_attention_compute:\n",
        "        attn_compute_flops = 2 * d_model * seq_len * seq_len\n",
        "    \n",
        "    # FFN (typically 4x expansion)\n",
        "    d_ff = 4 * d_model\n",
        "    ffn_flops = 2 * d_model * d_ff + 2 * d_ff * d_model\n",
        "    ffn_flops = 16 * d_model * d_model\n",
        "    \n",
        "    # Per-layer FWD\n",
        "    layer_fwd_flops = attn_proj_flops + attn_compute_flops + ffn_flops\n",
        "    \n",
        "    # Embedding and unembedding\n",
        "    embed_flops = 2 * vocab_size * d_model\n",
        "    \n",
        "    # Total FWD\n",
        "    total_fwd = num_layers * layer_fwd_flops + embed_flops\n",
        "    \n",
        "    # BWD = 2 \u00d7 FWD\n",
        "    total_bwd = 2 * total_fwd\n",
        "    \n",
        "    # Full step\n",
        "    total_step = total_fwd + total_bwd\n",
        "    \n",
        "    return {\n",
        "        'fwd_per_layer': layer_fwd_flops,\n",
        "        'fwd_total': total_fwd,\n",
        "        'bwd_total': total_bwd,\n",
        "        'step_total': total_step,\n",
        "        'fwd_to_bwd_ratio': total_bwd / total_fwd,\n",
        "        'step_to_fwd_ratio': total_step / total_fwd,\n",
        "    }\n",
        "\n",
        "# Example: GPT-3 style model\n",
        "gpt3_config = {\n",
        "    'd_model': 12288,\n",
        "    'num_layers': 96,\n",
        "    'seq_len': 2048,\n",
        "    'vocab_size': 50257\n",
        "}\n",
        "\n",
        "flops = calculate_transformer_flops(**gpt3_config)\n",
        "\n",
        "print(\"FLOPs Analysis for GPT-3 Style Model:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model: d_model={gpt3_config['d_model']}, layers={gpt3_config['num_layers']}\")\n",
        "print(f\"\\nFWD per layer: {flops['fwd_per_layer']:,} FLOPs\")\n",
        "print(f\"FWD total:     {flops['fwd_total']:,} FLOPs\")\n",
        "print(f\"BWD total:     {flops['bwd_total']:,} FLOPs\")\n",
        "print(f\"Full step:     {flops['step_total']:,} FLOPs\")\n",
        "print(f\"\\nBWD / FWD ratio: {flops['fwd_to_bwd_ratio']:.1f}x\")\n",
        "print(f\"Step / FWD ratio: {flops['step_to_fwd_ratio']:.1f}x\")\n",
        "print(f\"\\nThis matches the rule: BWD = 2\u00d7FWD, Total = 3\u00d7FWD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verification: 6N per token rule\n",
        "def verify_6n_rule(model_params_b: float, d_model: int, num_layers: int):\n",
        "    \"\"\"\n",
        "    Verify the 6N FLOPs per token approximation.\n",
        "    \"\"\"\n",
        "    # Detailed calculation\n",
        "    flops = calculate_transformer_flops(d_model, num_layers, seq_len=1)\n",
        "    detailed_flops = flops['step_total']\n",
        "    \n",
        "    # Simple rule: 6N\n",
        "    simple_flops = 6 * model_params_b * 1e9\n",
        "    \n",
        "    print(f\"Model: {model_params_b}B params, d_model={d_model}, layers={num_layers}\")\n",
        "    print(f\"Detailed calculation: {detailed_flops:,.0f} FLOPs\")\n",
        "    print(f\"Simple rule (6N):     {simple_flops:,.0f} FLOPs\")\n",
        "    print(f\"Ratio: {detailed_flops / simple_flops:.2f}\")\n",
        "    print()\n",
        "\n",
        "# Test with different models\n",
        "print(\"Verification of 6N Rule:\")\n",
        "print(\"=\" * 60)\n",
        "verify_6n_rule(7, 4096, 32)  # LLaMA-7B\n",
        "verify_6n_rule(70, 8192, 80)  # LLaMA-70B\n",
        "verify_6n_rule(175, 12288, 96)  # GPT-3\n",
        "\n",
        "print(\"The 6N rule is a good approximation for training FLOPs!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Sharding Strategies\n",
        "\n",
        "Different parallelism strategies for training large models:\n",
        "\n",
        "### 1. **Tensor Parallel (TP)**\n",
        "- **What**: Split weight matrices across devices\n",
        "- **How**: Each GPU holds a slice of weight matrix\n",
        "- **Example**: For a 4096\u00d74096 matrix with TP=4, each GPU holds 4096\u00d71024\n",
        "- **Communication**: AllReduce after attention, AllReduce after MLP\n",
        "- **Use case**: Large models that don't fit on single GPU, need low latency\n",
        "- **Typical values**: TP=2, 4, 8 (within a node)\n",
        "\n",
        "### 2. **Pipeline Parallel (PP)**\n",
        "- **What**: Split model layers across devices\n",
        "- **How**: Each GPU holds consecutive layers\n",
        "- **Example**: 32-layer model with PP=4, each GPU holds 8 layers\n",
        "- **Communication**: Point-to-point (P2P) activation passing\n",
        "- **Use case**: Very deep models, minimize communication\n",
        "- **Drawback**: GPU bubble (idle time), needs micro-batching\n",
        "- **Typical values**: PP=2, 4, 8\n",
        "\n",
        "### 3. **Fully Sharded Data Parallel (FSDP)**\n",
        "- **What**: Data parallelism with sharded optimizer states\n",
        "- **Variants**:\n",
        "  - **ZeRO-1**: Shard optimizer states only\n",
        "  - **ZeRO-2**: Shard optimizer states + gradients\n",
        "  - **ZeRO-3**: Shard optimizer states + gradients + parameters\n",
        "- **Communication**: \n",
        "  - ZeRO-1: AllReduce for gradients\n",
        "  - ZeRO-2: ReduceScatter for gradients\n",
        "  - ZeRO-3: AllGather for parameters, ReduceScatter for gradients\n",
        "- **Use case**: Training medium to large models with data parallelism\n",
        "\n",
        "### 4. **Gradient Checkpointing (Activation Checkpointing)**\n",
        "- **What**: Trade compute for memory\n",
        "- **How**: Don't save activations during forward, recompute during backward\n",
        "- **Memory saved**: ~N/(C+1) where C is number of checkpoints\n",
        "- **Compute cost**: +33% to +100% depending on strategy\n",
        "- **Use case**: When memory-bound, not compute-bound\n",
        "\n",
        "### 5. **Expert Parallel (EP)**\n",
        "- **What**: For MoE models, distribute experts across devices\n",
        "- **How**: Each GPU holds subset of experts\n",
        "- **Communication**: AllToAll for token routing\n",
        "- **Use case**: MoE models with many experts\n",
        "\n",
        "### 6. **Context Parallel (CP)**\n",
        "- **What**: Split sequence length across devices\n",
        "- **How**: Each GPU processes part of the sequence\n",
        "- **Communication**: AllGather for attention (Ring Attention)\n",
        "- **Use case**: Very long sequences (>100k tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory requirements for different parallelism strategies\n",
        "\n",
        "def calculate_memory_per_gpu(\n",
        "    model_params_b: float,\n",
        "    tp_size: int = 1,\n",
        "    pp_size: int = 1,\n",
        "    dp_size: int = 1,\n",
        "    zero_stage: int = 0,\n",
        "    dtype_bytes: int = 2,  # BF16/FP16\n",
        "    gradient_checkpointing: bool = False\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate memory per GPU for different parallelism strategies.\n",
        "    \n",
        "    Args:\n",
        "        model_params_b: Model parameters in billions\n",
        "        tp_size: Tensor parallel size\n",
        "        pp_size: Pipeline parallel size\n",
        "        dp_size: Data parallel size\n",
        "        zero_stage: ZeRO stage (0, 1, 2, 3)\n",
        "        dtype_bytes: Bytes per parameter (2 for BF16/FP16, 4 for FP32)\n",
        "        gradient_checkpointing: Whether using gradient checkpointing\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with memory breakdown in GB\n",
        "    \"\"\"\n",
        "    total_params = model_params_b * 1e9\n",
        "    \n",
        "    # Parameters memory (sharded by TP and PP)\n",
        "    params_per_gpu = total_params / (tp_size * pp_size)\n",
        "    if zero_stage == 3:\n",
        "        params_per_gpu = total_params / (tp_size * pp_size * dp_size)\n",
        "    \n",
        "    # Gradients memory\n",
        "    grads_per_gpu = params_per_gpu\n",
        "    if zero_stage >= 2:\n",
        "        grads_per_gpu = total_params / (tp_size * pp_size * dp_size)\n",
        "    \n",
        "    # Optimizer states (Adam: 2x for momentum and variance)\n",
        "    optimizer_per_gpu = 2 * params_per_gpu * 4  # FP32 optimizer states\n",
        "    if zero_stage >= 1:\n",
        "        optimizer_per_gpu = 2 * (total_params / (tp_size * pp_size * dp_size)) * 4\n",
        "    \n",
        "    # Activations (rough estimate)\n",
        "    # For transformer: ~activations \u2248 34 * batch_size * seq_len * hidden_dim * num_layers\n",
        "    # Simplified: assume activations \u2248 10x model params for typical batch\n",
        "    activation_multiplier = 3 if gradient_checkpointing else 10\n",
        "    activations_per_gpu = (total_params / (tp_size * pp_size)) * activation_multiplier * dtype_bytes\n",
        "    \n",
        "    # Convert to GB\n",
        "    params_gb = params_per_gpu * dtype_bytes / 1e9\n",
        "    grads_gb = grads_per_gpu * dtype_bytes / 1e9\n",
        "    optimizer_gb = optimizer_per_gpu / 1e9\n",
        "    activations_gb = activations_per_gpu / 1e9\n",
        "    \n",
        "    total_gb = params_gb + grads_gb + optimizer_gb + activations_gb\n",
        "    \n",
        "    return {\n",
        "        'parameters': params_gb,\n",
        "        'gradients': grads_gb,\n",
        "        'optimizer': optimizer_gb,\n",
        "        'activations': activations_gb,\n",
        "        'total': total_gb\n",
        "    }\n",
        "\n",
        "# Example: LLaMA-2-70B on different configurations\n",
        "model_size = 70\n",
        "\n",
        "configs = [\n",
        "    (\"Baseline (no parallelism)\", 1, 1, 1, 0, False),\n",
        "    (\"TP=8\", 8, 1, 1, 0, False),\n",
        "    (\"TP=8 + Gradient Checkpointing\", 8, 1, 1, 0, True),\n",
        "    (\"TP=8 + ZeRO-1\", 8, 1, 8, 1, False),\n",
        "    (\"TP=8 + ZeRO-2\", 8, 1, 8, 2, False),\n",
        "    (\"TP=8 + ZeRO-3\", 8, 1, 8, 3, False),\n",
        "]\n",
        "\n",
        "print(f\"Memory Requirements for {model_size}B Parameter Model:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Configuration':<40} {'Total Memory':<15} {'Per GPU (80GB fit?)'}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for config_name, tp, pp, dp, zero, gc in configs:\n",
        "    mem = calculate_memory_per_gpu(model_size, tp, pp, dp, zero, gradient_checkpointing=gc)\n",
        "    fits = \"\u2713 Yes\" if mem['total'] < 80 else \"\u2717 No\"\n",
        "    print(f\"{config_name:<40} {mem['total']:>10.1f} GB     {fits}\")\n",
        "\n",
        "print(\"\\nNote: These are rough estimates. Actual memory usage varies with batch size,\")\n",
        "print(\"sequence length, and implementation details.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Communication Patterns\n",
        "\n",
        "Understanding collective operations is crucial for distributed training:\n",
        "\n",
        "### Collective Operations:\n",
        "\n",
        "#### 1. **AllReduce**\n",
        "- **Operation**: Sum tensors across all ranks, broadcast result to all\n",
        "- **Tensor shape**: Same on all ranks (e.g., [d_model, d_model])\n",
        "- **Bytes moved**: 2 \u00d7 tensor_size (in ring allreduce)\n",
        "- **Used in**: TP (after attention & MLP), standard DDP\n",
        "- **Cost**: 2 \u00d7 S / B where S = size, B = bandwidth\n",
        "\n",
        "#### 2. **AllGather**\n",
        "- **Operation**: Gather tensors from all ranks, broadcast to all\n",
        "- **Input shape**: [batch_size / N, ...] per rank\n",
        "- **Output shape**: [batch_size, ...] on all ranks\n",
        "- **Bytes moved**: (N-1)/N \u00d7 total_size per rank\n",
        "- **Used in**: ZeRO-3 (parameter gathering), Context Parallel\n",
        "\n",
        "#### 3. **ReduceScatter**\n",
        "- **Operation**: Reduce tensors, scatter results\n",
        "- **Input shape**: [N \u00d7 size] per rank\n",
        "- **Output shape**: [size] per rank\n",
        "- **Bytes moved**: (N-1)/N \u00d7 total_size\n",
        "- **Used in**: ZeRO-2/3 (gradient reduction)\n",
        "\n",
        "#### 4. **AllToAll**\n",
        "- **Operation**: Each rank sends different data to each other rank\n",
        "- **Bytes moved**: Total_data_size\n",
        "- **Used in**: Expert Parallel (MoE token routing)\n",
        "\n",
        "#### 5. **Point-to-Point (P2P)**\n",
        "- **Operation**: Direct send/receive between two ranks\n",
        "- **Used in**: Pipeline Parallel (activation passing)\n",
        "\n",
        "### Communication Cost Formula:\n",
        "```\n",
        "Latency = \u03b1 + \u03b2 \u00d7 Message_Size\n",
        "```\n",
        "Where:\n",
        "- \u03b1 = Network latency (microseconds)\n",
        "- \u03b2 = Inverse bandwidth (1/bandwidth)\n",
        "- Message_Size = Bytes to transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Communication cost estimation\n",
        "\n",
        "class NetworkSpecs:\n",
        "    \"\"\"Network specifications for different interconnects.\"\"\"\n",
        "    # Bandwidth in GB/s\n",
        "    NVLINK_BW = 600  # NVLink 4.0 (H100)\n",
        "    INFINIBAND_BW = 400  # InfiniBand NDR\n",
        "    ETHERNET_BW = 100  # 100GbE\n",
        "    \n",
        "    # Latency in microseconds\n",
        "    NVLINK_LATENCY = 1\n",
        "    INFINIBAND_LATENCY = 2\n",
        "    ETHERNET_LATENCY = 10\n",
        "\n",
        "def estimate_communication_time(\n",
        "    message_size_gb: float,\n",
        "    bandwidth_gbs: float,\n",
        "    latency_us: float,\n",
        "    algorithm: str = \"ring\"\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Estimate communication time for collective operations.\n",
        "    \n",
        "    Args:\n",
        "        message_size_gb: Size of message in GB\n",
        "        bandwidth_gbs: Network bandwidth in GB/s\n",
        "        latency_us: Network latency in microseconds\n",
        "        algorithm: Algorithm type ('ring', 'tree', 'direct')\n",
        "    \n",
        "    Returns:\n",
        "        Time in milliseconds\n",
        "    \"\"\"\n",
        "    # Convert latency to seconds\n",
        "    latency_s = latency_us / 1e6\n",
        "    \n",
        "    # Transfer time\n",
        "    transfer_time_s = message_size_gb / bandwidth_gbs\n",
        "    \n",
        "    # Total time (latency + transfer)\n",
        "    total_time_s = latency_s + transfer_time_s\n",
        "    \n",
        "    # Convert to milliseconds\n",
        "    return total_time_s * 1000\n",
        "\n",
        "def analyze_tp_communication(\n",
        "    d_model: int,\n",
        "    tp_size: int,\n",
        "    batch_size: int,\n",
        "    seq_len: int,\n",
        "    bandwidth_gbs: float,\n",
        "    latency_us: float\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Analyze communication for Tensor Parallel.\n",
        "    \n",
        "    TP requires 2 AllReduce per layer (after attention and after MLP).\n",
        "    \"\"\"\n",
        "    # AllReduce after attention: reduce [batch, seq_len, d_model]\n",
        "    attention_size_gb = batch_size * seq_len * d_model * 2 / 1e9  # BF16\n",
        "    \n",
        "    # AllReduce after MLP: same size\n",
        "    mlp_size_gb = attention_size_gb\n",
        "    \n",
        "    # Time per AllReduce (Ring AllReduce: 2(N-1)/N \u2248 2 for large N)\n",
        "    attention_time = estimate_communication_time(2 * attention_size_gb, bandwidth_gbs, latency_us)\n",
        "    mlp_time = estimate_communication_time(2 * mlp_size_gb, bandwidth_gbs, latency_us)\n",
        "    \n",
        "    return {\n",
        "        'attention_allreduce_ms': attention_time,\n",
        "        'mlp_allreduce_ms': mlp_time,\n",
        "        'total_per_layer_ms': attention_time + mlp_time,\n",
        "        'bytes_per_layer_gb': 4 * attention_size_gb  # 2 allreduces, each moves 2x data\n",
        "    }\n",
        "\n",
        "# Example: Analyze TP=8 for LLaMA-70B\n",
        "print(\"Tensor Parallel Communication Analysis:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "config = {\n",
        "    'd_model': 8192,\n",
        "    'tp_size': 8,\n",
        "    'batch_size': 4,\n",
        "    'seq_len': 2048,\n",
        "}\n",
        "\n",
        "for network_name, bw, lat in [\n",
        "    (\"NVLink\", NetworkSpecs.NVLINK_BW, NetworkSpecs.NVLINK_LATENCY),\n",
        "    (\"InfiniBand\", NetworkSpecs.INFINIBAND_BW, NetworkSpecs.INFINIBAND_LATENCY),\n",
        "]:\n",
        "    result = analyze_tp_communication(**config, bandwidth_gbs=bw, latency_us=lat)\n",
        "    print(f\"\\n{network_name} ({bw} GB/s):\")\n",
        "    print(f\"  Attention AllReduce: {result['attention_allreduce_ms']:.2f} ms\")\n",
        "    print(f\"  MLP AllReduce:       {result['mlp_allreduce_ms']:.2f} ms\")\n",
        "    print(f\"  Total per layer:     {result['total_per_layer_ms']:.2f} ms\")\n",
        "    print(f\"  Bytes per layer:     {result['bytes_per_layer_gb']:.3f} GB\")\n",
        "\n",
        "print(\"\\nKey Insight: TP requires fast interconnect (NVLink/InfiniBand).\")\n",
        "print(\"Use TP within a node, not across nodes!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FSDP Communication Analysis\n",
        "\n",
        "def analyze_fsdp_communication(\n",
        "    model_params_b: float,\n",
        "    num_layers: int,\n",
        "    dp_size: int,\n",
        "    zero_stage: int,\n",
        "    bandwidth_gbs: float,\n",
        "    latency_us: float\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Analyze communication for FSDP/ZeRO.\n",
        "    \"\"\"\n",
        "    total_params = model_params_b * 1e9\n",
        "    params_per_layer = total_params / num_layers\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    if zero_stage == 0 or zero_stage == 1:\n",
        "        # AllReduce for gradients\n",
        "        grad_size_gb = total_params * 2 / 1e9  # BF16\n",
        "        comm_time = estimate_communication_time(2 * grad_size_gb, bandwidth_gbs, latency_us)\n",
        "        results = {\n",
        "            'type': 'AllReduce',\n",
        "            'total_comm_ms': comm_time,\n",
        "            'comm_per_layer_ms': comm_time / num_layers,\n",
        "            'bytes_total_gb': 2 * grad_size_gb\n",
        "        }\n",
        "    \n",
        "    elif zero_stage == 2:\n",
        "        # ReduceScatter for gradients\n",
        "        grad_size_gb = total_params * 2 / 1e9\n",
        "        comm_time = estimate_communication_time((dp_size - 1) / dp_size * grad_size_gb, \n",
        "                                                 bandwidth_gbs, latency_us)\n",
        "        results = {\n",
        "            'type': 'ReduceScatter',\n",
        "            'total_comm_ms': comm_time,\n",
        "            'comm_per_layer_ms': comm_time / num_layers,\n",
        "            'bytes_total_gb': (dp_size - 1) / dp_size * grad_size_gb\n",
        "        }\n",
        "    \n",
        "    elif zero_stage == 3:\n",
        "        # AllGather for params + ReduceScatter for grads (per layer)\n",
        "        params_layer_gb = params_per_layer * 2 / 1e9\n",
        "        \n",
        "        # AllGather time per layer\n",
        "        allgather_time = estimate_communication_time((dp_size - 1) / dp_size * params_layer_gb,\n",
        "                                                      bandwidth_gbs, latency_us)\n",
        "        \n",
        "        # ReduceScatter time per layer\n",
        "        reducescatter_time = estimate_communication_time((dp_size - 1) / dp_size * params_layer_gb,\n",
        "                                                          bandwidth_gbs, latency_us)\n",
        "        \n",
        "        results = {\n",
        "            'type': 'AllGather + ReduceScatter',\n",
        "            'allgather_per_layer_ms': allgather_time,\n",
        "            'reducescatter_per_layer_ms': reducescatter_time,\n",
        "            'comm_per_layer_ms': allgather_time + reducescatter_time,\n",
        "            'total_comm_ms': (allgather_time + reducescatter_time) * num_layers,\n",
        "            'bytes_per_layer_gb': 2 * (dp_size - 1) / dp_size * params_layer_gb\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example: Compare ZeRO stages\n",
        "print(\"FSDP/ZeRO Communication Analysis:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "model_size = 70\n",
        "num_layers = 80\n",
        "dp_size = 64\n",
        "\n",
        "for zero_stage in [1, 2, 3]:\n",
        "    result = analyze_fsdp_communication(\n",
        "        model_size, num_layers, dp_size, zero_stage,\n",
        "        NetworkSpecs.INFINIBAND_BW, NetworkSpecs.INFINIBAND_LATENCY\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nZeRO-{zero_stage} ({result['type']}):\")\n",
        "    if zero_stage < 3:\n",
        "        print(f\"  Total communication: {result['total_comm_ms']:.2f} ms\")\n",
        "        print(f\"  Per layer:           {result['comm_per_layer_ms']:.2f} ms\")\n",
        "    else:\n",
        "        print(f\"  AllGather per layer:       {result['allgather_per_layer_ms']:.2f} ms\")\n",
        "        print(f\"  ReduceScatter per layer:   {result['reducescatter_per_layer_ms']:.2f} ms\")\n",
        "        print(f\"  Total per layer:           {result['comm_per_layer_ms']:.2f} ms\")\n",
        "        print(f\"  Total for all layers:      {result['total_comm_ms']:.2f} ms\")\n",
        "\n",
        "print(\"\\nWhy ZeRO-3 is rarely used:\")\n",
        "print(\"  - High communication overhead (AllGather + ReduceScatter per layer)\")\n",
        "print(\"  - 2\u00d7 communication per layer vs ZeRO-2\")\n",
        "print(\"  - Better to use TP or PP for large models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Parallelism Trade-offs and Best Practices\n",
        "\n",
        "### When to Use Each Strategy:\n",
        "\n",
        "#### **Tensor Parallel (TP)**:\n",
        "- \u2705 **Use when**: Model doesn't fit in single GPU, have fast interconnect (NVLink)\n",
        "- \u2705 **Use within**: Single node (up to 8 GPUs)\n",
        "- \u274c **Don't use**: Across nodes (too slow)\n",
        "- **Typical**: TP=2, 4, or 8\n",
        "\n",
        "#### **Pipeline Parallel (PP)**:\n",
        "- \u2705 **Use when**: Model very deep, want to minimize communication\n",
        "- \u2705 **Use with**: Micro-batching to reduce bubble\n",
        "- \u274c **Don't use**: If batch size is small (large bubble)\n",
        "- **Typical**: PP=2, 4, or 8\n",
        "\n",
        "#### **Data Parallel (DP) / FSDP**:\n",
        "- \u2705 **ZeRO-1**: Always safe, minimal overhead\n",
        "- \u2705 **ZeRO-2**: Good balance, use for most training\n",
        "- \u26a0\ufe0f **ZeRO-3**: Rarely used - high communication overhead\n",
        "- \u274c **ZeRO-3**: Don't use unless desperate for memory\n",
        "\n",
        "#### **Gradient Checkpointing**:\n",
        "- \u2705 **Use when**: Memory-bound, not compute-bound\n",
        "- \u274c **Don't use**: If already compute-bound (adds 33-100% compute)\n",
        "\n",
        "### Typical Production Setup:\n",
        "\n",
        "```\n",
        "3D Parallelism = TP \u00d7 PP \u00d7 DP\n",
        "```\n",
        "\n",
        "**Example for 1024 GPUs (128 nodes \u00d7 8 GPUs/node)**:\n",
        "- TP = 8 (within node, use all NVLink bandwidth)\n",
        "- PP = 4 (across nodes)\n",
        "- DP = 32 (remaining parallelism)\n",
        "- Total: 8 \u00d7 4 \u00d7 32 = 1024 GPUs\n",
        "\n",
        "### Why ZeRO-3 is Rarely Used:\n",
        "\n",
        "1. **Communication Overhead**: AllGather parameters before each layer\n",
        "2. **Latency**: Multiple small communications vs one large communication\n",
        "3. **Bandwidth**: Uses 2x communication bandwidth per layer\n",
        "4. **Better Alternatives**: Use TP or PP to reduce memory instead\n",
        "\n",
        "**Real-world**: Companies use TP+PP+DP with ZeRO-1 or ZeRO-2, not ZeRO-3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Practical parallelism configuration\n",
        "\n",
        "def recommend_parallelism(\n",
        "    model_params_b: float,\n",
        "    num_gpus: int,\n",
        "    gpus_per_node: int = 8,\n",
        "    gpu_memory_gb: int = 80\n",
        ") -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Recommend parallelism configuration for a given setup.\n",
        "    \"\"\"\n",
        "    num_nodes = num_gpus // gpus_per_node\n",
        "    \n",
        "    # Rule of thumb: model fits in GPU if params_gb * 20 < gpu_memory\n",
        "    # (factor of 20 accounts for gradients, optimizer, activations)\n",
        "    model_size_gb = model_params_b * 2  # BF16\n",
        "    needs_model_parallel = (model_size_gb * 20) > gpu_memory_gb\n",
        "    \n",
        "    # Recommend configuration\n",
        "    if not needs_model_parallel:\n",
        "        # Pure data parallel\n",
        "        config = {\n",
        "            'tp_size': 1,\n",
        "            'pp_size': 1,\n",
        "            'dp_size': num_gpus,\n",
        "            'zero_stage': 2,\n",
        "            'gradient_checkpointing': False,\n",
        "            'rationale': 'Model fits on single GPU, use pure DP with ZeRO-2'\n",
        "        }\n",
        "    elif num_nodes == 1:\n",
        "        # Single node: use TP\n",
        "        tp_size = min(gpus_per_node, 8)\n",
        "        config = {\n",
        "            'tp_size': tp_size,\n",
        "            'pp_size': 1,\n",
        "            'dp_size': num_gpus // tp_size,\n",
        "            'zero_stage': 1,\n",
        "            'gradient_checkpointing': True,\n",
        "            'rationale': f'Single node, use TP={tp_size} within node'\n",
        "        }\n",
        "    else:\n",
        "        # Multi-node: use 3D parallelism\n",
        "        tp_size = gpus_per_node  # TP within node\n",
        "        \n",
        "        # Decide PP size based on model size\n",
        "        if model_params_b > 100:\n",
        "            pp_size = min(4, num_nodes // 2)\n",
        "        elif model_params_b > 50:\n",
        "            pp_size = 2\n",
        "        else:\n",
        "            pp_size = 1\n",
        "        \n",
        "        dp_size = num_gpus // (tp_size * pp_size)\n",
        "        \n",
        "        config = {\n",
        "            'tp_size': tp_size,\n",
        "            'pp_size': pp_size,\n",
        "            'dp_size': dp_size,\n",
        "            'zero_stage': 2,\n",
        "            'gradient_checkpointing': True,\n",
        "            'rationale': f'Multi-node 3D parallelism: TP={tp_size} (within node), '\n",
        "                        f'PP={pp_size} (across nodes), DP={dp_size}'\n",
        "        }\n",
        "    \n",
        "    config['total_gpus'] = config['tp_size'] * config['pp_size'] * config['dp_size']\n",
        "    return config\n",
        "\n",
        "# Examples\n",
        "print(\"Parallelism Recommendations:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_cases = [\n",
        "    (\"LLaMA-2-7B\", 7, 64),\n",
        "    (\"LLaMA-2-13B\", 13, 128),\n",
        "    (\"LLaMA-2-70B\", 70, 512),\n",
        "    (\"GPT-3-175B\", 175, 1024),\n",
        "    (\"GPT-4-1.7T (MoE)\", 1700, 2048),\n",
        "]\n",
        "\n",
        "for model_name, params, gpus in test_cases:\n",
        "    config = recommend_parallelism(params, gpus)\n",
        "    print(f\"\\n{model_name} on {gpus} GPUs:\")\n",
        "    print(f\"  TP = {config['tp_size']}, PP = {config['pp_size']}, DP = {config['dp_size']}\")\n",
        "    print(f\"  ZeRO-{config['zero_stage']}, Gradient Checkpointing = {config['gradient_checkpointing']}\")\n",
        "    print(f\"  Rationale: {config['rationale']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Inference: Latency and Throughput\n",
        "\n",
        "Inference has two phases with different characteristics:\n",
        "\n",
        "### Prefill Phase (First Token):\n",
        "- **Operation**: Process entire input prompt\n",
        "- **Characteristic**: Compute-bound (matrix multiplications)\n",
        "- **FLOPs**: `2 \u00d7 N \u00d7 prompt_len` (per token in prompt)\n",
        "- **Latency**: `prompt_len \u00d7 (2N / GPU_FLOPS)`\n",
        "- **Can batch**: Yes, efficiently!\n",
        "\n",
        "### Decode Phase (Subsequent Tokens):\n",
        "- **Operation**: Generate one token at a time\n",
        "- **Characteristic**: Memory-bound (reading KV cache + weights)\n",
        "- **FLOPs**: `2 \u00d7 N` per token\n",
        "- **Latency**: Dominated by memory bandwidth\n",
        "- **Memory reads**: `N \u00d7 2 bytes` (parameters) + KV cache\n",
        "- **Can batch**: Yes, but limited by KV cache memory\n",
        "\n",
        "### KV Cache Memory:\n",
        "\n",
        "For each token generated, we store keys and values:\n",
        "\n",
        "```\n",
        "KV_cache_per_token = 2 \u00d7 num_layers \u00d7 d_model \u00d7 2 bytes\n",
        "```\n",
        "\n",
        "For batch size B and sequence length S:\n",
        "```\n",
        "Total_KV_cache = B \u00d7 S \u00d7 2 \u00d7 num_layers \u00d7 d_model \u00d7 2 bytes\n",
        "```\n",
        "\n",
        "### Latency Formula:\n",
        "\n",
        "**Prefill (compute-bound)**:\n",
        "```\n",
        "Latency_prefill = prompt_len \u00d7 2N / GPU_FLOPS\n",
        "```\n",
        "\n",
        "**Decode (memory-bound)**:\n",
        "```\n",
        "Latency_decode = (N \u00d7 2 bytes) / Memory_Bandwidth\n",
        "```\n",
        "\n",
        "For H100:\n",
        "- GPU_FLOPS = 990 TFLOPS (BF16)\n",
        "- Memory_Bandwidth = 3.35 TB/s (HBM3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference latency calculation\n",
        "\n",
        "def calculate_inference_latency(\n",
        "    model_params_b: float,\n",
        "    prompt_len: int,\n",
        "    gen_len: int,\n",
        "    batch_size: int,\n",
        "    gpu_tflops: float = 990,  # H100\n",
        "    memory_bandwidth_tbs: float = 3.35,  # H100 HBM3\n",
        "    d_model: int = 8192,\n",
        "    num_layers: int = 80\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate inference latency for prefill and decode phases.\n",
        "    \n",
        "    Args:\n",
        "        model_params_b: Model parameters in billions\n",
        "        prompt_len: Input prompt length\n",
        "        gen_len: Number of tokens to generate\n",
        "        batch_size: Batch size\n",
        "        gpu_tflops: GPU TFLOPS (theoretical)\n",
        "        memory_bandwidth_tbs: Memory bandwidth in TB/s\n",
        "        d_model: Model dimension\n",
        "        num_layers: Number of layers\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with latency estimates\n",
        "    \"\"\"\n",
        "    # Convert to actual numbers\n",
        "    model_params = model_params_b * 1e9\n",
        "    gpu_flops = gpu_tflops * 1e12\n",
        "    memory_bandwidth_bs = memory_bandwidth_tbs * 1e12\n",
        "    \n",
        "    # Prefill: compute-bound\n",
        "    # FLOPs = batch_size \u00d7 prompt_len \u00d7 2N\n",
        "    prefill_flops = batch_size * prompt_len * 2 * model_params\n",
        "    prefill_time_s = prefill_flops / gpu_flops\n",
        "    prefill_time_ms = prefill_time_s * 1000\n",
        "    \n",
        "    # Decode: memory-bound\n",
        "    # Memory reads per token = N \u00d7 2 bytes (parameters)\n",
        "    bytes_per_token = model_params * 2  # BF16\n",
        "    decode_time_per_token_s = bytes_per_token / memory_bandwidth_bs\n",
        "    decode_time_per_token_ms = decode_time_per_token_s * 1000\n",
        "    \n",
        "    # Total decode time for generating gen_len tokens\n",
        "    # Note: batch_size doesn't affect latency much if memory-bound\n",
        "    total_decode_time_ms = decode_time_per_token_ms * gen_len\n",
        "    \n",
        "    # KV cache memory\n",
        "    # Per token: 2 (K,V) \u00d7 num_layers \u00d7 d_model \u00d7 2 bytes\n",
        "    kv_cache_per_token_bytes = 2 * num_layers * d_model * 2\n",
        "    kv_cache_per_seq_gb = (prompt_len + gen_len) * kv_cache_per_token_bytes / 1e9\n",
        "    kv_cache_total_gb = batch_size * kv_cache_per_seq_gb\n",
        "    \n",
        "    # Total latency\n",
        "    total_latency_ms = prefill_time_ms + total_decode_time_ms\n",
        "    \n",
        "    # Tokens per second\n",
        "    total_tokens = batch_size * gen_len\n",
        "    tokens_per_second = total_tokens / (total_latency_ms / 1000)\n",
        "    \n",
        "    return {\n",
        "        'prefill_ms': prefill_time_ms,\n",
        "        'decode_per_token_ms': decode_time_per_token_ms,\n",
        "        'decode_total_ms': total_decode_time_ms,\n",
        "        'total_latency_ms': total_latency_ms,\n",
        "        'tokens_per_second': tokens_per_second,\n",
        "        'kv_cache_per_seq_gb': kv_cache_per_seq_gb,\n",
        "        'kv_cache_total_gb': kv_cache_total_gb,\n",
        "    }\n",
        "\n",
        "# Example: LLaMA-2-70B inference\n",
        "print(\"Inference Latency for LLaMA-2-70B on H100:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "test_configs = [\n",
        "    (\"Short prompt, short gen\", 100, 50, 1),\n",
        "    (\"Long prompt, short gen\", 2000, 50, 1),\n",
        "    (\"Short prompt, long gen\", 100, 500, 1),\n",
        "    (\"Batched (bs=8)\", 100, 50, 8),\n",
        "]\n",
        "\n",
        "for name, prompt_len, gen_len, batch_size in test_configs:\n",
        "    result = calculate_inference_latency(\n",
        "        model_params_b=70,\n",
        "        prompt_len=prompt_len,\n",
        "        gen_len=gen_len,\n",
        "        batch_size=batch_size,\n",
        "        d_model=8192,\n",
        "        num_layers=80\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Prompt: {prompt_len} tokens, Generate: {gen_len} tokens, Batch: {batch_size}\")\n",
        "    print(f\"  Prefill time:        {result['prefill_ms']:6.1f} ms\")\n",
        "    print(f\"  Decode per token:    {result['decode_per_token_ms']:6.1f} ms\")\n",
        "    print(f\"  Total decode time:   {result['decode_total_ms']:6.1f} ms\")\n",
        "    print(f\"  Total latency:       {result['total_latency_ms']:6.1f} ms\")\n",
        "    print(f\"  Throughput:          {result['tokens_per_second']:6.1f} tokens/s\")\n",
        "    print(f\"  KV cache per seq:    {result['kv_cache_per_seq_gb']:6.2f} GB\")\n",
        "    print(f\"  KV cache total:      {result['kv_cache_total_gb']:6.2f} GB\")\n",
        "\n",
        "print(\"\\nKey Insight: Decode is memory-bound. Time per token \u2248 constant.\")\n",
        "print(\"Prefill can be batched efficiently, decode is limited by KV cache memory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU requirements for serving\n",
        "\n",
        "def estimate_serving_gpus(\n",
        "    model_params_b: float,\n",
        "    num_users: int,\n",
        "    requests_per_second_per_user: float,\n",
        "    avg_prompt_len: int,\n",
        "    avg_gen_len: int,\n",
        "    target_latency_ms: float,\n",
        "    gpu_memory_gb: int = 80,\n",
        "    d_model: int = 8192,\n",
        "    num_layers: int = 80\n",
        ") -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Estimate number of GPUs needed for serving.\n",
        "    \"\"\"\n",
        "    # Total requests per second\n",
        "    total_rps = num_users * requests_per_second_per_user\n",
        "    \n",
        "    # Calculate single request latency\n",
        "    single_request = calculate_inference_latency(\n",
        "        model_params_b=model_params_b,\n",
        "        prompt_len=avg_prompt_len,\n",
        "        gen_len=avg_gen_len,\n",
        "        batch_size=1,\n",
        "        d_model=d_model,\n",
        "        num_layers=num_layers\n",
        "    )\n",
        "    \n",
        "    # Throughput per GPU (tokens/s)\n",
        "    tokens_per_request = avg_gen_len\n",
        "    throughput_per_gpu = single_request['tokens_per_second']\n",
        "    requests_per_gpu = throughput_per_gpu / tokens_per_request\n",
        "    \n",
        "    # KV cache memory constraint\n",
        "    kv_cache_per_request = single_request['kv_cache_per_seq_gb']\n",
        "    # Leave 20GB for model weights (rough estimate)\n",
        "    model_weight_memory = model_params_b * 2 / 1e9  # BF16 in GB\n",
        "    available_for_kv = gpu_memory_gb - model_weight_memory\n",
        "    max_concurrent_requests_memory = available_for_kv / kv_cache_per_request\n",
        "    \n",
        "    # Latency constraint\n",
        "    # If latency = single_request_latency, we can only serve 1 request at a time\n",
        "    # With batching, we can serve more\n",
        "    max_concurrent_requests_latency = target_latency_ms / single_request['total_latency_ms']\n",
        "    \n",
        "    # Effective concurrent requests (limited by memory or latency)\n",
        "    max_concurrent_requests = min(max_concurrent_requests_memory, \n",
        "                                   max_concurrent_requests_latency)\n",
        "    \n",
        "    # GPUs needed based on throughput\n",
        "    gpus_needed_throughput = total_rps / requests_per_gpu\n",
        "    \n",
        "    # GPUs needed (round up)\n",
        "    gpus_needed = int(np.ceil(gpus_needed_throughput))\n",
        "    \n",
        "    return {\n",
        "        'total_rps': total_rps,\n",
        "        'requests_per_gpu': requests_per_gpu,\n",
        "        'gpus_needed': gpus_needed,\n",
        "        'max_concurrent_requests': max_concurrent_requests,\n",
        "        'kv_cache_per_request_gb': kv_cache_per_request,\n",
        "        'model_memory_gb': model_weight_memory,\n",
        "        'single_request_latency_ms': single_request['total_latency_ms']\n",
        "    }\n",
        "\n",
        "# Example: How many H100s for 10k users?\n",
        "print(\"GPU Requirements for Serving LLaMA-2-70B:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "scenarios = [\n",
        "    (\"Low load\", 1000, 0.01),    # 10 RPS total\n",
        "    (\"Medium load\", 5000, 0.02), # 100 RPS total\n",
        "    (\"High load\", 10000, 0.05),  # 500 RPS total\n",
        "]\n",
        "\n",
        "for name, num_users, rps_per_user in scenarios:\n",
        "    result = estimate_serving_gpus(\n",
        "        model_params_b=70,\n",
        "        num_users=num_users,\n",
        "        requests_per_second_per_user=rps_per_user,\n",
        "        avg_prompt_len=100,\n",
        "        avg_gen_len=50,\n",
        "        target_latency_ms=1000,  # 1 second\n",
        "        d_model=8192,\n",
        "        num_layers=80\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Users: {num_users:,}, RPS per user: {rps_per_user}\")\n",
        "    print(f\"  Total RPS: {result['total_rps']:.1f}\")\n",
        "    print(f\"  Requests per GPU: {result['requests_per_gpu']:.2f}\")\n",
        "    print(f\"  GPUs needed: {result['gpus_needed']}\")\n",
        "    print(f\"  Single request latency: {result['single_request_latency_ms']:.1f} ms\")\n",
        "\n",
        "print(\"\\nNote: This is a simplified estimate. Real serving systems use:\")\n",
        "print(\"  - Continuous batching (vLLM, TGI)\")\n",
        "print(\"  - PagedAttention for efficient KV cache management\")\n",
        "print(\"  - Request queuing and load balancing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Reinforcement Learning Training\n",
        "\n",
        "RL training (e.g., RLHF, PPO) has different characteristics than supervised learning:\n",
        "\n",
        "### RL Training Components:\n",
        "\n",
        "1. **Policy Model**: The model being trained\n",
        "2. **Reference Model**: Frozen copy for KL penalty\n",
        "3. **Reward Model**: Scores generated outputs\n",
        "4. **Critic Model** (PPO): Value function estimator\n",
        "\n",
        "### Two Training Paradigms:\n",
        "\n",
        "#### **Synchronous Training**:\n",
        "- All models on same GPUs\n",
        "- Sequential execution: generate \u2192 score \u2192 train\n",
        "- **Pros**: Simple, deterministic\n",
        "- **Cons**: Low GPU utilization (models waiting)\n",
        "\n",
        "#### **Asynchronous Training**:\n",
        "- Models on different GPUs\n",
        "- Parallel execution: generation on GPU set A, training on GPU set B\n",
        "- **Pros**: High GPU utilization, faster\n",
        "- **Cons**: More complex, requires more GPUs\n",
        "\n",
        "### Throughput Calculation:\n",
        "\n",
        "**Synchronous**:\n",
        "```\n",
        "Time per iteration = T_generate + T_score + T_train\n",
        "Throughput = Batch_size / Time_per_iteration\n",
        "```\n",
        "\n",
        "**Asynchronous**:\n",
        "```\n",
        "Time per iteration = max(T_generate, T_train)\n",
        "Throughput = Batch_size / max(T_generate, T_train)\n",
        "```\n",
        "\n",
        "### GPU Allocation:\n",
        "\n",
        "**Synchronous (256 GPUs total)**:\n",
        "- Policy: 256 GPUs (training + generation)\n",
        "- Reference: 0 GPUs (share with policy)\n",
        "- Reward: 0 GPUs (share with policy)\n",
        "- Critic: 0 GPUs (share with policy)\n",
        "\n",
        "**Asynchronous (256 GPUs total)**:\n",
        "- Policy: 128 GPUs (training only)\n",
        "- Reference: 64 GPUs (generation only)\n",
        "- Reward: 32 GPUs (scoring only)\n",
        "- Critic: 32 GPUs (value estimation)\n",
        "\n",
        "### When to Use Each:\n",
        "\n",
        "- **Sync**: Small scale, debugging, deterministic results\n",
        "- **Async**: Production, large scale, maximize throughput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RL Training throughput estimation\n",
        "\n",
        "def estimate_rl_throughput(\n",
        "    model_params_b: float,\n",
        "    batch_size: int,\n",
        "    prompt_len: int,\n",
        "    gen_len: int,\n",
        "    num_ppo_epochs: int,\n",
        "    total_gpus: int,\n",
        "    mode: str = 'sync',  # 'sync' or 'async'\n",
        "    gpu_tflops: float = 990,\n",
        "    memory_bandwidth_tbs: float = 3.35\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Estimate RL training throughput.\n",
        "    \n",
        "    Args:\n",
        "        model_params_b: Model parameters in billions\n",
        "        batch_size: Batch size for generation\n",
        "        prompt_len: Prompt length\n",
        "        gen_len: Generation length\n",
        "        num_ppo_epochs: Number of PPO training epochs\n",
        "        total_gpus: Total number of GPUs\n",
        "        mode: 'sync' or 'async'\n",
        "        gpu_tflops: GPU TFLOPS\n",
        "        memory_bandwidth_tbs: Memory bandwidth\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with throughput estimates\n",
        "    \"\"\"\n",
        "    # Generation time\n",
        "    gen_result = calculate_inference_latency(\n",
        "        model_params_b=model_params_b,\n",
        "        prompt_len=prompt_len,\n",
        "        gen_len=gen_len,\n",
        "        batch_size=batch_size,\n",
        "        gpu_tflops=gpu_tflops,\n",
        "        memory_bandwidth_tbs=memory_bandwidth_tbs\n",
        "    )\n",
        "    t_generate_ms = gen_result['total_latency_ms']\n",
        "    \n",
        "    # Reward scoring time (much faster, small model)\n",
        "    # Assume reward model is 10x smaller\n",
        "    reward_result = calculate_inference_latency(\n",
        "        model_params_b=model_params_b / 10,\n",
        "        prompt_len=prompt_len + gen_len,\n",
        "        gen_len=1,  # Just scoring\n",
        "        batch_size=batch_size,\n",
        "        gpu_tflops=gpu_tflops,\n",
        "        memory_bandwidth_tbs=memory_bandwidth_tbs\n",
        "    )\n",
        "    t_score_ms = reward_result['prefill_ms']\n",
        "    \n",
        "    # Training time (simplified)\n",
        "    # FLOPs for training: batch_size \u00d7 (prompt_len + gen_len) \u00d7 6N per epoch\n",
        "    tokens_per_batch = batch_size * (prompt_len + gen_len)\n",
        "    flops_per_epoch = tokens_per_batch * 6 * model_params_b * 1e9\n",
        "    \n",
        "    # With data parallelism\n",
        "    if mode == 'sync':\n",
        "        training_gpus = total_gpus\n",
        "    else:\n",
        "        # In async mode, split GPUs\n",
        "        training_gpus = total_gpus // 2  # Half for training\n",
        "    \n",
        "    total_flops_per_epoch = flops_per_epoch\n",
        "    time_per_epoch_s = total_flops_per_epoch / (training_gpus * gpu_tflops * 1e12 * 0.5)  # 50% MFU\n",
        "    time_per_epoch_ms = time_per_epoch_s * 1000\n",
        "    t_train_ms = time_per_epoch_ms * num_ppo_epochs\n",
        "    \n",
        "    # Total time per iteration\n",
        "    if mode == 'sync':\n",
        "        # Sequential: generate + score + train\n",
        "        time_per_iter_ms = t_generate_ms + t_score_ms + t_train_ms\n",
        "    else:\n",
        "        # Parallel: max(generate + score, train)\n",
        "        time_per_iter_ms = max(t_generate_ms + t_score_ms, t_train_ms)\n",
        "    \n",
        "    # Throughput\n",
        "    samples_per_second = batch_size / (time_per_iter_ms / 1000)\n",
        "    tokens_per_second = (batch_size * gen_len) / (time_per_iter_ms / 1000)\n",
        "    \n",
        "    return {\n",
        "        't_generate_ms': t_generate_ms,\n",
        "        't_score_ms': t_score_ms,\n",
        "        't_train_ms': t_train_ms,\n",
        "        'time_per_iter_ms': time_per_iter_ms,\n",
        "        'samples_per_second': samples_per_second,\n",
        "        'tokens_per_second': tokens_per_second,\n",
        "        'speedup': 1.0  # Will be calculated later\n",
        "    }\n",
        "\n",
        "# Example: Compare sync vs async RL training\n",
        "print(\"RL Training Throughput Comparison:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "config = {\n",
        "    'model_params_b': 70,\n",
        "    'batch_size': 512,\n",
        "    'prompt_len': 100,\n",
        "    'gen_len': 50,\n",
        "    'num_ppo_epochs': 4,\n",
        "    'total_gpus': 256,\n",
        "}\n",
        "\n",
        "sync_result = estimate_rl_throughput(**config, mode='sync')\n",
        "async_result = estimate_rl_throughput(**config, mode='async')\n",
        "\n",
        "async_result['speedup'] = sync_result['time_per_iter_ms'] / async_result['time_per_iter_ms']\n",
        "\n",
        "print(f\"Configuration: {config['model_params_b']}B params, batch={config['batch_size']}, \"\n",
        "      f\"{config['total_gpus']} GPUs\")\n",
        "print()\n",
        "\n",
        "print(\"Synchronous Training:\")\n",
        "print(f\"  Generation time:  {sync_result['t_generate_ms']:7.1f} ms\")\n",
        "print(f\"  Scoring time:     {sync_result['t_score_ms']:7.1f} ms\")\n",
        "print(f\"  Training time:    {sync_result['t_train_ms']:7.1f} ms\")\n",
        "print(f\"  Total time:       {sync_result['time_per_iter_ms']:7.1f} ms\")\n",
        "print(f\"  Throughput:       {sync_result['samples_per_second']:7.1f} samples/s\")\n",
        "print(f\"                    {sync_result['tokens_per_second']:7.1f} tokens/s\")\n",
        "print()\n",
        "\n",
        "print(\"Asynchronous Training:\")\n",
        "print(f\"  Generation time:  {async_result['t_generate_ms']:7.1f} ms (parallel)\")\n",
        "print(f\"  Scoring time:     {async_result['t_score_ms']:7.1f} ms (parallel)\")\n",
        "print(f\"  Training time:    {async_result['t_train_ms']:7.1f} ms (parallel)\")\n",
        "print(f\"  Total time:       {async_result['time_per_iter_ms']:7.1f} ms\")\n",
        "print(f\"  Throughput:       {async_result['samples_per_second']:7.1f} samples/s\")\n",
        "print(f\"                    {async_result['tokens_per_second']:7.1f} tokens/s\")\n",
        "print()\n",
        "\n",
        "print(f\"Speedup (async vs sync): {async_result['speedup']:.2f}x\")\n",
        "print()\n",
        "print(\"Key Insight: Async training can provide significant speedup by overlapping\")\n",
        "print(\"generation and training, at the cost of more complexity and GPUs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: RL Training Comparison\n",
        "\n",
        "# Compare different batch sizes\n",
        "batch_sizes = [128, 256, 512, 1024]\n",
        "sync_throughputs = []\n",
        "async_throughputs = []\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    sync = estimate_rl_throughput(\n",
        "        model_params_b=70,\n",
        "        batch_size=bs,\n",
        "        prompt_len=100,\n",
        "        gen_len=50,\n",
        "        num_ppo_epochs=4,\n",
        "        total_gpus=256,\n",
        "        mode='sync'\n",
        "    )\n",
        "    async_rl = estimate_rl_throughput(\n",
        "        model_params_b=70,\n",
        "        batch_size=bs,\n",
        "        prompt_len=100,\n",
        "        gen_len=50,\n",
        "        num_ppo_epochs=4,\n",
        "        total_gpus=256,\n",
        "        mode='async'\n",
        "    )\n",
        "    sync_throughputs.append(sync['samples_per_second'])\n",
        "    async_throughputs.append(async_rl['samples_per_second'])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Throughput comparison\n",
        "x = np.arange(len(batch_sizes))\n",
        "width = 0.35\n",
        "\n",
        "ax1.bar(x - width/2, sync_throughputs, width, label='Synchronous', alpha=0.8)\n",
        "ax1.bar(x + width/2, async_throughputs, width, label='Asynchronous', alpha=0.8)\n",
        "\n",
        "ax1.set_xlabel('Batch Size', fontsize=12)\n",
        "ax1.set_ylabel('Throughput (samples/s)', fontsize=12)\n",
        "ax1.set_title('RL Training Throughput: Sync vs Async', fontsize=14)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(batch_sizes)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 2: Speedup\n",
        "speedups = [a / s for a, s in zip(async_throughputs, sync_throughputs)]\n",
        "ax2.plot(batch_sizes, speedups, marker='o', linewidth=2, markersize=8, color='green')\n",
        "ax2.axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='Baseline (sync)')\n",
        "\n",
        "ax2.set_xlabel('Batch Size', fontsize=12)\n",
        "ax2.set_ylabel('Speedup (async / sync)', fontsize=12)\n",
        "ax2.set_title('Async vs Sync Speedup', fontsize=14)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average speedup: {np.mean(speedups):.2f}x\")\n",
        "print(f\"\\nAsync training is particularly beneficial for RL where generation\")\n",
        "print(f\"and training can be done in parallel on different GPU sets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Key Takeaways for Interviews\n",
        "\n",
        "### Training Token Optimization:\n",
        "- \u2705 **Chinchilla optimal**: 20 tokens per parameter\n",
        "- \u2705 Real models often train beyond 20x for better quality\n",
        "- \u2705 MoE: Use active parameters for token calculation\n",
        "\n",
        "### FLOPs Calculations:\n",
        "- \u2705 **Training**: 6N FLOPs per token (2 FWD + 4 BWD)\n",
        "- \u2705 **BWD = 2 \u00d7 FWD**, **Total = 3 \u00d7 FWD**\n",
        "- \u2705 Use this to estimate GPU days\n",
        "\n",
        "### Parallelism Strategies:\n",
        "- \u2705 **TP**: Within node, use NVLink\n",
        "- \u2705 **PP**: Across nodes, minimize bubble with micro-batching\n",
        "- \u2705 **FSDP ZeRO-2**: Sweet spot for most training\n",
        "- \u274c **FSDP ZeRO-3**: Avoid - too much communication overhead\n",
        "- \u2705 **3D Parallelism**: TP \u00d7 PP \u00d7 DP for large scale\n",
        "\n",
        "### Communication:\n",
        "- \u2705 **TP**: 2 AllReduce per layer (attention + MLP)\n",
        "- \u2705 **ZeRO-3**: AllGather + ReduceScatter per layer (2x overhead)\n",
        "- \u2705 Fast interconnect is crucial (NVLink > InfiniBand > Ethernet)\n",
        "\n",
        "### Inference:\n",
        "- \u2705 **Prefill**: Compute-bound, can batch efficiently\n",
        "- \u2705 **Decode**: Memory-bound, limited by bandwidth\n",
        "- \u2705 **Time per token** \u2248 (Model size \u00d7 2 bytes) / Memory bandwidth\n",
        "- \u2705 **KV cache**: Major memory constraint for batching\n",
        "\n",
        "### RL Training:\n",
        "- \u2705 **Sync**: Simple, lower GPU utilization\n",
        "- \u2705 **Async**: Complex, higher throughput (1.5-2x speedup)\n",
        "- \u2705 Use async for production, sync for debugging\n",
        "\n",
        "### Interview Tips:\n",
        "1. Know the 6N rule for training FLOPs\n",
        "2. Understand when to use each parallelism strategy\n",
        "3. Be able to estimate GPU requirements from model size\n",
        "4. Explain why ZeRO-3 is rarely used (communication overhead)\n",
        "5. Understand inference is memory-bound during decode\n",
        "6. Know KV cache calculations for serving\n",
        "\n",
        "### Useful Rules of Thumb:\n",
        "- Training: 6N FLOPs per token\n",
        "- Inference decode: ~Model size / Memory bandwidth seconds per token\n",
        "- Memory: Model + Gradients + Optimizer + Activations \u2248 20 \u00d7 Model size\n",
        "- H100: ~990 TFLOPS, 3.35 TB/s memory bandwidth\n",
        "- Good MFU: 40-50%, Great MFU: 50-60%, Excellent: 60%+"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}