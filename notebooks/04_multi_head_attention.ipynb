{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention Mechanism\n",
    "\n",
    "This notebook demonstrates the Multi-Head Attention mechanism, a core component of Transformer architectures.\n",
    "\n",
    "We'll cover:\n",
    "1. Scaled Dot-Product Attention\n",
    "2. Multi-Head Attention implementation from scratch\n",
    "3. PyTorch's built-in MultiheadAttention\n",
    "4. Practical examples and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaled Dot-Product Attention\n",
    "\n",
    "The fundamental building block of multi-head attention:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        query: Query tensor of shape (batch_size, seq_len, d_k)\n",
    "        key: Key tensor of shape (batch_size, seq_len, d_k)\n",
    "        value: Value tensor of shape (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask tensor\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output of shape (batch_size, seq_len, d_v)\n",
    "        attention_weights: Attention weights of shape (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Apply attention weights to values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test scaled dot-product attention\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Query shape: {Q.shape}\")\n",
    "print(f\"Key shape: {K.shape}\")\n",
    "print(f\"Value shape: {V.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, title=\"Attention Weights\"):\n",
    "    \"\"\"Visualize attention weights as a heatmap.\"\"\"\n",
    "    # Take first sample from batch\n",
    "    attn = attention_weights[0].detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attn, annot=True, fmt='.3f', cmap='YlOrRd', cbar=True)\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention(attn_weights, \"Scaled Dot-Product Attention Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Attention Implementation\n",
    "\n",
    "Multi-head attention allows the model to attend to different representation subspaces:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of the model\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_k).\n",
    "        Transpose to shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Combine heads back to original shape.\n",
    "        Input shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        Output shape: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        return x.view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            query: Query tensor of shape (batch_size, seq_len, d_model)\n",
    "            key: Key tensor of shape (batch_size, seq_len, d_model)\n",
    "            value: Value tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask tensor\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output of shape (batch_size, seq_len, d_model)\n",
    "            attention_weights: Attention weights\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Compute attention for each head\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Combine heads\n",
    "        attention_output = self.combine_heads(attention_output)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Create multi-head attention module\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "print(f\"Multi-Head Attention Module:\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  num_heads: {num_heads}\")\n",
    "print(f\"  d_k (dimension per head): {d_model // num_heads}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in mha.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Self-attention (Q, K, V are all the same)\n",
    "mha.eval()\n",
    "with torch.no_grad():\n",
    "    output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  (batch_size, num_heads, seq_len, seq_len)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multihead_attention(attention_weights, num_heads_to_show=4):\n",
    "    \"\"\"\n",
    "    Visualize attention weights from multiple heads.\n",
    "    \"\"\"\n",
    "    # Take first sample from batch\n",
    "    attn = attention_weights[0].detach().numpy()  # (num_heads, seq_len, seq_len)\n",
    "    \n",
    "    num_heads = min(num_heads_to_show, attn.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_heads // 2, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_heads):\n",
    "        ax = axes[i]\n",
    "        sns.heatmap(attn[i], ax=ax, cmap='YlOrRd', cbar=True, square=True)\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "        ax.set_title(f'Head {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Multi-Head Attention Weights', y=1.02, fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize attention from multiple heads\n",
    "visualize_multihead_attention(attn_weights, num_heads_to_show=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare with PyTorch's Built-in MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch's built-in MultiheadAttention\n",
    "pytorch_mha = nn.MultiheadAttention(\n",
    "    embed_dim=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dropout=0.1,\n",
    "    batch_first=True  # Use batch_first=True for (batch, seq, feature) format\n",
    ")\n",
    "\n",
    "print(\"PyTorch's MultiheadAttention:\")\n",
    "print(pytorch_mha)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in pytorch_mha.parameters()):,}\")\n",
    "\n",
    "# Test PyTorch's implementation\n",
    "pytorch_mha.eval()\n",
    "with torch.no_grad():\n",
    "    pytorch_output, pytorch_attn_weights = pytorch_mha(x, x, x)\n",
    "\n",
    "print(f\"\\nPyTorch output shape: {pytorch_output.shape}\")\n",
    "print(f\"PyTorch attention weights shape: {pytorch_attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Example: Sentence Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example with meaningful tokens\n",
    "def create_sentence_example():\n",
    "    \"\"\"Create a simple example with interpretable attention.\"\"\"\n",
    "    # Simulate embeddings for a simple sentence\n",
    "    # \"The cat sat on the mat\"\n",
    "    seq_len = 6\n",
    "    d_model = 64\n",
    "    \n",
    "    # Create simple embeddings with some structure\n",
    "    embeddings = torch.randn(1, seq_len, d_model)\n",
    "    \n",
    "    # Make some positions more similar to demonstrate attention\n",
    "    embeddings[0, 1] = embeddings[0, 1] + 0.5 * embeddings[0, 4]  # \"cat\" attends to \"the\"\n",
    "    embeddings[0, 2] = embeddings[0, 2] + 0.5 * embeddings[0, 5]  # \"sat\" attends to \"mat\"\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Create example\n",
    "sentence_embeddings = create_sentence_example()\n",
    "words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "# Create a smaller multi-head attention for visualization\n",
    "small_mha = MultiHeadAttention(d_model=64, num_heads=4)\n",
    "small_mha.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, attn_weights = small_mha(sentence_embeddings, sentence_embeddings, sentence_embeddings)\n",
    "\n",
    "print(f\"Sentence: {' '.join(words)}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for the sentence\n",
    "def visualize_sentence_attention(attention_weights, words, num_heads_to_show=4):\n",
    "    \"\"\"Visualize attention weights with word labels.\"\"\"\n",
    "    attn = attention_weights[0].detach().numpy()\n",
    "    num_heads = min(num_heads_to_show, attn.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_heads, figsize=(16, 4))\n",
    "    \n",
    "    for i in range(num_heads):\n",
    "        ax = axes[i]\n",
    "        sns.heatmap(\n",
    "            attn[i],\n",
    "            ax=ax,\n",
    "            cmap='YlOrRd',\n",
    "            cbar=True,\n",
    "            xticklabels=words,\n",
    "            yticklabels=words,\n",
    "            square=True,\n",
    "            annot=True,\n",
    "            fmt='.2f'\n",
    "        )\n",
    "        ax.set_xlabel('Key (attending to)')\n",
    "        ax.set_ylabel('Query (from)')\n",
    "        ax.set_title(f'Head {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sentence Attention Weights', y=1.02, fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "visualize_sentence_attention(attn_weights, words, num_heads_to_show=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Masked Multi-Head Attention (for Decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal mask to prevent attention to future positions.\n",
    "    Used in decoder self-attention.\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dimensions\n",
    "\n",
    "# Create causal mask\n",
    "seq_len = 6\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal Mask (prevents attention to future positions):\")\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(causal_mask[0, 0].numpy(), annot=True, cmap='Blues', cbar=False, square=True)\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Causal Attention Mask')\n",
    "plt.show()\n",
    "\n",
    "# Apply masked attention\n",
    "with torch.no_grad():\n",
    "    masked_output, masked_attn = small_mha(sentence_embeddings, sentence_embeddings, sentence_embeddings, mask=causal_mask)\n",
    "\n",
    "print(f\"\\nMasked attention output shape: {masked_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize masked attention\n",
    "visualize_sentence_attention(masked_attn, words, num_heads_to_show=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Properties of Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Key Properties of Multi-Head Attention:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Multiple Representation Subspaces:\")\n",
    "print(\"   - Each head learns different attention patterns\")\n",
    "print(\"   - Captures different aspects of relationships\")\n",
    "print(\"   - Increases model capacity and expressiveness\")\n",
    "\n",
    "print(\"\\n2. Parallel Processing:\")\n",
    "print(\"   - All heads computed in parallel\")\n",
    "print(\"   - Efficient on modern hardware (GPUs/TPUs)\")\n",
    "print(\"   - No sequential dependencies\")\n",
    "\n",
    "print(\"\\n3. Permutation Invariance:\")\n",
    "print(\"   - Attention is permutation equivariant\")\n",
    "print(\"   - Position information must be added separately\")\n",
    "print(\"   - Usually via positional encodings\")\n",
    "\n",
    "print(\"\\n4. Variable Length Sequences:\")\n",
    "print(\"   - Can handle sequences of any length\")\n",
    "print(\"   - No recurrence required\")\n",
    "print(\"   - Enables efficient batching\")\n",
    "\n",
    "print(\"\\n5. Interpretability:\")\n",
    "print(\"   - Attention weights can be visualized\")\n",
    "print(\"   - Shows which positions model focuses on\")\n",
    "print(\"   - Helps understand model behavior\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nComputational Complexity:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Time: O(n² · d) where n=seq_len, d=d_model\")\n",
    "print(f\"  Space: O(n² + n·d)\")\n",
    "print(f\"  Quadratic in sequence length\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Scaled Dot-Product Attention**: The fundamental attention mechanism\n",
    "2. **Multi-Head Attention**: Implementation from scratch and PyTorch's version\n",
    "3. **Attention Visualization**: Heatmaps showing attention patterns\n",
    "4. **Practical Examples**: Sentence-level attention with interpretable results\n",
    "5. **Masked Attention**: Causal masking for decoder applications\n",
    "\n",
    "Key concepts:\n",
    "- **Query, Key, Value**: Three learned projections of the input\n",
    "- **Attention Scores**: Similarity between queries and keys\n",
    "- **Attention Weights**: Normalized scores (softmax)\n",
    "- **Multi-Head**: Multiple attention mechanisms in parallel\n",
    "- **Masking**: Control which positions can attend to each other\n",
    "\n",
    "Applications:\n",
    "- Transformers (BERT, GPT, T5, etc.)\n",
    "- Machine translation\n",
    "- Language modeling\n",
    "- Vision Transformers (ViT)\n",
    "- Any sequence-to-sequence tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
