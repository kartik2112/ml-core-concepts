{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Embeddings: ROPE, Sinusoidal, and Learned\n",
    "\n",
    "This notebook explores different positional embedding techniques used in transformer models.\n",
    "\n",
    "We'll cover:\n",
    "1. **Sinusoidal Positional Encodings** (original Transformer)\n",
    "2. **Learned Positional Embeddings** (BERT, GPT)\n",
    "3. **Rotary Position Embeddings (ROPE)** (modern LLMs like LLaMA, GPT-NeoX)\n",
    "4. **Comparison and visualizations**\n",
    "\n",
    "## Why Positional Embeddings?\n",
    "\n",
    "Transformers process all tokens in parallel, losing sequential information. Positional embeddings inject position information into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sinusoidal Positional Encodings\n",
    "\n",
    "The original Transformer paper used fixed sinusoidal functions:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "**Benefits:**\n",
    "- No learned parameters\n",
    "- Can extrapolate to longer sequences\n",
    "- Encodes relative positions through linear transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding as in 'Attention is All You Need'.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create position encodings matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create division term for wavelengths\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x with positional encoding added\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:seq_len, :].unsqueeze(0)\n",
    "\n",
    "# Example usage\n",
    "d_model = 128\n",
    "seq_len = 50\n",
    "batch_size = 4\n",
    "\n",
    "sinusoidal_pe = SinusoidalPositionalEncoding(d_model)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "x_with_pe = sinusoidal_pe(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {x_with_pe.shape}\")\n",
    "print(f\"Positional encoding shape: {sinusoidal_pe.pe.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sinusoidal Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the positional encoding matrix\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Show first 100 positions and all dimensions\n",
    "pe_matrix = sinusoidal_pe.pe[:100, :].numpy()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(pe_matrix.T, aspect='auto', cmap='RdBu')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Sinusoidal Positional Encoding Heatmap')\n",
    "plt.colorbar(label='Value')\n",
    "\n",
    "# Show some specific dimensions over position\n",
    "plt.subplot(1, 2, 2)\n",
    "positions = np.arange(100)\n",
    "for dim in [0, 1, 10, 20, 50]:\n",
    "    plt.plot(positions, pe_matrix[:, dim], label=f'dim {dim}')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Encoding Value')\n",
    "plt.title('Positional Encoding Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Different dimensions have different wavelengths.\")\n",
    "print(\"Lower dimensions change slowly, higher dimensions change quickly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learned Positional Embeddings\n",
    "\n",
    "Models like BERT and GPT-2 use learned positional embeddings:\n",
    "- Simple lookup table of learned vectors\n",
    "- Each position has its own learned embedding\n",
    "- More flexible but limited to training sequence length\n",
    "\n",
    "**Trade-offs:**\n",
    "- ✅ Can learn task-specific position patterns\n",
    "- ❌ Cannot extrapolate beyond max_len seen during training\n",
    "- ❌ Requires storing parameters for each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned positional embeddings as in BERT and GPT.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        self.positional_embeddings = nn.Embedding(max_len, d_model)\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x with positional embedding added\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds max_len {self.max_len}\")\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.positional_embeddings(positions)\n",
    "        \n",
    "        return x + pos_emb\n",
    "\n",
    "# Example usage\n",
    "learned_pe = LearnedPositionalEmbedding(d_model, max_len=512)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "x_with_learned_pe = learned_pe(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {x_with_learned_pe.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in learned_pe.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rotary Position Embeddings (ROPE)\n",
    "\n",
    "ROPE is used in modern LLMs (LLaMA, GPT-NeoX, PaLM). It rotates query and key vectors based on position.\n",
    "\n",
    "**Key idea:** Apply position-dependent rotation to Q and K:\n",
    "- Relative positions are preserved through dot products\n",
    "- Works well with long sequences\n",
    "- Better extrapolation than learned embeddings\n",
    "\n",
    "**Mathematical formulation:**\n",
    "$$f_q(x_m, m) = (W_q x_m) e^{im\\theta}$$\n",
    "$$f_k(x_n, n) = (W_k x_n) e^{in\\theta}$$\n",
    "\n",
    "The attention score becomes:\n",
    "$$f_q(x_m, m)^T f_k(x_n, n) = (W_q x_m)^T (W_k x_n) e^{i(m-n)\\theta}$$\n",
    "\n",
    "**Benefits:**\n",
    "- ✅ Encodes relative position in attention scores\n",
    "- ✅ Good extrapolation to longer sequences\n",
    "- ✅ No extra parameters\n",
    "- ✅ Works in 2D complex space (pairs of dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Position Embedding (ROPE) as used in LLaMA and other modern LLMs.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        \n",
    "        # Compute frequencies for each dimension pair\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Cache cos and sin values\n",
    "        self._set_cos_sin_cache(max_seq_len)\n",
    "    \n",
    "    def _set_cos_sin_cache(self, seq_len):\n",
    "        \"\"\"Precompute cos and sin values for efficiency.\"\"\"\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq)\n",
    "        \n",
    "        # Compute position * frequency for all positions and frequencies\n",
    "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
    "        \n",
    "        # Concatenate to match dimension (each freq used for 2 dimensions)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        \n",
    "        self.register_buffer('cos_cached', emb.cos())\n",
    "        self.register_buffer('sin_cached', emb.sin())\n",
    "    \n",
    "    def forward(self, x, seq_dim=1):\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (..., seq_len, ..., dim)\n",
    "            seq_dim: Dimension index for sequence length\n",
    "        \n",
    "        Returns:\n",
    "            Tensor with rotary embeddings applied\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[seq_dim]\n",
    "        \n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len)\n",
    "        \n",
    "        return self.apply_rotary_emb(x, self.cos_cached[:seq_len], self.sin_cached[:seq_len])\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_rotary_emb(x, cos, sin):\n",
    "        \"\"\"Apply rotation using cos and sin.\"\"\"\n",
    "        # Split x into pairs: [x0, x1, x2, x3, ...] -> [[x0, x1], [x2, x3], ...]\n",
    "        x1 = x[..., ::2]\n",
    "        x2 = x[..., 1::2]\n",
    "        \n",
    "        # Apply rotation\n",
    "        # [cos * x1 - sin * x2, sin * x1 + cos * x2]\n",
    "        cos = cos.unsqueeze(0)  # Add batch dimension\n",
    "        sin = sin.unsqueeze(0)\n",
    "        \n",
    "        # Interleave the rotated pairs\n",
    "        x_rotated = torch.stack([\n",
    "            x1 * cos[..., ::2] - x2 * sin[..., ::2],\n",
    "            x1 * sin[..., 1::2] + x2 * cos[..., 1::2]\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return x_rotated.flatten(-2)\n",
    "\n",
    "# Example usage\n",
    "head_dim = 64  # Typical attention head dimension\n",
    "rope = RotaryPositionalEmbedding(head_dim)\n",
    "\n",
    "# Apply to query and key\n",
    "q = torch.randn(batch_size, seq_len, 8, head_dim)  # 8 heads\n",
    "k = torch.randn(batch_size, seq_len, 8, head_dim)\n",
    "\n",
    "print(f\"Query shape: {q.shape}\")\n",
    "print(f\"Key shape: {k.shape}\")\n",
    "\n",
    "# Apply ROPE to each head\n",
    "q_rotated = rope(q, seq_dim=1)\n",
    "k_rotated = rope(k, seq_dim=1)\n",
    "\n",
    "print(f\"Query with ROPE shape: {q_rotated.shape}\")\n",
    "print(f\"Key with ROPE shape: {k_rotated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize ROPE Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ROPE cos and sin values\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Show cos values\n",
    "plt.subplot(3, 2, 1)\n",
    "cos_matrix = rope.cos_cached[:100, :].cpu().numpy()\n",
    "plt.imshow(cos_matrix.T, aspect='auto', cmap='RdBu')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('ROPE: Cosine Values')\n",
    "plt.colorbar(label='Value')\n",
    "\n",
    "# Show sin values\n",
    "plt.subplot(3, 2, 2)\n",
    "sin_matrix = rope.sin_cached[:100, :].cpu().numpy()\n",
    "plt.imshow(sin_matrix.T, aspect='auto', cmap='RdBu')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('ROPE: Sine Values')\n",
    "plt.colorbar(label='Value')\n",
    "\n",
    "# Show frequency spectrum\n",
    "plt.subplot(3, 2, 3)\n",
    "inv_freq = rope.inv_freq.cpu().numpy()\n",
    "plt.plot(inv_freq)\n",
    "plt.xlabel('Dimension Index')\n",
    "plt.ylabel('Inverse Frequency')\n",
    "plt.title('ROPE: Frequency Spectrum')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show cos for specific dimensions\n",
    "plt.subplot(3, 2, 4)\n",
    "positions = np.arange(100)\n",
    "for dim in [0, 5, 15, 31]:\n",
    "    plt.plot(positions, cos_matrix[:, dim], label=f'dim {dim}')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Cosine Value')\n",
    "plt.title('ROPE: Cosine Curves by Dimension')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Demonstrate position encoding effect\n",
    "plt.subplot(3, 2, 5)\n",
    "# Sample vector\n",
    "sample_vec = torch.randn(1, 1, head_dim)\n",
    "positions_to_show = [0, 10, 20, 30, 40]\n",
    "rotated_vecs = []\n",
    "for pos in positions_to_show:\n",
    "    sample_at_pos = torch.zeros(1, 50, head_dim)\n",
    "    sample_at_pos[:, pos, :] = sample_vec\n",
    "    rotated = rope(sample_at_pos, seq_dim=1)\n",
    "    rotated_vecs.append(rotated[0, pos, :].cpu().detach().numpy())\n",
    "\n",
    "for i, pos in enumerate(positions_to_show):\n",
    "    plt.plot(rotated_vecs[i], label=f'pos {pos}', alpha=0.7)\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Value after ROPE')\n",
    "plt.title('Same Vector at Different Positions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Relative position encoding visualization\n",
    "plt.subplot(3, 2, 6)\n",
    "# Compute dot product between vectors at different relative positions\n",
    "base_pos = 10\n",
    "relative_positions = list(range(-10, 11))\n",
    "similarities = []\n",
    "\n",
    "for rel_pos in relative_positions:\n",
    "    target_pos = base_pos + rel_pos\n",
    "    if 0 <= target_pos < 50:\n",
    "        q_vec = torch.randn(1, 50, head_dim)\n",
    "        k_vec = q_vec.clone()  # Same vector\n",
    "        q_rot = rope(q_vec, seq_dim=1)\n",
    "        k_rot = rope(k_vec, seq_dim=1)\n",
    "        \n",
    "        sim = (q_rot[0, base_pos] @ k_rot[0, target_pos]).item()\n",
    "        similarities.append(sim)\n",
    "    else:\n",
    "        similarities.append(np.nan)\n",
    "\n",
    "plt.plot(relative_positions, similarities, marker='o')\n",
    "plt.xlabel('Relative Position')\n",
    "plt.ylabel('Dot Product (Similarity)')\n",
    "plt.title('ROPE Preserves Relative Position Information')\n",
    "plt.axvline(x=0, color='red', linestyle='--', label='Same position')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"1. Different dimensions rotate at different frequencies\")\n",
    "print(\"2. Lower dimensions (low frequency) capture long-range position info\")\n",
    "print(\"3. Higher dimensions (high frequency) capture fine-grained position info\")\n",
    "print(\"4. Dot product between rotated vectors depends only on relative position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ROPE in Attention Mechanism\n",
    "\n",
    "Let's implement a complete attention layer with ROPE to see how it's used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention with Rotary Position Embeddings (ROPE).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # ROPE for Q and K\n",
    "        self.rope = RotaryPositionalEmbedding(self.head_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        Returns:\n",
    "            Output tensor (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x)  # (batch, seq_len, d_model)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Apply ROPE to Q and K (NOT to V!)\n",
    "        q = self.rope(q, seq_dim=1)\n",
    "        k = self.rope(k, seq_dim=1)\n",
    "        \n",
    "        # Transpose for attention computation: (batch, num_heads, seq_len, head_dim)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax and dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        return out, attn_weights\n",
    "\n",
    "# Example usage\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "seq_len = 32\n",
    "batch_size = 4\n",
    "\n",
    "rope_attn = RoPEAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = rope_attn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nNumber of parameters: {sum(p.numel() for p in rope_attn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: Sinusoidal vs Learned vs ROPE\n",
    "\n",
    "Let's compare the three approaches on key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Property': [\n",
    "        'Parameters',\n",
    "        'Extrapolation',\n",
    "        'Relative Position',\n",
    "        'Memory Cost',\n",
    "        'Computation Cost',\n",
    "        'Length Generalization',\n",
    "        'Used In'\n",
    "    ],\n",
    "    'Sinusoidal': [\n",
    "        'None (fixed)',\n",
    "        'Excellent',\n",
    "        'Implicit',\n",
    "        'Low (cache)',\n",
    "        'Low',\n",
    "        'Good',\n",
    "        'Original Transformer'\n",
    "    ],\n",
    "    'Learned': [\n",
    "        'max_len × d_model',\n",
    "        'Poor',\n",
    "        'Absolute only',\n",
    "        'High',\n",
    "        'Low',\n",
    "        'Poor',\n",
    "        'BERT, GPT-2'\n",
    "    ],\n",
    "    'ROPE': [\n",
    "        'None (fixed)',\n",
    "        'Excellent',\n",
    "        'Explicit in QK^T',\n",
    "        'Low (cache)',\n",
    "        'Medium',\n",
    "        'Excellent',\n",
    "        'LLaMA, GPT-NeoX, PaLM'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n=== Positional Embedding Comparison ===\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Visualize parameter counts\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "max_len = 512\n",
    "d_model_val = 768\n",
    "\n",
    "param_counts = {\n",
    "    'Sinusoidal': 0,\n",
    "    'Learned': max_len * d_model_val,\n",
    "    'ROPE': 0\n",
    "}\n",
    "\n",
    "plt.bar(param_counts.keys(), param_counts.values(), color=['blue', 'orange', 'green'])\n",
    "plt.ylabel('Number of Parameters')\n",
    "plt.title(f'Parameter Count Comparison\\n(max_len={max_len}, d_model={d_model_val})')\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (name, count) in enumerate(param_counts.items()):\n",
    "    plt.text(i, count, f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Length Extrapolation Test\n",
    "\n",
    "Test how well each method handles sequences longer than seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_length_extrapolation():\n",
    "    \"\"\"\n",
    "    Test how positional encodings handle sequences longer than training length.\n",
    "    \"\"\"\n",
    "    d_model = 128\n",
    "    train_len = 64\n",
    "    test_len = 128  # Double the training length\n",
    "    \n",
    "    print(\"Testing length extrapolation...\")\n",
    "    print(f\"Training length: {train_len}\")\n",
    "    print(f\"Test length: {test_len}\\n\")\n",
    "    \n",
    "    # Sinusoidal: Works fine\n",
    "    sin_pe = SinusoidalPositionalEncoding(d_model, max_len=train_len)\n",
    "    try:\n",
    "        # Extend the cache\n",
    "        sin_pe_extended = SinusoidalPositionalEncoding(d_model, max_len=test_len)\n",
    "        x_test = torch.randn(1, test_len, d_model)\n",
    "        out = sin_pe_extended(x_test)\n",
    "        print(\"✅ Sinusoidal: Successfully handled longer sequence\")\n",
    "        print(f\"   Output shape: {out.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Sinusoidal failed: {e}\")\n",
    "    \n",
    "    # Learned: Fails without retraining\n",
    "    learned_pe = LearnedPositionalEmbedding(d_model, max_len=train_len)\n",
    "    try:\n",
    "        x_test = torch.randn(1, test_len, d_model)\n",
    "        out = learned_pe(x_test)\n",
    "        print(\"✅ Learned: Successfully handled longer sequence\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Learned: Failed with longer sequence\")\n",
    "        print(f\"   Error: {type(e).__name__}: {e}\")\n",
    "    \n",
    "    # ROPE: Works fine\n",
    "    rope = RotaryPositionalEmbedding(d_model, max_seq_len=train_len)\n",
    "    try:\n",
    "        x_test = torch.randn(1, test_len, d_model)\n",
    "        out = rope(x_test, seq_dim=1)\n",
    "        print(\"✅ ROPE: Successfully handled longer sequence\")\n",
    "        print(f\"   Output shape: {out.shape}\")\n",
    "        print(f\"   Cache automatically extended to {rope.max_seq_len_cached}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ROPE failed: {e}\")\n",
    "\n",
    "test_length_extrapolation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### Sinusoidal Positional Encoding\n",
    "- ✅ No learnable parameters\n",
    "- ✅ Good extrapolation to longer sequences\n",
    "- ✅ Computationally efficient\n",
    "- ❌ Less flexible than learned embeddings\n",
    "\n",
    "### Learned Positional Embeddings\n",
    "- ✅ Can learn task-specific patterns\n",
    "- ✅ Simple to implement\n",
    "- ❌ Poor extrapolation beyond training length\n",
    "- ❌ Requires storage for each position\n",
    "\n",
    "### Rotary Position Embeddings (ROPE)\n",
    "- ✅ No learnable parameters\n",
    "- ✅ Excellent extrapolation\n",
    "- ✅ Explicitly encodes relative positions\n",
    "- ✅ State-of-the-art for long context LLMs\n",
    "- ⚠️ Slightly more complex to implement\n",
    "\n",
    "### When to Use Each?\n",
    "\n",
    "1. **Use Sinusoidal** for:\n",
    "   - Research and experimentation\n",
    "   - When you need simple, parameter-free positional encoding\n",
    "   - Standard sequence lengths\n",
    "\n",
    "2. **Use Learned** for:\n",
    "   - Fixed-length sequences (e.g., BERT-style models)\n",
    "   - When you want the model to learn position patterns\n",
    "   - Classification tasks with fixed input size\n",
    "\n",
    "3. **Use ROPE** for:\n",
    "   - Modern LLMs and autoregressive models\n",
    "   - Long context windows\n",
    "   - When relative position is more important than absolute\n",
    "   - Production LLM systems\n",
    "\n",
    "### Modern Trends\n",
    "- **ROPE is dominant** in state-of-the-art LLMs (LLaMA, GPT-NeoX, PaLM)\n",
    "- **ALiBi** (Attention with Linear Biases) is another modern alternative\n",
    "- Research continues on even better position encoding methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "In this notebook, we explored three major positional embedding techniques:\n",
    "\n",
    "1. **Sinusoidal Encodings**: Fixed, parameter-free, good for standard transformers\n",
    "2. **Learned Embeddings**: Flexible but limited to training sequence lengths\n",
    "3. **ROPE**: Modern, efficient, excellent for long-context LLMs\n",
    "\n",
    "ROPE has become the de facto standard for modern large language models due to its:\n",
    "- Zero additional parameters\n",
    "- Excellent length extrapolation\n",
    "- Explicit relative position encoding\n",
    "- Strong empirical performance\n",
    "\n",
    "Understanding these positional embedding methods is crucial for working with modern transformer architectures and large language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
