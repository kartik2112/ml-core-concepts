{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure NumPy MLP with Parallelism Strategies\n",
    "\n",
    "This notebook demonstrates pure NumPy implementations of Multi-Layer Perceptrons (MLPs) with various parallelism strategies:\n",
    "\n",
    "1. **2-Layer MLP with ReLU** - Basic implementation with forward and backward passes\n",
    "2. **Batch Dimension** - Support for batch processing\n",
    "3. **Row-Sharded (Tensor-Parallel) MLP** - Splitting weight matrices by rows\n",
    "4. **Column-Parallel Sharded MLP** - Splitting weight matrices by columns\n",
    "5. **Data-Parallel MLP** - Splitting data across workers\n",
    "\n",
    "All implementations include:\n",
    "- Forward pass (FWD)\n",
    "- Backward pass (BWD) with gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Derivative of ReLU activation.\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def mse_loss(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    \"\"\"Mean Squared Error loss.\"\"\"\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def mse_loss_derivative(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Derivative of MSE loss.\"\"\"\n",
    "    n = y_pred.size\n",
    "    return 2 * (y_pred - y_true) / n\n",
    "\n",
    "def initialize_weights(input_size: int, output_size: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Initialize weights and biases using He initialization.\"\"\"\n",
    "    W = np.random.randn(input_size, output_size) * np.sqrt(2.0 / input_size)\n",
    "    b = np.zeros((1, output_size))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic 2-Layer MLP with ReLU (Forward + Backward)\n",
    "\n",
    "### Architecture:\n",
    "- Input layer: `input_size` dimensions\n",
    "- Hidden layer: `hidden_size` dimensions with ReLU activation\n",
    "- Output layer: `output_size` dimensions\n",
    "\n",
    "### Forward Pass:\n",
    "```\n",
    "z1 = X @ W1 + b1\n",
    "a1 = ReLU(z1)\n",
    "z2 = a1 @ W2 + b2\n",
    "y_pred = z2\n",
    "```\n",
    "\n",
    "### Backward Pass:\n",
    "```\n",
    "dz2 = dL/dy_pred\n",
    "dW2 = a1.T @ dz2\n",
    "db2 = sum(dz2, axis=0)\n",
    "da1 = dz2 @ W2.T\n",
    "dz1 = da1 * ReLU'(z1)\n",
    "dW1 = X.T @ dz1\n",
    "db1 = sum(dz1, axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerMLP:\n",
    "    \"\"\"2-Layer MLP with ReLU activation - Pure NumPy implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W1, self.b1 = initialize_weights(input_size, hidden_size)\n",
    "        self.W2, self.b2 = initialize_weights(hidden_size, output_size)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data of shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            Output of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Layer 1: Linear + ReLU\n",
    "        z1 = X @ self.W1 + self.b1  # (batch_size, hidden_size)\n",
    "        a1 = relu(z1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Layer 2: Linear\n",
    "        z2 = a1 @ self.W2 + self.b2  # (batch_size, output_size)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {\n",
    "            'X': X,\n",
    "            'z1': z1,\n",
    "            'a1': a1,\n",
    "            'z2': z2\n",
    "        }\n",
    "        \n",
    "        return z2\n",
    "    \n",
    "    def backward(self, dL_dy: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        \n",
    "        Args:\n",
    "            dL_dy: Gradient of loss w.r.t. output, shape (batch_size, output_size)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing gradients for all parameters\n",
    "        \"\"\"\n",
    "        X = self.cache['X']\n",
    "        z1 = self.cache['z1']\n",
    "        a1 = self.cache['a1']\n",
    "        \n",
    "        # Gradient for Layer 2\n",
    "        dz2 = dL_dy  # (batch_size, output_size)\n",
    "        dW2 = a1.T @ dz2  # (hidden_size, output_size)\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)  # (1, output_size)\n",
    "        \n",
    "        # Gradient for Layer 1\n",
    "        da1 = dz2 @ self.W2.T  # (batch_size, hidden_size)\n",
    "        dz1 = da1 * relu_derivative(z1)  # (batch_size, hidden_size)\n",
    "        dW1 = X.T @ dz1  # (input_size, hidden_size)\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)  # (1, hidden_size)\n",
    "        \n",
    "        gradients = {\n",
    "            'dW1': dW1,\n",
    "            'db1': db1,\n",
    "            'dW2': dW2,\n",
    "            'db2': db2\n",
    "        }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients: dict, learning_rate: float):\n",
    "        \"\"\"Update parameters using gradient descent.\"\"\"\n",
    "        self.W1 -= learning_rate * gradients['dW1']\n",
    "        self.b1 -= learning_rate * gradients['db1']\n",
    "        self.W2 -= learning_rate * gradients['dW2']\n",
    "        self.b2 -= learning_rate * gradients['db2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2-Layer MLP with Batch Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "batch_size = 32\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "\n",
    "X = np.random.randn(batch_size, input_size)\n",
    "y_true = np.random.randn(batch_size, output_size)\n",
    "\n",
    "# Initialize model\n",
    "model = TwoLayerMLP(input_size, hidden_size, output_size)\n",
    "\n",
    "# Forward pass\n",
    "y_pred = model.forward(X)\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {y_pred.shape}\")\n",
    "print(f\"Initial loss: {loss:.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "dL_dy = mse_loss_derivative(y_pred, y_true)\n",
    "gradients = model.backward(dL_dy)\n",
    "\n",
    "print(\"\\nGradient shapes:\")\n",
    "for name, grad in gradients.items():\n",
    "    print(f\"  {name}: {grad.shape}\")\n",
    "\n",
    "# Train for a few iterations\n",
    "learning_rate = 0.01\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    y_pred = model.forward(X)\n",
    "    loss = mse_loss(y_pred, y_true)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    dL_dy = mse_loss_derivative(y_pred, y_true)\n",
    "    gradients = model.backward(dL_dy)\n",
    "    \n",
    "    # Update parameters\n",
    "    model.update_parameters(gradients, learning_rate)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss - 2-Layer MLP')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Row-Sharded (Tensor-Parallel) MLP\n",
    "\n",
    "In row-sharding (also known as tensor parallelism), we split the weight matrices by rows across multiple workers.\n",
    "\n",
    "### For Layer 1 (Input \u2192 Hidden):\n",
    "- Weight matrix W1: (input_size, hidden_size)\n",
    "- Split W1 by rows into N shards: W1_shard_i has shape (input_size // N, hidden_size)\n",
    "- Each worker processes a portion of the input\n",
    "\n",
    "### Forward Pass:\n",
    "```\n",
    "Each worker i:\n",
    "  X_shard_i = X[:, shard_i_indices]\n",
    "  z1_partial_i = X_shard_i @ W1_shard_i\n",
    "All-Reduce: z1 = sum(z1_partial_i) + b1\n",
    "a1 = ReLU(z1)\n",
    "```\n",
    "\n",
    "### Backward Pass:\n",
    "```\n",
    "Each worker i:\n",
    "  dW1_shard_i = X_shard_i.T @ dz1\n",
    "  dX_shard_i = dz1 @ W1_shard_i.T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowShardedMLP:\n",
    "    \"\"\"Row-Sharded (Tensor-Parallel) MLP - Pure NumPy implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_shards: int):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_shards = num_shards\n",
    "        \n",
    "        assert input_size % num_shards == 0, \"input_size must be divisible by num_shards\"\n",
    "        self.shard_size = input_size // num_shards\n",
    "        \n",
    "        # Initialize sharded weights for Layer 1\n",
    "        self.W1_shards = []\n",
    "        for i in range(num_shards):\n",
    "            W_shard, _ = initialize_weights(self.shard_size, hidden_size)\n",
    "            self.W1_shards.append(W_shard)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Layer 2 is not sharded (for simplicity)\n",
    "        self.W2, self.b2 = initialize_weights(hidden_size, output_size)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass with row-sharding.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data of shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            Output of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Split input by columns (each shard gets a portion of features)\n",
    "        X_shards = np.split(X, self.num_shards, axis=1)\n",
    "        \n",
    "        # Compute partial outputs on each shard\n",
    "        z1_partials = []\n",
    "        for i in range(self.num_shards):\n",
    "            z1_partial = X_shards[i] @ self.W1_shards[i]  # (batch_size, hidden_size)\n",
    "            z1_partials.append(z1_partial)\n",
    "        \n",
    "        # All-Reduce: Sum partial results\n",
    "        z1 = np.sum(z1_partials, axis=0) + self.b1  # (batch_size, hidden_size)\n",
    "        a1 = relu(z1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Layer 2 (not sharded)\n",
    "        z2 = a1 @ self.W2 + self.b2  # (batch_size, output_size)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {\n",
    "            'X_shards': X_shards,\n",
    "            'z1': z1,\n",
    "            'a1': a1,\n",
    "            'z2': z2\n",
    "        }\n",
    "        \n",
    "        return z2\n",
    "    \n",
    "    def backward(self, dL_dy: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Backward pass with row-sharding.\n",
    "        \n",
    "        Args:\n",
    "            dL_dy: Gradient of loss w.r.t. output, shape (batch_size, output_size)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing gradients for all parameters\n",
    "        \"\"\"\n",
    "        X_shards = self.cache['X_shards']\n",
    "        z1 = self.cache['z1']\n",
    "        a1 = self.cache['a1']\n",
    "        \n",
    "        # Gradient for Layer 2\n",
    "        dz2 = dL_dy  # (batch_size, output_size)\n",
    "        dW2 = a1.T @ dz2  # (hidden_size, output_size)\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)  # (1, output_size)\n",
    "        \n",
    "        # Gradient for Layer 1\n",
    "        da1 = dz2 @ self.W2.T  # (batch_size, hidden_size)\n",
    "        dz1 = da1 * relu_derivative(z1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Compute gradients for each shard\n",
    "        dW1_shards = []\n",
    "        for i in range(self.num_shards):\n",
    "            dW1_shard = X_shards[i].T @ dz1  # (shard_size, hidden_size)\n",
    "            dW1_shards.append(dW1_shard)\n",
    "        \n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)  # (1, hidden_size)\n",
    "        \n",
    "        gradients = {\n",
    "            'dW1_shards': dW1_shards,\n",
    "            'db1': db1,\n",
    "            'dW2': dW2,\n",
    "            'db2': db2\n",
    "        }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients: dict, learning_rate: float):\n",
    "        \"\"\"Update parameters using gradient descent.\"\"\"\n",
    "        for i in range(self.num_shards):\n",
    "            self.W1_shards[i] -= learning_rate * gradients['dW1_shards'][i]\n",
    "        self.b1 -= learning_rate * gradients['db1']\n",
    "        self.W2 -= learning_rate * gradients['dW2']\n",
    "        self.b2 -= learning_rate * gradients['db2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Row-Sharded MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "batch_size = 32\n",
    "input_size = 16  # Must be divisible by num_shards\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "num_shards = 4\n",
    "\n",
    "X = np.random.randn(batch_size, input_size)\n",
    "y_true = np.random.randn(batch_size, output_size)\n",
    "\n",
    "# Initialize row-sharded model\n",
    "row_model = RowShardedMLP(input_size, hidden_size, output_size, num_shards)\n",
    "\n",
    "print(f\"Number of shards: {num_shards}\")\n",
    "print(f\"Shard size: {row_model.shard_size}\")\n",
    "print(f\"\\nWeight shard shapes:\")\n",
    "for i, W_shard in enumerate(row_model.W1_shards):\n",
    "    print(f\"  W1_shard_{i}: {W_shard.shape}\")\n",
    "\n",
    "# Train for a few iterations\n",
    "learning_rate = 0.01\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    y_pred = row_model.forward(X)\n",
    "    loss = mse_loss(y_pred, y_true)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    dL_dy = mse_loss_derivative(y_pred, y_true)\n",
    "    gradients = row_model.backward(dL_dy)\n",
    "    \n",
    "    # Update parameters\n",
    "    row_model.update_parameters(gradients, learning_rate)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss - Row-Sharded MLP')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Column-Parallel Sharded MLP\n",
    "\n",
    "In column-sharding, we split the weight matrices by columns across multiple workers.\n",
    "\n",
    "### For Layer 1 (Input \u2192 Hidden):\n",
    "- Weight matrix W1: (input_size, hidden_size)\n",
    "- Split W1 by columns into N shards: W1_shard_i has shape (input_size, hidden_size // N)\n",
    "- Each worker computes a portion of the hidden layer\n",
    "\n",
    "### Forward Pass:\n",
    "```\n",
    "Each worker i:\n",
    "  z1_shard_i = X @ W1_shard_i + b1_shard_i\n",
    "  a1_shard_i = ReLU(z1_shard_i)\n",
    "Concatenate: a1 = concat([a1_shard_0, ..., a1_shard_N])\n",
    "```\n",
    "\n",
    "### Backward Pass:\n",
    "```\n",
    "Split da1 by columns\n",
    "Each worker i:\n",
    "  dz1_shard_i = da1_shard_i * ReLU'(z1_shard_i)\n",
    "  dW1_shard_i = X.T @ dz1_shard_i\n",
    "All-Reduce: dX = sum(dz1_shard_i @ W1_shard_i.T)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnShardedMLP:\n",
    "    \"\"\"Column-Parallel Sharded MLP - Pure NumPy implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_shards: int):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_shards = num_shards\n",
    "        \n",
    "        assert hidden_size % num_shards == 0, \"hidden_size must be divisible by num_shards\"\n",
    "        self.shard_size = hidden_size // num_shards\n",
    "        \n",
    "        # Initialize sharded weights for Layer 1 (column-wise)\n",
    "        self.W1_shards = []\n",
    "        self.b1_shards = []\n",
    "        for i in range(num_shards):\n",
    "            W_shard, b_shard = initialize_weights(input_size, self.shard_size)\n",
    "            self.W1_shards.append(W_shard)\n",
    "            self.b1_shards.append(b_shard)\n",
    "        \n",
    "        # Layer 2 is not sharded (for simplicity)\n",
    "        self.W2, self.b2 = initialize_weights(hidden_size, output_size)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass with column-sharding.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data of shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            Output of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute outputs on each shard (each produces a portion of hidden layer)\n",
    "        z1_shards = []\n",
    "        a1_shards = []\n",
    "        for i in range(self.num_shards):\n",
    "            z1_shard = X @ self.W1_shards[i] + self.b1_shards[i]  # (batch_size, shard_size)\n",
    "            a1_shard = relu(z1_shard)  # (batch_size, shard_size)\n",
    "            z1_shards.append(z1_shard)\n",
    "            a1_shards.append(a1_shard)\n",
    "        \n",
    "        # Concatenate shard outputs\n",
    "        a1 = np.concatenate(a1_shards, axis=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Layer 2 (not sharded)\n",
    "        z2 = a1 @ self.W2 + self.b2  # (batch_size, output_size)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {\n",
    "            'X': X,\n",
    "            'z1_shards': z1_shards,\n",
    "            'a1_shards': a1_shards,\n",
    "            'a1': a1,\n",
    "            'z2': z2\n",
    "        }\n",
    "        \n",
    "        return z2\n",
    "    \n",
    "    def backward(self, dL_dy: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Backward pass with column-sharding.\n",
    "        \n",
    "        Args:\n",
    "            dL_dy: Gradient of loss w.r.t. output, shape (batch_size, output_size)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing gradients for all parameters\n",
    "        \"\"\"\n",
    "        X = self.cache['X']\n",
    "        z1_shards = self.cache['z1_shards']\n",
    "        a1 = self.cache['a1']\n",
    "        \n",
    "        # Gradient for Layer 2\n",
    "        dz2 = dL_dy  # (batch_size, output_size)\n",
    "        dW2 = a1.T @ dz2  # (hidden_size, output_size)\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)  # (1, output_size)\n",
    "        \n",
    "        # Gradient for Layer 1\n",
    "        da1 = dz2 @ self.W2.T  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Split da1 by columns (corresponding to shards)\n",
    "        da1_shards = np.split(da1, self.num_shards, axis=1)\n",
    "        \n",
    "        # Compute gradients for each shard\n",
    "        dW1_shards = []\n",
    "        db1_shards = []\n",
    "        for i in range(self.num_shards):\n",
    "            dz1_shard = da1_shards[i] * relu_derivative(z1_shards[i])  # (batch_size, shard_size)\n",
    "            dW1_shard = X.T @ dz1_shard  # (input_size, shard_size)\n",
    "            db1_shard = np.sum(dz1_shard, axis=0, keepdims=True)  # (1, shard_size)\n",
    "            \n",
    "            dW1_shards.append(dW1_shard)\n",
    "            db1_shards.append(db1_shard)\n",
    "        \n",
    "        gradients = {\n",
    "            'dW1_shards': dW1_shards,\n",
    "            'db1_shards': db1_shards,\n",
    "            'dW2': dW2,\n",
    "            'db2': db2\n",
    "        }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients: dict, learning_rate: float):\n",
    "        \"\"\"Update parameters using gradient descent.\"\"\"\n",
    "        for i in range(self.num_shards):\n",
    "            self.W1_shards[i] -= learning_rate * gradients['dW1_shards'][i]\n",
    "            self.b1_shards[i] -= learning_rate * gradients['db1_shards'][i]\n",
    "        self.W2 -= learning_rate * gradients['dW2']\n",
    "        self.b2 -= learning_rate * gradients['db2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Column-Sharded MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "batch_size = 32\n",
    "input_size = 10\n",
    "hidden_size = 16  # Must be divisible by num_shards\n",
    "output_size = 5\n",
    "num_shards = 4\n",
    "\n",
    "X = np.random.randn(batch_size, input_size)\n",
    "y_true = np.random.randn(batch_size, output_size)\n",
    "\n",
    "# Initialize column-sharded model\n",
    "col_model = ColumnShardedMLP(input_size, hidden_size, output_size, num_shards)\n",
    "\n",
    "print(f\"Number of shards: {num_shards}\")\n",
    "print(f\"Shard size: {col_model.shard_size}\")\n",
    "print(f\"\\nWeight shard shapes:\")\n",
    "for i, W_shard in enumerate(col_model.W1_shards):\n",
    "    print(f\"  W1_shard_{i}: {W_shard.shape}\")\n",
    "\n",
    "# Train for a few iterations\n",
    "learning_rate = 0.01\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    y_pred = col_model.forward(X)\n",
    "    loss = mse_loss(y_pred, y_true)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    dL_dy = mse_loss_derivative(y_pred, y_true)\n",
    "    gradients = col_model.backward(dL_dy)\n",
    "    \n",
    "    # Update parameters\n",
    "    col_model.update_parameters(gradients, learning_rate)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss - Column-Sharded MLP')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data-Parallel MLP\n",
    "\n",
    "In data parallelism, we split the input data (batch) across multiple workers. Each worker has a full copy of the model.\n",
    "\n",
    "### Forward Pass:\n",
    "```\n",
    "Each worker i:\n",
    "  X_batch_i = X[shard_i_indices]\n",
    "  y_pred_i = forward(X_batch_i)  # Using full model\n",
    "```\n",
    "\n",
    "### Backward Pass:\n",
    "```\n",
    "Each worker i:\n",
    "  gradients_i = backward(X_batch_i, y_batch_i)\n",
    "All-Reduce: gradients = average(gradients_i)\n",
    "Update model with averaged gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParallelMLP:\n",
    "    \"\"\"Data-Parallel MLP - Pure NumPy implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_workers: int):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Each worker has a copy of the full model\n",
    "        # Initialize shared weights (in practice, these would be synchronized)\n",
    "        self.W1, self.b1 = initialize_weights(input_size, hidden_size)\n",
    "        self.W2, self.b2 = initialize_weights(hidden_size, output_size)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward_worker(self, X_batch: np.ndarray, worker_id: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass for a single worker.\n",
    "        \n",
    "        Args:\n",
    "            X_batch: Input data batch for this worker\n",
    "            worker_id: ID of the worker\n",
    "        \n",
    "        Returns:\n",
    "            Output for this batch\n",
    "        \"\"\"\n",
    "        # Layer 1: Linear + ReLU\n",
    "        z1 = X_batch @ self.W1 + self.b1\n",
    "        a1 = relu(z1)\n",
    "        \n",
    "        # Layer 2: Linear\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        \n",
    "        return z2, {'X': X_batch, 'z1': z1, 'a1': a1, 'z2': z2}\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass with data parallelism.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data of shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            Output of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Split data across workers\n",
    "        # Handle cases where batch_size is not evenly divisible\n",
    "        worker_batch_size = batch_size // self.num_workers\n",
    "        \n",
    "        outputs = []\n",
    "        worker_caches = []\n",
    "        \n",
    "        for worker_id in range(self.num_workers):\n",
    "            start_idx = worker_id * worker_batch_size\n",
    "            if worker_id == self.num_workers - 1:\n",
    "                # Last worker handles remainder\n",
    "                end_idx = batch_size\n",
    "            else:\n",
    "                end_idx = start_idx + worker_batch_size\n",
    "            \n",
    "            X_batch = X[start_idx:end_idx]\n",
    "            y_batch, cache = self.forward_worker(X_batch, worker_id)\n",
    "            outputs.append(y_batch)\n",
    "            worker_caches.append(cache)\n",
    "        \n",
    "        # Concatenate outputs from all workers\n",
    "        y_pred = np.concatenate(outputs, axis=0)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {'worker_caches': worker_caches}\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def backward_worker(self, dL_dy_batch: np.ndarray, cache: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Backward pass for a single worker.\n",
    "        \n",
    "        Args:\n",
    "            dL_dy_batch: Gradient for this batch\n",
    "            cache: Cached values from forward pass\n",
    "        \n",
    "        Returns:\n",
    "            Gradients for this worker\n",
    "        \"\"\"\n",
    "        X = cache['X']\n",
    "        z1 = cache['z1']\n",
    "        a1 = cache['a1']\n",
    "        \n",
    "        # Gradient for Layer 2\n",
    "        dz2 = dL_dy_batch\n",
    "        dW2 = a1.T @ dz2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient for Layer 1\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * relu_derivative(z1)\n",
    "        dW1 = X.T @ dz1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return {\n",
    "            'dW1': dW1,\n",
    "            'db1': db1,\n",
    "            'dW2': dW2,\n",
    "            'db2': db2\n",
    "        }\n",
    "    \n",
    "    def backward(self, dL_dy: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Backward pass with data parallelism.\n",
    "        \n",
    "        Args:\n",
    "            dL_dy: Gradient of loss w.r.t. output\n",
    "        \n",
    "        Returns:\n",
    "            Averaged gradients from all workers\n",
    "        \"\"\"\n",
    "        worker_caches = self.cache['worker_caches']\n",
    "        worker_batch_size = batch_size // self.num_workers\n",
    "        \n",
    "        # Compute gradients on each worker\n",
    "        worker_gradients = []\n",
    "        \n",
    "        for worker_id in range(self.num_workers):\n",
    "            start_idx = worker_id * worker_batch_size\n",
    "            if worker_id == self.num_workers - 1:\n",
    "                end_idx = batch_size\n",
    "            else:\n",
    "                end_idx = start_idx + worker_batch_size\n",
    "            \n",
    "            dL_dy_batch = dL_dy[start_idx:end_idx]\n",
    "            gradients = self.backward_worker(dL_dy_batch, worker_caches[worker_id])\n",
    "            worker_gradients.append(gradients)\n",
    "        \n",
    "        # All-Reduce: Average gradients across workers\n",
    "        averaged_gradients = {\n",
    "            'dW1': np.mean([g['dW1'] for g in worker_gradients], axis=0),\n",
    "            'db1': np.mean([g['db1'] for g in worker_gradients], axis=0),\n",
    "            'dW2': np.mean([g['dW2'] for g in worker_gradients], axis=0),\n",
    "            'db2': np.mean([g['db2'] for g in worker_gradients], axis=0)\n",
    "        }\n",
    "        \n",
    "        return averaged_gradients\n",
    "    \n",
    "    def update_parameters(self, gradients: dict, learning_rate: float):\n",
    "        \"\"\"Update parameters using gradient descent.\"\"\"\n",
    "        self.W1 -= learning_rate * gradients['dW1']\n",
    "        self.b1 -= learning_rate * gradients['db1']\n",
    "        self.W2 -= learning_rate * gradients['dW2']\n",
    "        self.b2 -= learning_rate * gradients['db2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data-Parallel MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "batch_size = 32\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "num_workers = 4\n",
    "\n",
    "X = np.random.randn(batch_size, input_size)\n",
    "y_true = np.random.randn(batch_size, output_size)\n",
    "\n",
    "# Initialize data-parallel model\n",
    "dp_model = DataParallelMLP(input_size, hidden_size, output_size, num_workers)\n",
    "\n",
    "print(f\"Number of workers: {num_workers}\")\n",
    "print(f\"Batch size per worker: {batch_size // num_workers}\")\n",
    "print(f\"\\nModel weight shapes:\")\n",
    "print(f\"  W1: {dp_model.W1.shape}\")\n",
    "print(f\"  W2: {dp_model.W2.shape}\")\n",
    "\n",
    "# Train for a few iterations\n",
    "learning_rate = 0.01\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    y_pred = dp_model.forward(X)\n",
    "    loss = mse_loss(y_pred, y_true)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    dL_dy = mse_loss_derivative(y_pred, y_true)\n",
    "    gradients = dp_model.backward(dL_dy)\n",
    "    \n",
    "    # Update parameters\n",
    "    dp_model.update_parameters(gradients, learning_rate)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss - Data-Parallel MLP')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison of All Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create common synthetic data\n",
    "batch_size = 32\n",
    "input_size = 16  # Divisible by 4\n",
    "hidden_size = 16  # Divisible by 4\n",
    "output_size = 5\n",
    "num_shards = 4\n",
    "\n",
    "X = np.random.randn(batch_size, input_size)\n",
    "y_true = np.random.randn(batch_size, output_size)\n",
    "\n",
    "# Initialize all models\n",
    "models = {\n",
    "    'Standard MLP': TwoLayerMLP(input_size, hidden_size, output_size),\n",
    "    'Row-Sharded MLP': RowShardedMLP(input_size, hidden_size, output_size, num_shards),\n",
    "    'Column-Sharded MLP': ColumnShardedMLP(input_size, hidden_size, output_size, num_shards),\n",
    "    'Data-Parallel MLP': DataParallelMLP(input_size, hidden_size, output_size, num_shards)\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "all_losses = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        y_pred = model.forward(X)\n",
    "        loss = mse_loss(y_pred, y_true)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        dL_dy = mse_loss_derivative(y_pred, y_true)\n",
    "        gradients = model.backward(dL_dy)\n",
    "        \n",
    "        # Update parameters\n",
    "        model.update_parameters(gradients, learning_rate)\n",
    "    \n",
    "    all_losses[name] = losses\n",
    "    print(f\"  Final loss: {losses[-1]:.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, losses in all_losses.items():\n",
    "    plt.plot(losses, label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.title('Training Loss Comparison - All MLP Approaches', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary: All approaches converge to similar loss values.\")\n",
    "print(\"This demonstrates that parallelism strategies maintain\")\n",
    "print(\"mathematical equivalence while enabling distributed computation.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "### Implementation Summary:\n",
    "\n",
    "1. **2-Layer MLP with ReLU**\n",
    "   - Pure NumPy implementation with batch support\n",
    "   - Forward pass: Linear \u2192 ReLU \u2192 Linear\n",
    "   - Backward pass: Computes gradients using chain rule\n",
    "\n",
    "2. **Row-Sharded (Tensor-Parallel) MLP**\n",
    "   - Splits input features across workers\n",
    "   - Each worker processes a subset of input dimensions\n",
    "   - Requires all-reduce operation to sum partial results\n",
    "   - Memory savings: Weights distributed across workers\n",
    "\n",
    "3. **Column-Parallel Sharded MLP**\n",
    "   - Splits hidden layer neurons across workers\n",
    "   - Each worker computes a portion of hidden activations\n",
    "   - Outputs are concatenated\n",
    "   - Useful when hidden layer is very large\n",
    "\n",
    "4. **Data-Parallel MLP**\n",
    "   - Splits batch across workers\n",
    "   - Each worker has full model copy\n",
    "   - Gradients are averaged across workers\n",
    "   - Most common parallelism strategy in practice\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Forward Pass**: Compute outputs from inputs\n",
    "- **Backward Pass**: Compute gradients using backpropagation\n",
    "- **Batch Processing**: Process multiple samples simultaneously\n",
    "- **Parallelism**: Distribute computation for efficiency\n",
    "- **All-Reduce**: Synchronization operation for distributed computing\n",
    "\n",
    "### Mathematical Equivalence:\n",
    "All parallelism strategies produce mathematically equivalent results to the standard MLP, assuming proper synchronization. The choice depends on:\n",
    "- Model size (tensor parallelism for large models)\n",
    "- Batch size (data parallelism for large batches)\n",
    "- Communication bandwidth\n",
    "- Hardware constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}