{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Transformer Architecture with Mixture of Experts\n",
    "\n",
    "This notebook demonstrates a complete Transformer architecture with:\n",
    "1. Input embeddings and positional encoding\n",
    "2. Multi-head attention mechanism\n",
    "3. Mixture of Experts (MoE) feed-forward network\n",
    "4. Residual connections and layer normalization\n",
    "5. Complete transformer block\n",
    "6. Practical sequence modeling example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Positional Encoding\n",
    "\n",
    "Positional encodings add information about the position of tokens in a sequence.\n",
    "We use sinusoidal functions:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding for transformers.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        max_len: Maximum sequence length\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor with positional encoding added\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Test positional encoding\n",
    "d_model = 64\n",
    "seq_len = 50\n",
    "batch_size = 2\n",
    "\n",
    "pos_encoder = PositionalEncoding(d_model)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "x_with_pos = pos_encoder(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {x_with_pos.shape}\")\n",
    "\n",
    "# Visualize positional encoding\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pos_encoder.pe[0, :seq_len, :].cpu().numpy(), aspect='auto', cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Embedding Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Sinusoidal Positional Encoding')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to attend to information from different representation subspaces.\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot-product attention.\n",
    "        \n",
    "        Args:\n",
    "            Q: Query tensor (batch_size, num_heads, seq_len, d_k)\n",
    "            K: Key tensor (batch_size, num_heads, seq_len, d_k)\n",
    "            V: Value tensor (batch_size, num_heads, seq_len, d_k)\n",
    "            mask: Optional mask tensor\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            query: Query tensor (batch_size, seq_len, d_model)\n",
    "            key: Key tensor (batch_size, seq_len, d_model)\n",
    "            value: Value tensor (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask tensor\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output (batch_size, seq_len, d_model)\n",
    "            attention_weights: Attention weights for visualization\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        x, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(x)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output, attention_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in mha.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mixture of Experts Feed-Forward Network\n",
    "\n",
    "Instead of a standard feed-forward network, we use a Mixture of Experts layer\n",
    "that routes tokens to specialized expert networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\"Single expert network.\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    \"\"\"Gating network for routing tokens to experts.\"\"\"\n",
    "    def __init__(self, d_model, num_experts):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.gate(x)\n",
    "\n",
    "class MixtureOfExpertsFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts Feed-Forward Network.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        d_ff: Feed-forward dimension\n",
    "        num_experts: Number of expert networks\n",
    "        top_k: Number of experts to use per token\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, num_experts=8, top_k=2, dropout=0.1):\n",
    "        super(MixtureOfExpertsFFN, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create expert networks\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff, dropout) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Gating network\n",
    "        self.gate = GatingNetwork(d_model, num_experts)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through MoE layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            output: Output tensor (batch_size, seq_len, d_model)\n",
    "            aux_loss: Load balancing loss\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Compute gate logits\n",
    "        gate_logits = self.gate(x)  # (batch_size, seq_len, num_experts)\n",
    "        gate_probs = F.softmax(gate_logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts\n",
    "        top_k_probs, top_k_indices = torch.topk(gate_probs, self.top_k, dim=-1)\n",
    "        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Initialize output\n",
    "        output = torch.zeros_like(x)\n",
    "        \n",
    "        # Process through selected experts\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Get expert output\n",
    "            expert_output = expert(x)\n",
    "            \n",
    "            # Get weights for this expert\n",
    "            expert_weights = torch.where(\n",
    "                top_k_indices == i,\n",
    "                top_k_probs,\n",
    "                torch.zeros_like(top_k_probs)\n",
    "            ).sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Add weighted expert output\n",
    "            output += expert_weights * expert_output\n",
    "        \n",
    "        # Compute load balancing loss\n",
    "        expert_usage = torch.zeros(self.num_experts, device=x.device)\n",
    "        for i in range(self.num_experts):\n",
    "            expert_usage[i] = (top_k_indices == i).float().mean()\n",
    "        aux_loss = self.num_experts * (expert_usage ** 2).sum()\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        return output, aux_loss\n",
    "\n",
    "# Test MoE FFN\n",
    "moe_ffn = MixtureOfExpertsFFN(d_model=64, d_ff=256, num_experts=4, top_k=2)\n",
    "x = torch.randn(2, 10, 64)\n",
    "output, aux_loss = moe_ffn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Auxiliary loss: {aux_loss.item():.4f}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in moe_ffn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Block with MoE\n",
    "\n",
    "A complete transformer block combining:\n",
    "- Multi-head self-attention\n",
    "- Mixture of Experts feed-forward network\n",
    "- Residual connections\n",
    "- Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with multi-head attention and MoE feed-forward.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        d_ff: Feed-forward dimension\n",
    "        num_experts: Number of expert networks\n",
    "        top_k: Number of experts per token\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_experts=8, top_k=2, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # MoE feed-forward network\n",
    "        self.moe_ffn = MixtureOfExpertsFFN(d_model, d_ff, num_experts, top_k, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through transformer block.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Output tensor (batch_size, seq_len, d_model)\n",
    "            attention_weights: Attention weights for visualization\n",
    "            aux_loss: Load balancing loss from MoE\n",
    "        \"\"\"\n",
    "        # Multi-head attention with residual connection and layer norm\n",
    "        attn_output, attention_weights = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # MoE feed-forward with residual connection and layer norm\n",
    "        moe_output, aux_loss = self.moe_ffn(x)\n",
    "        x = self.norm2(x + moe_output)\n",
    "        \n",
    "        return x, attention_weights, aux_loss\n",
    "\n",
    "# Test transformer block\n",
    "transformer_block = TransformerBlock(\n",
    "    d_model=64,\n",
    "    num_heads=8,\n",
    "    d_ff=256,\n",
    "    num_experts=4,\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 10, 64)\n",
    "output, attention_weights, aux_loss = transformer_block(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Auxiliary loss: {aux_loss.item():.4f}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in transformer_block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Transformer Model\n",
    "\n",
    "Stack multiple transformer blocks to create a complete model with:\n",
    "- Token embeddings\n",
    "- Positional encoding\n",
    "- Multiple transformer blocks\n",
    "- Output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer model with Mixture of Experts.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        d_ff: Feed-forward dimension\n",
    "        num_layers: Number of transformer blocks\n",
    "        num_experts: Number of expert networks\n",
    "        top_k: Number of experts per token\n",
    "        max_len: Maximum sequence length\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, \n",
    "                 num_experts=8, top_k=2, max_len=5000, dropout=0.1):\n",
    "        super(TransformerMoE, self).__init__()\n",
    "        \n",
    "        # Token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, num_experts, top_k, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Model parameters\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer.\n",
    "        \n",
    "        Args:\n",
    "            x: Input token indices (batch_size, seq_len)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            logits: Output logits (batch_size, seq_len, vocab_size)\n",
    "            total_aux_loss: Sum of auxiliary losses from all layers\n",
    "            attention_weights: List of attention weights from each layer\n",
    "        \"\"\"\n",
    "        # Token embedding with scaling\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        total_aux_loss = 0.0\n",
    "        attention_weights_list = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, attention_weights, aux_loss = layer(x, mask)\n",
    "            total_aux_loss += aux_loss\n",
    "            attention_weights_list.append(attention_weights)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits, total_aux_loss, attention_weights_list\n",
    "\n",
    "# Create model\n",
    "vocab_size = 1000\n",
    "model = TransformerMoE(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    d_ff=512,\n",
    "    num_layers=4,\n",
    "    num_experts=4,\n",
    "    top_k=2,\n",
    "    max_len=512,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Test the model\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "\n",
    "logits, aux_loss, attention_weights = model(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Total auxiliary loss: {aux_loss.item():.4f}\")\n",
    "print(f\"Number of attention weight tensors: {len(attention_weights)}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single sequence for visualization\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_seq = torch.randint(0, vocab_size, (1, 15)).to(device)\n",
    "    _, _, attention_weights = model(test_seq)\n",
    "\n",
    "# Visualize attention from first layer, first head\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for layer_idx in range(min(4, len(attention_weights))):\n",
    "    # Get attention weights for first head\n",
    "    attn = attention_weights[layer_idx][0, 0].cpu().numpy()  # (seq_len, seq_len)\n",
    "    \n",
    "    sns.heatmap(attn, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "                square=True, ax=axes[layer_idx],\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    axes[layer_idx].set_title(f'Layer {layer_idx + 1} - Head 1 Attention')\n",
    "    axes[layer_idx].set_xlabel('Key Position')\n",
    "    axes[layer_idx].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Example: Sequence Modeling\n",
    "\n",
    "Let's train the model on a simple sequence prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple synthetic dataset\n",
    "# Task: Predict the next token in a sequence\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def generate_synthetic_data(num_samples, seq_len, vocab_size):\n",
    "    \"\"\"Generate synthetic sequential data.\"\"\"\n",
    "    # Create sequences where next token is related to previous tokens\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        # Generate random sequence\n",
    "        seq = torch.randint(0, vocab_size // 2, (seq_len,))\n",
    "        # Target is shifted sequence (next token prediction)\n",
    "        target = torch.cat([seq[1:], torch.tensor([seq[-1] + 1])])\n",
    "        data.append((seq, target))\n",
    "    return data\n",
    "\n",
    "# Generate dataset\n",
    "num_samples = 500\n",
    "seq_len = 20\n",
    "vocab_size = 100\n",
    "dataset = generate_synthetic_data(num_samples, seq_len, vocab_size)\n",
    "\n",
    "# Create a smaller model for training\n",
    "small_model = TransformerMoE(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    d_ff=256,\n",
    "    num_layers=2,\n",
    "    num_experts=4,\n",
    "    top_k=2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(small_model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "aux_loss_weight = 0.01\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "losses = []\n",
    "aux_losses = []\n",
    "\n",
    "small_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_aux_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    indices = torch.randperm(len(dataset))\n",
    "    \n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        batch_seqs = torch.stack([dataset[idx][0] for idx in batch_indices]).to(device)\n",
    "        batch_targets = torch.stack([dataset[idx][1] for idx in batch_indices]).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, aux_loss, _ = small_model(batch_seqs)\n",
    "        \n",
    "        # Compute loss\n",
    "        classification_loss = criterion(logits.view(-1, vocab_size), batch_targets.view(-1))\n",
    "        total_loss = classification_loss + aux_loss_weight * aux_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(small_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += classification_loss.item()\n",
    "        epoch_aux_loss += aux_loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    epoch_loss /= num_batches\n",
    "    epoch_aux_loss /= num_batches\n",
    "    losses.append(epoch_loss)\n",
    "    aux_losses.append(epoch_aux_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Aux Loss: {epoch_aux_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Classification loss\n",
    "ax1.plot(losses, label='Classification Loss', color='blue', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Classification Loss During Training')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Auxiliary loss\n",
    "ax2.plot(aux_losses, label='Auxiliary Loss (Load Balancing)', color='orange', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Auxiliary Loss')\n",
    "ax2.set_title('Load Balancing Loss During Training')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model\n",
    "small_model.eval()\n",
    "total_correct = 0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq, target in dataset[:100]:  # Evaluate on first 100 samples\n",
    "        seq = seq.unsqueeze(0).to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        logits, _, _ = small_model(seq)\n",
    "        predictions = torch.argmax(logits[0], dim=-1)\n",
    "        \n",
    "        total_correct += (predictions == target).sum().item()\n",
    "        total_tokens += target.size(0)\n",
    "\n",
    "accuracy = total_correct / total_tokens\n",
    "print(f\"\\nModel Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in small_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Inference and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a sample sequence\n",
    "small_model.eval()\n",
    "test_seq, test_target = dataset[0]\n",
    "test_seq_batch = test_seq.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, aux_loss, attention_weights = small_model(test_seq_batch)\n",
    "    predictions = torch.argmax(logits[0], dim=-1).cpu()\n",
    "\n",
    "# Print results\n",
    "print(\"Sample Sequence Prediction:\")\n",
    "print(f\"Input:       {test_seq.tolist()[:10]}...\")\n",
    "print(f\"Target:      {test_target.tolist()[:10]}...\")\n",
    "print(f\"Predictions: {predictions.tolist()[:10]}...\")\n",
    "print(f\"\\nMatches: {(predictions == test_target).sum().item()} / {len(test_target)}\")\n",
    "\n",
    "# Visualize attention pattern for this sequence\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Average attention across all heads for first layer\n",
    "avg_attention = attention_weights[0][0].mean(dim=0).cpu().numpy()\n",
    "sns.heatmap(avg_attention, cmap='YlOrRd', square=True,\n",
    "            cbar_kws={'label': 'Average Attention Weight'})\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Average Attention Pattern (Layer 1, All Heads)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we implemented a complete Transformer architecture with the following components:\n",
    "\n",
    "### 1. **Input Embeddings and Positional Encoding**\n",
    "   - Token embeddings convert discrete tokens to continuous vectors\n",
    "   - Sinusoidal positional encodings add position information\n",
    "   - Scaling by âˆšd_model for better training dynamics\n",
    "\n",
    "### 2. **Multi-Head Attention**\n",
    "   - Scaled dot-product attention mechanism\n",
    "   - Multiple attention heads for different representation subspaces\n",
    "   - Allows model to attend to different positions simultaneously\n",
    "\n",
    "### 3. **Mixture of Experts (MoE)**\n",
    "   - Replaces standard feed-forward network\n",
    "   - Multiple expert networks with specialized capabilities\n",
    "   - Top-K routing for efficient sparse activation\n",
    "   - Load balancing loss for uniform expert usage\n",
    "\n",
    "### 4. **Residual Connections**\n",
    "   - Skip connections around attention and feed-forward layers\n",
    "   - Enables training of very deep networks\n",
    "   - Helps with gradient flow\n",
    "\n",
    "### 5. **Layer Normalization**\n",
    "   - Applied after residual connections\n",
    "   - Stabilizes training\n",
    "   - Improves convergence\n",
    "\n",
    "### Key Advantages of This Architecture:\n",
    "\n",
    "- **Scalability**: MoE allows scaling to billions of parameters while keeping computation constant\n",
    "- **Efficiency**: Sparse activation through Top-K routing\n",
    "- **Specialization**: Different experts can learn different patterns\n",
    "- **Parallelism**: Attention and expert computation can be parallelized\n",
    "- **Flexibility**: Can be adapted for various sequence modeling tasks\n",
    "\n",
    "### Practical Applications:\n",
    "\n",
    "- Language modeling and text generation\n",
    "- Machine translation\n",
    "- Document understanding\n",
    "- Code generation\n",
    "- Time series forecasting\n",
    "- Any sequential data processing task\n",
    "\n",
    "This architecture combines the best aspects of modern Transformers with the scalability benefits of Mixture of Experts, making it suitable for large-scale applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
