{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharded MLP (Multi-Layer Perceptron)\n",
    "\n",
    "This notebook demonstrates the concept of a Sharded MLP, which is useful for:\n",
    "- Distributed training across multiple devices\n",
    "- Handling large models that don't fit in single GPU memory\n",
    "- Model parallelism strategies\n",
    "\n",
    "We'll implement:\n",
    "1. A basic MLP architecture\n",
    "2. Sharding strategy to split the model across multiple \"devices\" (simulated)\n",
    "3. Forward and backward pass with sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standard MLP (Non-Sharded) for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardMLP(nn.Module):\n",
    "    \"\"\"Standard Multi-Layer Perceptron without sharding.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(StandardMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create standard MLP\n",
    "standard_mlp = StandardMLP(input_size=784, hidden_sizes=[512, 256, 128], output_size=10)\n",
    "print(\"Standard MLP:\")\n",
    "print(standard_mlp)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in standard_mlp.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sharded MLP Implementation\n",
    "\n",
    "In a sharded MLP, we split the model across multiple devices. Each \"shard\" contains a portion of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPShard(nn.Module):\n",
    "    \"\"\"A single shard of the MLP.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size, shard_id, device='cpu'):\n",
    "        super(MLPShard, self).__init__()\n",
    "        self.shard_id = shard_id\n",
    "        self.device = device\n",
    "        \n",
    "        # Each shard has a linear layer and activation\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Move to specified device\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move input to this shard's device\n",
    "        x = x.to(self.device)\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class ShardedMLP(nn.Module):\n",
    "    \"\"\"Sharded Multi-Layer Perceptron distributed across multiple devices.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, devices=None):\n",
    "        super(ShardedMLP, self).__init__()\n",
    "        \n",
    "        # If no devices specified, simulate with CPU\n",
    "        if devices is None:\n",
    "            devices = ['cpu'] * (len(hidden_sizes) + 1)\n",
    "        \n",
    "        self.shards = nn.ModuleList()\n",
    "        self.devices = devices\n",
    "        \n",
    "        # Create shards for hidden layers\n",
    "        prev_size = input_size\n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            shard = MLPShard(prev_size, hidden_size, shard_id=i, device=devices[i])\n",
    "            self.shards.append(shard)\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer (final shard)\n",
    "        self.output_layer = nn.Linear(prev_size, output_size)\n",
    "        self.output_device = devices[-1]\n",
    "        self.output_layer.to(self.output_device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass through each shard sequentially\n",
    "        for i, shard in enumerate(self.shards):\n",
    "            x = shard(x)\n",
    "            # Data is automatically moved to next shard's device\n",
    "        \n",
    "        # Final output layer\n",
    "        x = x.to(self.output_device)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def print_shard_info(self):\n",
    "        \"\"\"Print information about each shard.\"\"\"\n",
    "        print(\"Sharded MLP Architecture:\")\n",
    "        print(\"=\" * 60)\n",
    "        for i, shard in enumerate(self.shards):\n",
    "            params = sum(p.numel() for p in shard.parameters())\n",
    "            print(f\"Shard {i}: Device={shard.device}, \"\n",
    "                  f\"Shape={shard.linear.weight.shape}, \"\n",
    "                  f\"Parameters={params:,}\")\n",
    "        \n",
    "        output_params = sum(p.numel() for p in self.output_layer.parameters())\n",
    "        print(f\"Output Layer: Device={self.output_device}, \"\n",
    "              f\"Shape={self.output_layer.weight.shape}, \"\n",
    "              f\"Parameters={output_params:,}\")\n",
    "        print(\"=\" * 60)\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"Total Parameters: {total_params:,}\")\n",
    "\n",
    "# Create sharded MLP (simulating multiple devices with CPU)\n",
    "sharded_mlp = ShardedMLP(\n",
    "    input_size=784,\n",
    "    hidden_sizes=[512, 256, 128],\n",
    "    output_size=10,\n",
    "    devices=['cpu', 'cpu', 'cpu', 'cpu']  # In practice, these would be different GPUs\n",
    ")\n",
    "\n",
    "sharded_mlp.print_shard_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Memory Distribution\n",
    "\n",
    "In a real sharded setup, each shard would be on a different GPU, reducing memory pressure on each device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_memory_per_device(model):\n",
    "    \"\"\"Calculate memory usage per device for a sharded model.\"\"\"\n",
    "    device_memory = {}\n",
    "    \n",
    "    if isinstance(model, ShardedMLP):\n",
    "        for i, shard in enumerate(model.shards):\n",
    "            device = str(shard.device)\n",
    "            params = sum(p.numel() * p.element_size() for p in shard.parameters())\n",
    "            device_memory[f\"Shard {i} ({device})\"] = params\n",
    "        \n",
    "        # Output layer\n",
    "        device = str(model.output_device)\n",
    "        params = sum(p.numel() * p.element_size() for p in model.output_layer.parameters())\n",
    "        device_memory[f\"Output ({device})\"] = params\n",
    "    else:\n",
    "        # Standard model - all on one device\n",
    "        total_memory = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "        device_memory[\"Single Device\"] = total_memory\n",
    "    \n",
    "    return device_memory\n",
    "\n",
    "# Calculate memory for both models\n",
    "standard_memory = calculate_memory_per_device(standard_mlp)\n",
    "sharded_memory = calculate_memory_per_device(sharded_mlp)\n",
    "\n",
    "print(\"\\nMemory Distribution:\")\n",
    "print(\"\\nStandard MLP:\")\n",
    "for device, memory in standard_memory.items():\n",
    "    print(f\"  {device}: {memory / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nSharded MLP:\")\n",
    "for device, memory in sharded_memory.items():\n",
    "    print(f\"  {device}: {memory / 1024**2:.2f} MB\")\n",
    "\n",
    "# Visualize memory distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Standard MLP\n",
    "ax1.bar(standard_memory.keys(), [m / 1024**2 for m in standard_memory.values()], color='blue')\n",
    "ax1.set_ylabel('Memory (MB)')\n",
    "ax1.set_title('Standard MLP Memory Distribution')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sharded MLP\n",
    "ax2.bar(sharded_memory.keys(), [m / 1024**2 for m in sharded_memory.values()], color='green')\n",
    "ax2.set_ylabel('Memory (MB)')\n",
    "ax2.set_title('Sharded MLP Memory Distribution')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input\n",
    "batch_size = 8\n",
    "input_data = torch.randn(batch_size, 784)\n",
    "\n",
    "print(f\"Input shape: {input_data.shape}\")\n",
    "\n",
    "# Forward pass through standard MLP\n",
    "standard_mlp.eval()\n",
    "with torch.no_grad():\n",
    "    standard_output = standard_mlp(input_data)\n",
    "print(f\"Standard MLP output shape: {standard_output.shape}\")\n",
    "\n",
    "# Forward pass through sharded MLP\n",
    "sharded_mlp.eval()\n",
    "with torch.no_grad():\n",
    "    sharded_output = sharded_mlp(input_data)\n",
    "print(f\"Sharded MLP output shape: {sharded_output.shape}\")\n",
    "\n",
    "# Verify outputs have same shape\n",
    "print(f\"\\nOutputs match in shape: {standard_output.shape == sharded_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demonstrate Communication Between Shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseShardedMLP(ShardedMLP):\n",
    "    \"\"\"Sharded MLP with verbose output to show data movement.\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input: shape={x.shape}, device={x.device}\")\n",
    "        \n",
    "        for i, shard in enumerate(self.shards):\n",
    "            print(f\"\\nProcessing Shard {i}:\")\n",
    "            print(f\"  Before: device={x.device}\")\n",
    "            x = shard(x)\n",
    "            print(f\"  After: shape={x.shape}, device={x.device}\")\n",
    "        \n",
    "        print(f\"\\nProcessing Output Layer:\")\n",
    "        print(f\"  Before: device={x.device}\")\n",
    "        x = x.to(self.output_device)\n",
    "        x = self.output_layer(x)\n",
    "        print(f\"  After: shape={x.shape}, device={x.device}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create verbose sharded MLP\n",
    "verbose_sharded = VerboseShardedMLP(\n",
    "    input_size=784,\n",
    "    hidden_sizes=[512, 256, 128],\n",
    "    output_size=10,\n",
    "    devices=['cpu', 'cpu', 'cpu', 'cpu']\n",
    ")\n",
    "\n",
    "# Demonstrate forward pass with verbose output\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERBOSE FORWARD PASS - Data Movement Between Shards\")\n",
    "print(\"=\"*60)\n",
    "sample_input = torch.randn(2, 784)\n",
    "with torch.no_grad():\n",
    "    output = verbose_sharded(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training with Sharded MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for demonstration\n",
    "def generate_synthetic_data(num_samples=1000, input_size=784, num_classes=10):\n",
    "    \"\"\"Generate synthetic data for training.\"\"\"\n",
    "    X = torch.randn(num_samples, input_size)\n",
    "    y = torch.randint(0, num_classes, (num_samples,))\n",
    "    return X, y\n",
    "\n",
    "# Generate training data\n",
    "X_train, y_train = generate_synthetic_data(num_samples=1000)\n",
    "X_test, y_test = generate_synthetic_data(num_samples=200)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Test data: {X_test.shape}\")\n",
    "\n",
    "# Training function\n",
    "def train_model(model, X, y, epochs=10, lr=0.001, batch_size=32):\n",
    "    \"\"\"Train a model (standard or sharded).\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch_X = X[i:i+batch_size]\n",
    "            batch_y = y[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            # Move target to same device as output\n",
    "            batch_y = batch_y.to(outputs.device)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / (len(X) // batch_size)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train sharded MLP\n",
    "print(\"\\nTraining Sharded MLP:\")\n",
    "print(\"-\" * 40)\n",
    "losses = train_model(sharded_mlp, X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(losses) + 1), losses, 'b-', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss - Sharded MLP')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Evaluate model accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # Move y to same device as predicted\n",
    "        y = y.to(predicted.device)\n",
    "        accuracy = (predicted == y).sum().item() / len(y)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate\n",
    "train_acc = evaluate_model(sharded_mlp, X_train, y_train)\n",
    "test_acc = evaluate_model(sharded_mlp, X_test, y_test)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_acc*100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advantages of Sharded MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAdvantages of Sharded MLP:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Memory Distribution:\")\n",
    "print(\"   - Each shard uses only a fraction of total model memory\")\n",
    "print(\"   - Enables training of larger models\")\n",
    "print(\"\\n2. Parallel Processing:\")\n",
    "print(\"   - Different shards can process different samples\")\n",
    "print(\"   - Pipeline parallelism possible\")\n",
    "print(\"\\n3. Scalability:\")\n",
    "print(\"   - Can scale to very large models by adding more shards\")\n",
    "print(\"   - Not limited by single device memory\")\n",
    "print(\"\\n4. Flexibility:\")\n",
    "print(\"   - Can place different shards on different devices\")\n",
    "print(\"   - Mix of GPU/CPU placement possible\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTrade-offs:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Communication Overhead:\")\n",
    "print(\"   - Data transfer between devices takes time\")\n",
    "print(\"   - Network bandwidth can be a bottleneck\")\n",
    "print(\"\\n2. Complexity:\")\n",
    "print(\"   - More complex to implement and debug\")\n",
    "print(\"   - Requires careful device placement strategy\")\n",
    "print(\"\\n3. Load Balancing:\")\n",
    "print(\"   - Shards should have similar compute requirements\")\n",
    "print(\"   - Imbalanced shards lead to inefficiency\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Standard MLP**: Traditional implementation on a single device\n",
    "2. **Sharded MLP**: Distributed implementation across multiple devices\n",
    "3. **Memory Distribution**: How sharding reduces per-device memory usage\n",
    "4. **Data Movement**: Communication between shards during forward/backward pass\n",
    "5. **Training**: Complete training pipeline with sharded model\n",
    "\n",
    "Key concepts:\n",
    "- **Model Sharding**: Splitting model layers across devices\n",
    "- **Device Placement**: Strategic placement of model components\n",
    "- **Inter-device Communication**: Data transfer during forward/backward pass\n",
    "- **Memory Efficiency**: Reduced memory footprint per device\n",
    "\n",
    "Use cases:\n",
    "- Training very large models (billions of parameters)\n",
    "- Multi-GPU training with model parallelism\n",
    "- Distributed deep learning systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
